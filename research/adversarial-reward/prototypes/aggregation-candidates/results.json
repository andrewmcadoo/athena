{
  "generated_at_utc": "2026-02-22T07:35:36.765494+00:00",
  "candidate_order": [
    "IVW-CDF",
    "HTG-Max",
    "Fisher-UP",
    "Hybrid"
  ],
  "fixtures": [
    {
      "idx": 1,
      "name": "Noisy TV",
      "what_it_tests": "One metric with high divergence and high uncertainty; inflation should be suppressed.",
      "pass_criterion": "score(value*2, se*2) <= score(value, se)",
      "datasets": {
        "base": [
          {
            "kind": "AbsoluteDifference",
            "value": 0.0012,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 80,
                "standard_error": 0.3,
                "interval": null,
                "method_ref": "s1.absdiff.base.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s1.absdiff.base"
          }
        ],
        "doubled": [
          {
            "kind": "AbsoluteDifference",
            "value": 0.0024,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 80,
                "standard_error": 0.6,
                "interval": null,
                "method_ref": "s1.absdiff.doubled.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s1.absdiff.doubled"
          }
        ]
      }
    },
    {
      "idx": 2,
      "name": "Unanimous weak signal",
      "what_it_tests": "Eight weak but consistent contradiction metrics.",
      "pass_criterion": "aggregate >= 1.5 * max(single_metric_scores)",
      "datasets": {
        "unanimous": [
          {
            "kind": "ZScore",
            "value": 0.3,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 120,
                "standard_error": 0.25,
                "interval": null,
                "method_ref": "s2.z.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.z.1"
          },
          {
            "kind": "EffectSize",
            "value": 0.25,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 140,
                "standard_error": 0.23,
                "interval": null,
                "method_ref": "s2.d.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.d.1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.1,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 150,
                "standard_error": 0.21,
                "interval": null,
                "method_ref": "s2.kl.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.kl.1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0003,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 160,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s2.abs.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s2.abs.1"
          },
          {
            "kind": "ZScore",
            "value": 0.34,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s2.z.2.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.z.2"
          },
          {
            "kind": "EffectSize",
            "value": 0.28,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 130,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s2.d.2.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.d.2"
          },
          {
            "kind": "KLDivergence",
            "value": 0.12,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 115,
                "standard_error": 0.25,
                "interval": null,
                "method_ref": "s2.kl.2.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.kl.2"
          },
          {
            "kind": "Custom",
            "value": 0.15,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 125,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s2.custom.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s2.custom.1"
          }
        ]
      }
    },
    {
      "idx": 3,
      "name": "Mixed signal",
      "what_it_tests": "Three contradiction + three agreement metrics.",
      "pass_criterion": "all_agreement <= mixed <= all_contradiction",
      "datasets": {
        "mixed": [
          {
            "kind": "ZScore",
            "value": 1.5,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s3.z.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.z.c1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.8,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 95,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s3.kl.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.kl.c1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0011,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s3.abs.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s3.abs.c1"
          },
          {
            "kind": "ZScore",
            "value": 1.4,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s3.z.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.z.a1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.7,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 95,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s3.kl.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.kl.a1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.001,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s3.abs.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s3.abs.a1"
          }
        ],
        "all_contradiction": [
          {
            "kind": "ZScore",
            "value": 1.5,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s3.z.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.z.c1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.8,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 95,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s3.kl.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.kl.c1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0011,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s3.abs.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s3.abs.c1"
          },
          {
            "kind": "ZScore",
            "value": 1.4,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s3.z.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.z.a1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.7,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 95,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s3.kl.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.kl.a1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.001,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s3.abs.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s3.abs.a1"
          }
        ],
        "all_agreement": [
          {
            "kind": "ZScore",
            "value": 1.5,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s3.z.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.z.c1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.8,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 95,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s3.kl.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.kl.c1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0011,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s3.abs.c1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s3.abs.c1"
          },
          {
            "kind": "ZScore",
            "value": 1.4,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.22,
                "interval": null,
                "method_ref": "s3.z.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.z.a1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.7,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 95,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s3.kl.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s3.kl.a1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.001,
            "direction": "Agreement",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s3.abs.a1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s3.abs.a1"
          }
        ]
      }
    },
    {
      "idx": 4,
      "name": "Missing data",
      "what_it_tests": "Partial uncertainty payloads and NoUncertainty variants.",
      "pass_criterion": "finite score and within 20% of full-uncertainty baseline",
      "datasets": {
        "missing": [
          {
            "kind": "ZScore",
            "value": 1.1,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "reason": "simulator omitted SE"
              },
              "distribution": null
            },
            "sample_size": 90,
            "units": null,
            "method_ref": "s4.z.1"
          },
          {
            "kind": "EffectSize",
            "value": 0.8,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 85,
                "standard_error": null,
                "interval": null,
                "method_ref": "s4.d.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s4.d.1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.4,
            "direction": "Contradiction",
            "uncertainty": null,
            "sample_size": 92,
            "units": null,
            "method_ref": "s4.kl.1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0008,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 80,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s4.abs.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s4.abs.1"
          }
        ],
        "baseline_full": [
          {
            "kind": "ZScore",
            "value": 1.1,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 90,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s4.z.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s4.z.1"
          },
          {
            "kind": "EffectSize",
            "value": 0.8,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 85,
                "standard_error": 0.21,
                "interval": null,
                "method_ref": "s4.d.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s4.d.1"
          },
          {
            "kind": "KLDivergence",
            "value": 0.4,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 92,
                "standard_error": 0.18,
                "interval": null,
                "method_ref": "s4.kl.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s4.kl.1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0008,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 80,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s4.abs.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s4.abs.1"
          }
        ]
      }
    },
    {
      "idx": 5,
      "name": "Scale heterogeneity",
      "what_it_tests": "Z=2.0, BF=100, AbsDiff=0.001eV normalization behavior.",
      "pass_criterion": "all normalized scores in [0.3, 0.99] (with tolerance) and stable ranking",
      "datasets": {
        "heterogeneous": [
          {
            "kind": "ZScore",
            "value": 2.0,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 120,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s5.z.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s5.z.1"
          },
          {
            "kind": "BayesFactor",
            "value": 100.0,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 120,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s5.bf.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s5.bf.1"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.001,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 120,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s5.abs.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": "eV",
            "method_ref": "s5.abs.1"
          }
        ]
      }
    },
    {
      "idx": 6,
      "name": "Calibration decomposability",
      "what_it_tests": "Per-component decomposition should reconstruct aggregate and expose dominant term.",
      "pass_criterion": "sum(w_i*u_i) ~= aggregate and one component clearly dominates",
      "datasets": {
        "calibration": [
          {
            "kind": "ZScore",
            "value": 3.0,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 220,
                "standard_error": 0.1,
                "interval": null,
                "method_ref": "s6.z.strong.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s6.z.strong"
          },
          {
            "kind": "EffectSize",
            "value": 0.9,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 140,
                "standard_error": 0.18,
                "interval": null,
                "method_ref": "s6.d.mid.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s6.d.mid"
          },
          {
            "kind": "KLDivergence",
            "value": 0.3,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 130,
                "standard_error": 0.24,
                "interval": null,
                "method_ref": "s6.kl.weak.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s6.kl.weak"
          },
          {
            "kind": "AbsoluteDifference",
            "value": 0.0009,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 110,
                "standard_error": 0.26,
                "interval": null,
                "method_ref": "s6.abs.mid.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s6.abs.mid"
          },
          {
            "kind": "BayesFactor",
            "value": 12.0,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 160,
                "standard_error": 0.16,
                "interval": null,
                "method_ref": "s6.bf.strong.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s6.bf.strong"
          },
          {
            "kind": "Custom",
            "value": 0.85,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 125,
                "standard_error": 0.25,
                "interval": null,
                "method_ref": "s6.custom.1.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s6.custom.1"
          }
        ]
      }
    },
    {
      "idx": 7,
      "name": "Boundary-seeking",
      "what_it_tests": "High contradiction near parameter bounds with inflated uncertainty.",
      "pass_criterion": "boundary_case < equivalent_non_boundary_case",
      "datasets": {
        "boundary": [
          {
            "kind": "ZScore",
            "value": 3.2,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 1.2,
                "interval": null,
                "method_ref": "s7.z.boundary.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s7.z.boundary"
          },
          {
            "kind": "KLDivergence",
            "value": 0.9,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s7.kl.boundary.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s7.kl.boundary"
          },
          {
            "kind": "EffectSize",
            "value": 1.0,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.25,
                "interval": null,
                "method_ref": "s7.d.boundary.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s7.d.boundary"
          }
        ],
        "non_boundary": [
          {
            "kind": "ZScore",
            "value": 3.2,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s7.z.boundary.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s7.z.boundary"
          },
          {
            "kind": "KLDivergence",
            "value": 0.9,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.2,
                "interval": null,
                "method_ref": "s7.kl.boundary.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s7.kl.boundary"
          },
          {
            "kind": "EffectSize",
            "value": 1.0,
            "direction": "Contradiction",
            "uncertainty": {
              "point": {
                "sample_size": 100,
                "standard_error": 0.25,
                "interval": null,
                "method_ref": "s7.d.boundary.unc"
              },
              "distribution": null
            },
            "sample_size": null,
            "units": null,
            "method_ref": "s7.d.boundary"
          }
        ]
      }
    }
  ],
  "matrix": {
    "IVW-CDF": {
      "Noisy TV": {
        "scenario_index": 1,
        "scenario_name": "Noisy TV",
        "candidate": "IVW-CDF",
        "passed": false,
        "raw_scores": {
          "base": 0.6456563062257953,
          "doubled": 0.8849332679544502
        },
        "score_summary": "base=0.6457, doubled=0.8849",
        "pass_reason": "doubled <= base",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "base": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.base",
              "kind": "AbsoluteDifference",
              "score": 0.6456563062257954,
              "weight": 1.0,
              "contribution": 0.6456563062257954,
              "diagnostics": {
                "raw_weight": 888.8888888790124,
                "sample_size": 80,
                "standard_error": 0.3,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.6456563062257954,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "doubled": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.doubled",
              "kind": "AbsoluteDifference",
              "score": 0.8849332679544502,
              "weight": 1.0,
              "contribution": 0.8849332679544502,
              "diagnostics": {
                "raw_weight": 222.22222222160497,
                "sample_size": 80,
                "standard_error": 0.6,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8849332679544502,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Unanimous weak signal": {
        "scenario_index": 2,
        "scenario_name": "Unanimous weak signal",
        "candidate": "IVW-CDF",
        "passed": false,
        "raw_scores": {
          "aggregate": 0.2697576939287771,
          "max_single": 0.5817593768418363,
          "threshold": 0.8726390652627545
        },
        "score_summary": "agg=0.2698, max1=0.5818, target=0.8726",
        "pass_reason": "aggregate >= 1.5 * max_single",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "aggregate": [
            {
              "index": 0,
              "method_ref": "s2.z.1",
              "kind": "ZScore",
              "score": 0.23582284437790513,
              "weight": 0.09469943869071112,
              "contribution": 0.022332290993034538,
              "diagnostics": {
                "raw_weight": 1919.9999999692798,
                "sample_size": 120,
                "standard_error": 0.25,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.23582284437790513,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s2.d.1",
              "kind": "EffectSize",
              "score": 0.1974126513658474,
              "weight": 0.13053246511362893,
              "contribution": 0.025768760027401465,
              "diagnostics": {
                "raw_weight": 2646.502835488724,
                "sample_size": 140,
                "standard_error": 0.23,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.1974126513658474,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s2.kl.1",
              "kind": "KLDivergence",
              "score": 0.09516258196404048,
              "weight": 0.16776402829166484,
              "contribution": 0.01596485809292316,
              "diagnostics": {
                "raw_weight": 3401.3605441405593,
                "sample_size": 150,
                "standard_error": 0.21,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.09516258196404048,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s2.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.1972904972705392,
              "contribution": 0.0754147118694953,
              "diagnostics": {
                "raw_weight": 3999.9999998999992,
                "sample_size": 160,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.382252125230751,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s2.z.2",
              "kind": "ZScore",
              "score": 0.26614347207205613,
              "weight": 0.08025870820998826,
              "contribution": 0.021360331267024312,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.26614347207205613,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s2.d.2",
              "kind": "EffectSize",
              "score": 0.2205224951115945,
              "weight": 0.11131842293995671,
              "contribution": 0.02454821637860701,
              "diagnostics": {
                "raw_weight": 2256.9444444052615,
                "sample_size": 130,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.2205224951115945,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 6,
              "method_ref": "s2.kl.2",
              "kind": "KLDivergence",
              "score": 0.11307956328284252,
              "weight": 0.09075362874526484,
              "contribution": 0.010262380704847772,
              "diagnostics": {
                "raw_weight": 1839.9999999705599,
                "sample_size": 115,
                "standard_error": 0.25,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.11307956328284252,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 7,
              "method_ref": "s2.custom.1",
              "kind": "Custom",
              "score": 0.5817593768418363,
              "weight": 0.12738281073824592,
              "contribution": 0.07410614459544353,
              "diagnostics": {
                "raw_weight": 2582.644628045813,
                "sample_size": 125,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5817593768418363,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Mixed signal": {
        "scenario_index": 3,
        "scenario_name": "Mixed signal",
        "candidate": "IVW-CDF",
        "passed": true,
        "raw_scores": {
          "mixed": 0.5170604622769124,
          "all_contradiction": 0.6761340241864792,
          "all_agreement": 0.3238659758135209
        },
        "score_summary": "mixed=0.5171, allC=0.6761, allA=0.3239",
        "pass_reason": "all_agreement <= mixed <= all_contradiction",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "mixed": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.19336092529695567,
              "contribution": 0.16752512078926296,
              "diagnostics": {
                "raw_weight": 2066.1157024366507,
                "sample_size": 100,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.15435304418721113,
              "contribution": 0.08499775073423183,
              "diagnostics": {
                "raw_weight": 1649.3055555269218,
                "sample_size": 95,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.15228603051583325,
              "contribution": 0.09407437170820099,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.19336092529695567,
              "contribution": 0.031230364706665827,
              "diagnostics": {
                "raw_weight": 2066.1157024366507,
                "sample_size": 100,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.15435304418721113,
              "contribution": 0.07664945333883509,
              "diagnostics": {
                "raw_weight": 1649.3055555269218,
                "sample_size": 95,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.15228603051583325,
              "contribution": 0.06258340099971571,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement"
              }
            }
          ],
          "all_contradiction": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.19336092529695567,
              "contribution": 0.16752512078926296,
              "diagnostics": {
                "raw_weight": 2066.1157024366507,
                "sample_size": 100,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.15435304418721113,
              "contribution": 0.08499775073423183,
              "diagnostics": {
                "raw_weight": 1649.3055555269218,
                "sample_size": 95,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.15228603051583325,
              "contribution": 0.09407437170820099,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.8384866815324579,
              "weight": 0.19336092529695567,
              "contribution": 0.16213056059028985,
              "diagnostics": {
                "raw_weight": 2066.1157024366507,
                "sample_size": 100,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8384866815324579,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.5034146962085905,
              "weight": 0.15435304418721113,
              "contribution": 0.07770359084837604,
              "diagnostics": {
                "raw_weight": 1649.3055555269218,
                "sample_size": 95,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5034146962085905,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.15228603051583325,
              "contribution": 0.08970262951611754,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "all_agreement": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.13361440253771617,
              "weight": 0.19336092529695567,
              "contribution": 0.0258358045076927,
              "diagnostics": {
                "raw_weight": 2066.1157024366507,
                "sample_size": 100,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8663855974622838,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.44932896411722156,
              "weight": 0.15435304418721113,
              "contribution": 0.0693552934529793,
              "diagnostics": {
                "raw_weight": 1649.3055555269218,
                "sample_size": 95,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5506710358827784,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.15228603051583325,
              "contribution": 0.058211658807632266,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.617747874769249,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.19336092529695567,
              "contribution": 0.031230364706665827,
              "diagnostics": {
                "raw_weight": 2066.1157024366507,
                "sample_size": 100,
                "standard_error": 0.22,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.15435304418721113,
              "contribution": 0.07664945333883509,
              "diagnostics": {
                "raw_weight": 1649.3055555269218,
                "sample_size": 95,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.15228603051583325,
              "contribution": 0.06258340099971571,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement"
              }
            }
          ]
        }
      },
      "Missing data": {
        "scenario_index": 4,
        "scenario_name": "Missing data",
        "candidate": "IVW-CDF",
        "passed": true,
        "raw_scores": {
          "missing": 0.52999619864064,
          "baseline_full": 0.526117260618077,
          "relative_delta": 0.007372763284759124
        },
        "score_summary": "missing=0.5300, baseline=0.5261, delta=0.007",
        "pass_reason": "finite and <=20% delta from baseline",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "missing": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.000718448152004144,
              "contribution": 0.0005235100904509236,
              "diagnostics": {
                "raw_weight": 1.0,
                "sample_size": 90,
                "standard_error": null,
                "weight_source": "component.sample_size",
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.000718448152004144,
              "contribution": 0.00041403391279545864,
              "diagnostics": {
                "raw_weight": 1.0,
                "sample_size": 85,
                "standard_error": null,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.000718448152004144,
              "contribution": 0.0002368579536785062,
              "diagnostics": {
                "raw_weight": 1.0,
                "sample_size": 92,
                "standard_error": null,
                "weight_source": "component.sample_size",
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.9978446555439876,
              "contribution": 0.5288217966837151,
              "diagnostics": {
                "raw_weight": 1388.8888888647764,
                "sample_size": 80,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "baseline_full": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.26767128009536556,
              "contribution": 0.1950434636973373,
              "diagnostics": {
                "raw_weight": 2249.9999999437496,
                "sample_size": 90,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.22929764482901452,
              "contribution": 0.13214175695004454,
              "diagnostics": {
                "raw_weight": 1927.4376416796504,
                "sample_size": 85,
                "standard_error": 0.21,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.33780188983030496,
              "contribution": 0.11136651148832898,
              "diagnostics": {
                "raw_weight": 2839.5061727518673,
                "sample_size": 92,
                "standard_error": 0.18,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.16522918524531505,
              "contribution": 0.08756552848236615,
              "diagnostics": {
                "raw_weight": 1388.8888888647764,
                "sample_size": 80,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Scale heterogeneity": {
        "scenario_index": 5,
        "scenario_name": "Scale heterogeneity",
        "candidate": "IVW-CDF",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.8445463933544324,
          "s5.z.1": 0.9544997361036416,
          "s5.bf.1": 0.9900990099009901,
          "s5.abs.1": 0.5890404340586651
        },
        "score_summary": "agg=0.8445, scores=[0.9545, 0.9901, 0.589]",
        "pass_reason": "component scores in [0.3, 0.99] with tolerance; ranking checked post-pass",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "heterogeneous": [
            {
              "index": 0,
              "method_ref": "s5.z.1",
              "kind": "ZScore",
              "score": 0.9544997361036416,
              "weight": 0.3333333333333333,
              "contribution": 0.31816657870121384,
              "diagnostics": {
                "raw_weight": 2999.9999999249994,
                "sample_size": 120,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.9544997361036416,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s5.bf.1",
              "kind": "BayesFactor",
              "score": 0.9900990099009901,
              "weight": 0.3333333333333333,
              "contribution": 0.33003300330033003,
              "diagnostics": {
                "raw_weight": 2999.9999999249994,
                "sample_size": 120,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.9900990099009901,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s5.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.3333333333333333,
              "contribution": 0.19634681135288837,
              "diagnostics": {
                "raw_weight": 2999.9999999249994,
                "sample_size": 120,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Calibration decomposability": {
        "scenario_index": 6,
        "scenario_name": "Calibration decomposability",
        "candidate": "IVW-CDF",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.8683905880900508,
          "reconstructed": 0.868390588090051,
          "dominant_share": 0.6570205591339194
        },
        "score_summary": "agg=0.8684, recon=0.8684, dom_share=0.657",
        "pass_reason": "sum(weight*score) reconstructs aggregate and dominant component is identifiable",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "calibration": [
            {
              "index": 0,
              "method_ref": "s6.z.strong",
              "kind": "ZScore",
              "score": 0.9973002039367398,
              "weight": 0.5720950095882554,
              "contribution": 0.5705504697335583,
              "diagnostics": {
                "raw_weight": 21999.999997799994,
                "sample_size": 220,
                "standard_error": 0.1,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.9973002039367398,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s6.d.mid",
              "kind": "EffectSize",
              "score": 0.6318797493064809,
              "weight": 0.11236433971365475,
              "contribution": 0.07100075080925243,
              "diagnostics": {
                "raw_weight": 4320.987654187625,
                "sample_size": 140,
                "standard_error": 0.18,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.6318797493064809,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s6.kl.weak",
              "kind": "KLDivergence",
              "score": 0.2591817793182821,
              "weight": 0.058690302440513945,
              "contribution": 0.01521145701526052,
              "diagnostics": {
                "raw_weight": 2256.9444444052615,
                "sample_size": 130,
                "standard_error": 0.24,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.2591817793182821,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s6.abs.mid",
              "kind": "AbsoluteDifference",
              "score": 0.5597136492671929,
              "weight": 0.04231471964770724,
              "contribution": 0.023684126151736408,
              "diagnostics": {
                "raw_weight": 1627.218934887171,
                "sample_size": 110,
                "standard_error": 0.26,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5597136492671929,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s6.bf.strong",
              "kind": "BayesFactor",
              "score": 0.9230769230769231,
              "weight": 0.16252699137020388,
              "contribution": 0.15002491511095745,
              "diagnostics": {
                "raw_weight": 6249.99999975586,
                "sample_size": 160,
                "standard_error": 0.16,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.9230769230769231,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s6.custom.1",
              "kind": "Custom",
              "score": 0.7290879223493065,
              "weight": 0.05200863723966468,
              "contribution": 0.03791886926928589,
              "diagnostics": {
                "raw_weight": 1999.9999999679999,
                "sample_size": 125,
                "standard_error": 0.25,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.7290879223493065,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Boundary-seeking": {
        "scenario_index": 7,
        "scenario_name": "Boundary-seeking",
        "candidate": "IVW-CDF",
        "passed": true,
        "raw_scores": {
          "boundary": 0.6344317766875402,
          "non_boundary": 0.7685520224813673
        },
        "score_summary": "boundary=0.6344, non_boundary=0.7686",
        "pass_reason": "boundary < non_boundary for same values with lower uncertainty comparator",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.01665556295836854,
              "contribution": 0.016632673619996458,
              "diagnostics": {
                "raw_weight": 69.44444444439621,
                "sample_size": 100,
                "standard_error": 1.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.5996002664866938,
              "contribution": 0.3558209901608262,
              "diagnostics": {
                "raw_weight": 2499.9999999374995,
                "sample_size": 100,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.38374417055493776,
              "contribution": 0.2619781129067177,
              "diagnostics": {
                "raw_weight": 1599.9999999743998,
                "sample_size": 100,
                "standard_error": 0.25,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "non_boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.37878787878705233,
              "contribution": 0.37826731974317784,
              "diagnostics": {
                "raw_weight": 2499.9999999374995,
                "sample_size": 100,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.37878787878705233,
              "contribution": 0.22478421979473717,
              "diagnostics": {
                "raw_weight": 2499.9999999374995,
                "sample_size": 100,
                "standard_error": 0.2,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.24242424242589533,
              "contribution": 0.16550048294345227,
              "diagnostics": {
                "raw_weight": 1599.9999999743998,
                "sample_size": 100,
                "standard_error": 0.25,
                "weight_source": "uncertainty.point.sample_size",
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      }
    },
    "HTG-Max": {
      "Noisy TV": {
        "scenario_index": 1,
        "scenario_name": "Noisy TV",
        "candidate": "HTG-Max",
        "passed": true,
        "raw_scores": {
          "base": 0.11650358113896905,
          "doubled": 0.02381889131247231
        },
        "score_summary": "base=0.1165, doubled=0.0238",
        "pass_reason": "doubled <= base",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "base": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.base",
              "kind": "AbsoluteDifference",
              "score": 0.6456563062257954,
              "weight": 0.18044210211465983,
              "contribution": 0.11650358113896905,
              "diagnostics": {
                "confidence": 0.18044210211465983,
                "gated_score": 0.11650358113896905,
                "precision": 6.791096610976364,
                "winner_gate": 1.0,
                "raw_score": 0.6456563062257954,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ],
          "doubled": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.doubled",
              "kind": "AbsoluteDifference",
              "score": 0.8849332679544502,
              "weight": 0.026916031044386425,
              "contribution": 0.02381889131247231,
              "diagnostics": {
                "confidence": 0.026916031044386425,
                "gated_score": 0.02381889131247231,
                "precision": 5.40816778747595,
                "winner_gate": 1.0,
                "raw_score": 0.8849332679544502,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ]
        }
      },
      "Unanimous weak signal": {
        "scenario_index": 2,
        "scenario_name": "Unanimous weak signal",
        "candidate": "HTG-Max",
        "passed": false,
        "raw_scores": {
          "aggregate": 0.303297715139847,
          "max_single": 0.303297715139847,
          "threshold": 0.45494657270977046
        },
        "score_summary": "agg=0.3033, max1=0.3033, target=0.4549",
        "pass_reason": "aggregate >= 1.5 * max_single",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "aggregate": [
            {
              "index": 0,
              "method_ref": "s2.z.1",
              "kind": "ZScore",
              "score": 0.23582284437790513,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.4111778702799699,
                "gated_score": 0.0969651349146718,
                "precision": 7.560601162752564,
                "winner_gate": 0.0,
                "raw_score": 0.23582284437790513,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s2.d.1",
              "kind": "EffectSize",
              "score": 0.1974126513658474,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.5304767276821589,
                "gated_score": 0.10472281729961361,
                "precision": 7.881372148481116,
                "winner_gate": 0.0,
                "raw_score": 0.1974126513658474,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s2.kl.1",
              "kind": "KLDivergence",
              "score": 0.09516258196404048,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.6220684689819068,
                "gated_score": 0.059197641666735884,
                "precision": 8.132224747393392,
                "winner_gate": 0.0,
                "raw_score": 0.09516258196404048,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s2.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.6773127216648894,
                "gated_score": 0.2589042273022281,
                "precision": 8.29429960883224,
                "winner_gate": 0.0,
                "raw_score": 0.382252125230751,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 4,
              "method_ref": "s2.z.2",
              "kind": "ZScore",
              "score": 0.26614347207205613,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.09387214789512971,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.26614347207205613,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 5,
              "method_ref": "s2.d.2",
              "kind": "EffectSize",
              "score": 0.2205224951115945,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.47086185567086464,
                "gated_score": 0.10383563126541456,
                "precision": 7.7222101405120025,
                "winner_gate": 0.0,
                "raw_score": 0.2205224951115945,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 6,
              "method_ref": "s2.kl.2",
              "kind": "KLDivergence",
              "score": 0.11307956328284252,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3958221227013417,
                "gated_score": 0.044759392772755424,
                "precision": 7.5180641812170865,
                "winner_gate": 0.0,
                "raw_score": 0.11307956328284252,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 7,
              "method_ref": "s2.custom.1",
              "kind": "Custom",
              "score": 0.5817593768418363,
              "weight": 0.5213456408495586,
              "contribution": 0.303297715139847,
              "diagnostics": {
                "confidence": 0.5213456408495586,
                "gated_score": 0.303297715139847,
                "precision": 7.856956327598623,
                "winner_gate": 1.0,
                "raw_score": 0.5817593768418363,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ]
        }
      },
      "Mixed signal": {
        "scenario_index": 3,
        "scenario_name": "Mixed signal",
        "candidate": "HTG-Max",
        "passed": true,
        "raw_scores": {
          "mixed": 0.37950829341692455,
          "all_contradiction": 0.37950829341692455,
          "all_agreement": 0.17744998699480144
        },
        "score_summary": "mixed=0.3795, allC=0.3795, allA=0.1774",
        "pass_reason": "all_agreement <= mixed <= all_contradiction",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "mixed": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.43803624451807166,
              "contribution": 0.37950829341692455,
              "diagnostics": {
                "confidence": 0.43803624451807166,
                "gated_score": 0.37950829341692455,
                "precision": 7.633909534136771,
                "winner_gate": 1.0,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3573403917513822,
                "gated_score": 0.1967770036884915,
                "precision": 7.408715734917801,
                "winner_gate": 0.0,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.21788744022449996,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.43803624451807166,
                "gated_score": 0.07074868746117347,
                "precision": 7.633909534136771,
                "winner_gate": 0.0,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3573403917513822,
                "gated_score": 0.17744998699480144,
                "precision": 7.408715734917801,
                "winner_gate": 0.0,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.144950604471406,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            }
          ],
          "all_contradiction": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.43803624451807166,
              "contribution": 0.37950829341692455,
              "diagnostics": {
                "confidence": 0.43803624451807166,
                "gated_score": 0.37950829341692455,
                "precision": 7.633909534136771,
                "winner_gate": 1.0,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3573403917513822,
                "gated_score": 0.1967770036884915,
                "precision": 7.408715734917801,
                "winner_gate": 0.0,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.21788744022449996,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.8384866815324579,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.43803624451807166,
                "gated_score": 0.3672875570568982,
                "precision": 7.633909534136771,
                "winner_gate": 0.0,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.5034146962085905,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3573403917513822,
                "gated_score": 0.17989040475658077,
                "precision": 7.408715734917801,
                "winner_gate": 0.0,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.20776196504716782,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ],
          "all_agreement": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.13361440253771617,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.43803624451807166,
                "gated_score": 0.05852795110114709,
                "precision": 7.633909534136771,
                "winner_gate": 0.0,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.44932896411722156,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3573403917513822,
                "gated_score": 0.16056338806289072,
                "precision": 7.408715734917801,
                "winner_gate": 0.0,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.13482512929407386,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.617747874769249,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.43803624451807166,
                "gated_score": 0.07074868746117347,
                "precision": 7.633909534136771,
                "winner_gate": 0.0,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.3573403917513822,
              "contribution": 0.17744998699480144,
              "diagnostics": {
                "confidence": 0.3573403917513822,
                "gated_score": 0.17744998699480144,
                "precision": 7.408715734917801,
                "winner_gate": 1.0,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.144950604471406,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement",
                "mode": "hard_max"
              }
            }
          ]
        }
      },
      "Missing data": {
        "scenario_index": 4,
        "scenario_name": "Missing data",
        "candidate": "HTG-Max",
        "passed": false,
        "raw_scores": {
          "missing": 0.15929727530976898,
          "baseline_full": 0.34226318888641766,
          "relative_delta": 0.5345766635668411
        },
        "score_summary": "missing=0.1593, baseline=0.3423, delta=0.535",
        "pass_reason": "finite and <=20% delta from baseline",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "missing": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.15,
                "gated_score": 0.1093001817160852,
                "precision": null,
                "winner_gate": 0.0,
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.15,
                "gated_score": 0.08644338042498101,
                "precision": null,
                "winner_gate": 0.0,
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.15,
                "gated_score": 0.0494519930946541,
                "precision": null,
                "winner_gate": 0.0,
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.30058128429536257,
              "contribution": 0.15929727530976898,
              "diagnostics": {
                "confidence": 0.30058128429536257,
                "gated_score": 0.15929727530976898,
                "precision": 7.236979086861173,
                "winner_gate": 1.0,
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ],
          "baseline_full": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.4697108232291919,
              "contribution": 0.34226318888641766,
              "diagnostics": {
                "confidence": 0.4697108232291919,
                "gated_score": 0.34226318888641766,
                "precision": 7.7191298408817435,
                "winner_gate": 1.0,
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.412581963861255,
                "gated_score": 0.23776653105696155,
                "precision": 7.564465441984008,
                "winner_gate": 0.0,
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.5566571982151446,
                "gated_score": 0.18351871948149887,
                "precision": 7.951737545116407,
                "winner_gate": 0.0,
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.30058128429536257,
                "gated_score": 0.15929727530976898,
                "precision": 7.236979086861173,
                "winner_gate": 0.0,
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ]
        }
      },
      "Scale heterogeneity": {
        "scenario_index": 5,
        "scenario_name": "Scale heterogeneity",
        "candidate": "HTG-Max",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.5711859158910918,
          "s5.z.1": 0.9544997361036416,
          "s5.bf.1": 0.9900990099009901,
          "s5.abs.1": 0.5890404340586651
        },
        "score_summary": "agg=0.5712, scores=[0.9545, 0.9901, 0.589]",
        "pass_reason": "component scores in [0.3, 0.99] with tolerance; ranking checked post-pass",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "heterogeneous": [
            {
              "index": 0,
              "method_ref": "s5.z.1",
              "kind": "ZScore",
              "score": 0.9544997361036416,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.5768977750500027,
                "gated_score": 0.5506487740440056,
                "precision": 8.006700845415375,
                "winner_gate": 0.0,
                "raw_score": 0.9544997361036416,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s5.bf.1",
              "kind": "BayesFactor",
              "score": 0.9900990099009901,
              "weight": 0.5768977750500027,
              "contribution": 0.5711859158910918,
              "diagnostics": {
                "confidence": 0.5768977750500027,
                "gated_score": 0.5711859158910918,
                "precision": 8.006700845415375,
                "winner_gate": 1.0,
                "raw_score": 0.9900990099009901,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s5.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.5768977750500027,
                "gated_score": 0.33981611582293175,
                "precision": 8.006700845415375,
                "winner_gate": 0.0,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ]
        }
      },
      "Calibration decomposability": {
        "scenario_index": 6,
        "scenario_name": "Calibration decomposability",
        "candidate": "HTG-Max",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.9617656341721981,
          "reconstructed": 0.9617656341721981,
          "dominant_share": 1.0
        },
        "score_summary": "agg=0.9618, recon=0.9618, dom_share=1.000",
        "pass_reason": "sum(weight*score) reconstructs aggregate and dominant component is identifiable",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "calibration": [
            {
              "index": 0,
              "method_ref": "s6.z.strong",
              "kind": "ZScore",
              "score": 0.9973002039367398,
              "weight": 0.9643692344348546,
              "contribution": 0.9617656341721981,
              "diagnostics": {
                "confidence": 0.9643692344348546,
                "gated_score": 0.9617656341721981,
                "precision": 9.998843185752886,
                "winner_gate": 1.0,
                "raw_score": 0.9973002039367398,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s6.d.mid",
              "kind": "EffectSize",
              "score": 0.6318797493064809,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.7020765816852114,
                "gated_score": 0.44362797442920243,
                "precision": 8.371470680558268,
                "winner_gate": 0.0,
                "raw_score": 0.6318797493064809,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s6.kl.weak",
              "kind": "KLDivergence",
              "score": 0.2591817793182821,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.47086185567086464,
                "gated_score": 0.12203881356588285,
                "precision": 7.7222101405120025,
                "winner_gate": 0.0,
                "raw_score": 0.2591817793182821,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 3,
              "method_ref": "s6.abs.mid",
              "kind": "AbsoluteDifference",
              "score": 0.5597136492671929,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3527125695185738,
                "gated_score": 0.19741803942764943,
                "precision": 7.395242018409667,
                "winner_gate": 0.0,
                "raw_score": 0.5597136492671929,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 4,
              "method_ref": "s6.bf.strong",
              "kind": "BayesFactor",
              "score": 0.9230769230769231,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.803883438051732,
                "gated_score": 0.7420462505092912,
                "precision": 8.740496729892756,
                "winner_gate": 0.0,
                "raw_score": 0.9230769230769231,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 5,
              "method_ref": "s6.custom.1",
              "kind": "Custom",
              "score": 0.7290879223493065,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.42607178204403234,
                "gated_score": 0.3106437903421501,
                "precision": 7.601402334567742,
                "winner_gate": 0.0,
                "raw_score": 0.7290879223493065,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ]
        }
      },
      "Boundary-seeking": {
        "scenario_index": 7,
        "scenario_name": "Boundary-seeking",
        "candidate": "HTG-Max",
        "passed": true,
        "raw_scores": {
          "boundary": 0.3021546695427816,
          "non_boundary": 0.5084664621929549
        },
        "score_summary": "boundary=0.3022, non_boundary=0.5085",
        "pass_reason": "boundary < non_boundary for same values with lower uncertainty comparator",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.0048797799162317015,
                "gated_score": 0.0048730737524134565,
                "precision": 4.254824377100322,
                "winner_gate": 0.0,
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.5091661970143007,
              "contribution": 0.3021546695427816,
              "diagnostics": {
                "confidence": 0.5091661970143007,
                "gated_score": 0.3021546695427816,
                "precision": 7.824445930852629,
                "winner_gate": 1.0,
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3469610089756286,
                "gated_score": 0.23686663500894278,
                "precision": 7.378383712980725,
                "winner_gate": 0.0,
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ],
          "non_boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.5091661970143007,
              "contribution": 0.5084664621929549,
              "diagnostics": {
                "confidence": 0.5091661970143007,
                "gated_score": 0.5084664621929549,
                "precision": 7.824445930852629,
                "winner_gate": 1.0,
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.5091661970143007,
                "gated_score": 0.3021546695427816,
                "precision": 7.824445930852629,
                "winner_gate": 0.0,
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.0,
              "contribution": 0.0,
              "diagnostics": {
                "confidence": 0.3469610089756286,
                "gated_score": 0.23686663500894278,
                "precision": 7.378383712980725,
                "winner_gate": 0.0,
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction",
                "mode": "hard_max"
              }
            }
          ]
        }
      }
    },
    "Fisher-UP": {
      "Noisy TV": {
        "scenario_index": 1,
        "scenario_name": "Noisy TV",
        "candidate": "Fisher-UP",
        "passed": false,
        "raw_scores": {
          "base": 0.5639465103125809,
          "doubled": 0.822679142319085
        },
        "score_summary": "base=0.5639, doubled=0.8227",
        "pass_reason": "doubled <= base",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "base": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.base",
              "kind": "AbsoluteDifference",
              "score": 0.6456563062257954,
              "weight": 0.8734469173067452,
              "contribution": 0.5639465103125808,
              "diagnostics": {
                "p_value": 0.3543436937742046,
                "p_adj": 0.4360534896874191,
                "log_evidence": 1.6599807207774169,
                "reliability": 0.8,
                "sample_size": 80,
                "standard_error": 0.3,
                "raw_score": 0.6456563062257954,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "doubled": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.doubled",
              "kind": "AbsoluteDifference",
              "score": 0.8849332679544502,
              "weight": 0.9296510506614046,
              "contribution": 0.8226791423190849,
              "diagnostics": {
                "p_value": 0.11506673204554985,
                "p_adj": 0.17732085768091504,
                "log_evidence": 3.4595888644135826,
                "reliability": 0.8,
                "sample_size": 80,
                "standard_error": 0.6,
                "raw_score": 0.8849332679544502,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Unanimous weak signal": {
        "scenario_index": 2,
        "scenario_name": "Unanimous weak signal",
        "candidate": "Fisher-UP",
        "passed": false,
        "raw_scores": {
          "aggregate": 0.005584526686567615,
          "max_single": 0.5817593768418363,
          "threshold": 0.8726390652627545
        },
        "score_summary": "agg=0.0056, max1=0.5818, target=0.8726",
        "pass_reason": "aggregate >= 1.5 * max_single",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "aggregate": [
            {
              "index": 0,
              "method_ref": "s2.z.1",
              "kind": "ZScore",
              "score": 0.23582284437790513,
              "weight": 0.0015670858425946323,
              "contribution": 0.0003695546407850123,
              "diagnostics": {
                "p_value": 0.7641771556220949,
                "p_adj": 0.7641771556220949,
                "log_evidence": 0.5379112752178103,
                "reliability": 1.0,
                "sample_size": 120,
                "standard_error": 0.25,
                "raw_score": 0.23582284437790513,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s2.d.1",
              "kind": "EffectSize",
              "score": 0.1974126513658474,
              "weight": 0.001281345260593687,
              "contribution": 0.0002529537652088624,
              "diagnostics": {
                "p_value": 0.8025873486341526,
                "p_adj": 0.8025873486341526,
                "log_evidence": 0.43982916850237935,
                "reliability": 1.0,
                "sample_size": 140,
                "standard_error": 0.23,
                "raw_score": 0.1974126513658474,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s2.kl.1",
              "kind": "KLDivergence",
              "score": 0.09516258196404048,
              "weight": 0.000582655882035598,
              "contribution": 5.5447038131042906e-05,
              "diagnostics": {
                "p_value": 0.9048374180359595,
                "p_adj": 0.9048374180359595,
                "log_evidence": 0.20000000000000012,
                "reliability": 1.0,
                "sample_size": 150,
                "standard_error": 0.21,
                "raw_score": 0.09516258196404048,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s2.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.002806506987954376,
              "contribution": 0.0010727932606205142,
              "diagnostics": {
                "p_value": 0.617747874769249,
                "p_adj": 0.617747874769249,
                "log_evidence": 0.9633497487914867,
                "reliability": 1.0,
                "sample_size": 160,
                "standard_error": 0.2,
                "raw_score": 0.382252125230751,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s2.z.2",
              "kind": "ZScore",
              "score": 0.26614347207205613,
              "weight": 0.0018029804734326614,
              "contribution": 0.00047985148327748807,
              "diagnostics": {
                "p_value": 0.7338565279279439,
                "p_adj": 0.7338565279279439,
                "log_evidence": 0.6188834710236417,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.26614347207205613,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s2.d.2",
              "kind": "EffectSize",
              "score": 0.2205224951115945,
              "weight": 0.0014515790432674028,
              "contribution": 0.0003201058324730289,
              "diagnostics": {
                "p_value": 0.7794775048884055,
                "p_adj": 0.7794775048884055,
                "log_evidence": 0.49826289857268363,
                "reliability": 1.0,
                "sample_size": 130,
                "standard_error": 0.24,
                "raw_score": 0.2205224951115945,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 6,
              "method_ref": "s2.kl.2",
              "kind": "KLDivergence",
              "score": 0.11307956328284252,
              "weight": 0.0006991870584427176,
              "contribution": 7.90637672217178e-05,
              "diagnostics": {
                "p_value": 0.8869204367171575,
                "p_adj": 0.8869204367171575,
                "log_evidence": 0.24000000000000007,
                "reliability": 1.0,
                "sample_size": 115,
                "standard_error": 0.25,
                "raw_score": 0.11307956328284252,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 7,
              "method_ref": "s2.custom.1",
              "kind": "Custom",
              "score": 0.5817593768418363,
              "weight": 0.005079001759954892,
              "contribution": 0.002954756898849948,
              "diagnostics": {
                "p_value": 0.4182406231581637,
                "p_adj": 0.4182406231581637,
                "log_evidence": 1.7433967171877232,
                "reliability": 1.0,
                "sample_size": 125,
                "standard_error": 0.22,
                "raw_score": 0.5817593768418363,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Mixed signal": {
        "scenario_index": 3,
        "scenario_name": "Mixed signal",
        "candidate": "Fisher-UP",
        "passed": true,
        "raw_scores": {
          "mixed": 0.4001625920258801,
          "all_contradiction": 0.7133542540429578,
          "all_agreement": 0.04539771174825702
        },
        "score_summary": "mixed=0.4002, allC=0.7134, allA=0.0454",
        "pass_reason": "all_agreement <= mixed <= all_contradiction",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "mixed": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.24215329540315056,
              "contribution": 0.2097981275153195,
              "diagnostics": {
                "p_value": 0.13361440253771617,
                "p_adj": 0.13361440253771617,
                "log_evidence": 4.025594440527889,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.22,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.09143320680970592,
              "contribution": 0.050349618707985065,
              "diagnostics": {
                "p_value": 0.44932896411722156,
                "p_adj": 0.46766642700990924,
                "log_evidence": 1.52,
                "reliability": 0.95,
                "sample_size": 95,
                "standard_error": 0.24,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.11569607588726837,
              "contribution": 0.07147100499850179,
              "diagnostics": {
                "p_value": 0.382252125230751,
                "p_adj": 0.382252125230751,
                "log_evidence": 1.9233497487914866,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.021192843617988257,
              "contribution": 0.0034229265005049554,
              "diagnostics": {
                "p_value": 0.8384866815324579,
                "p_adj": 0.8384866815324579,
                "log_evidence": 0.3523131630544826,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.22,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.07844294856470002,
              "contribution": 0.038953615443295474,
              "diagnostics": {
                "p_value": 0.5034146962085905,
                "p_adj": 0.5209902493474232,
                "log_evidence": 1.3040479053359315,
                "reliability": 0.95,
                "sample_size": 95,
                "standard_error": 0.24,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.06367365801629447,
              "contribution": 0.026167298860273375,
              "diagnostics": {
                "p_value": 0.5890404340586651,
                "p_adj": 0.5890404340586651,
                "log_evidence": 1.0585208980605685,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement"
              }
            }
          ],
          "all_contradiction": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.2791435624294745,
              "contribution": 0.2418459621132106,
              "diagnostics": {
                "p_value": 0.13361440253771617,
                "p_adj": 0.13361440253771617,
                "log_evidence": 4.025594440527889,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.22,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.1054001393238117,
              "contribution": 0.05804080390363256,
              "diagnostics": {
                "p_value": 0.44932896411722156,
                "p_adj": 0.46766642700990924,
                "log_evidence": 1.52,
                "reliability": 0.95,
                "sample_size": 95,
                "standard_error": 0.24,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.13336929703357955,
              "contribution": 0.08238859980196248,
              "diagnostics": {
                "p_value": 0.382252125230751,
                "p_adj": 0.382252125230751,
                "log_evidence": 1.9233497487914866,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.8384866815324579,
              "weight": 0.2528449035323962,
              "contribution": 0.21200708410527333,
              "diagnostics": {
                "p_value": 0.16151331846754213,
                "p_adj": 0.16151331846754213,
                "log_evidence": 3.646335344856767,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.22,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.5034146962085905,
              "weight": 0.09222512190833523,
              "contribution": 0.0464274817282848,
              "diagnostics": {
                "p_value": 0.4965853037914095,
                "p_adj": 0.514273527706632,
                "log_evidence": 1.3299999999999998,
                "reliability": 0.95,
                "sample_size": 95,
                "standard_error": 0.24,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.12332654634598333,
              "contribution": 0.07264432239059411,
              "diagnostics": {
                "p_value": 0.41095956594133487,
                "p_adj": 0.41095956594133487,
                "log_evidence": 1.7785208980605687,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "all_agreement": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.13361440253771617,
              "weight": 0.006335675254559349,
              "contribution": 0.0008465374638109402,
              "diagnostics": {
                "p_value": 0.8663855974622838,
                "p_adj": 0.8663855974622838,
                "log_evidence": 0.2868504137223543,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.22,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.44932896411722156,
              "weight": 0.025037280069063837,
              "contribution": 0.011249975117745212,
              "diagnostics": {
                "p_value": 0.5506710358827784,
                "p_adj": 0.5673455103023971,
                "log_evidence": 1.1335735904590531,
                "reliability": 0.95,
                "sample_size": 95,
                "standard_error": 0.24,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.021277540045006888,
              "contribution": 0.008133384901886294,
              "diagnostics": {
                "p_value": 0.617747874769249,
                "p_adj": 0.617747874769249,
                "log_evidence": 0.9633497487914867,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.617747874769249,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.007781553319216522,
              "contribution": 0.0012568244994187777,
              "diagnostics": {
                "p_value": 0.8384866815324579,
                "p_adj": 0.8384866815324579,
                "log_evidence": 0.3523131630544826,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.22,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.02880255230377223,
              "contribution": 0.014302924185736697,
              "diagnostics": {
                "p_value": 0.5034146962085905,
                "p_adj": 0.5209902493474232,
                "log_evidence": 1.3040479053359315,
                "reliability": 0.95,
                "sample_size": 95,
                "standard_error": 0.24,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.023379588592009233,
              "contribution": 0.0096080655796591,
              "diagnostics": {
                "p_value": 0.5890404340586651,
                "p_adj": 0.5890404340586651,
                "log_evidence": 1.0585208980605685,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement"
              }
            }
          ]
        }
      },
      "Missing data": {
        "scenario_index": 4,
        "scenario_name": "Missing data",
        "candidate": "Fisher-UP",
        "passed": false,
        "raw_scores": {
          "missing": 0.06618287735710049,
          "baseline_full": 0.3249722391109765,
          "relative_delta": 0.7963429813630962
        },
        "score_summary": "missing=0.0662, baseline=0.3250, delta=0.796",
        "pass_reason": "finite and <=20% delta from baseline",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "missing": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.01016907621412361,
              "contribution": 0.007409879187256202,
              "diagnostics": {
                "p_value": 0.27133212189276534,
                "p_adj": 0.8777081299458058,
                "log_evidence": 0.26088233328527033,
                "reliability": 0.1,
                "sample_size": 90,
                "standard_error": null,
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.056902232994903934,
              "contribution": 0.03279214249206258,
              "diagnostics": {
                "p_value": 0.4237107971667933,
                "p_adj": 0.4819578977400822,
                "log_evidence": 1.4597970356673136,
                "reliability": 0.85,
                "sample_size": 85,
                "standard_error": null,
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.003118364079641655,
              "contribution": 0.0010280621262203768,
              "diagnostics": {
                "p_value": 0.6703200460356393,
                "p_adj": 0.9607894391523232,
                "log_evidence": 0.08000000000000007,
                "reliability": 0.1,
                "sample_size": 92,
                "standard_error": null,
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.047083936105625175,
              "contribution": 0.02495279355156133,
              "diagnostics": {
                "p_value": 0.47003594823542827,
                "p_adj": 0.5466443442927771,
                "log_evidence": 1.2079137625529817,
                "reliability": 0.8,
                "sample_size": 80,
                "standard_error": 0.24,
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "baseline_full": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.22213427759541934,
              "contribution": 0.16186211271033765,
              "diagnostics": {
                "p_value": 0.27133212189276534,
                "p_adj": 0.3091370726046696,
                "log_evidence": 2.3479409995674327,
                "reliability": 0.9,
                "sample_size": 90,
                "standard_error": 0.2,
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.138108649243586,
              "contribution": 0.07959052337695714,
              "diagnostics": {
                "p_value": 0.4237107971667933,
                "p_adj": 0.4819578977400822,
                "log_evidence": 1.4597970356673136,
                "reliability": 0.85,
                "sample_size": 85,
                "standard_error": 0.21,
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.06963157436253677,
              "contribution": 0.02295613423030708,
              "diagnostics": {
                "p_value": 0.6703200460356393,
                "p_adj": 0.6921171816887305,
                "log_evidence": 0.7359999999999998,
                "reliability": 0.92,
                "sample_size": 92,
                "standard_error": 0.18,
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.11427844698470047,
              "contribution": 0.06056346879337467,
              "diagnostics": {
                "p_value": 0.47003594823542827,
                "p_adj": 0.5466443442927771,
                "log_evidence": 1.2079137625529817,
                "reliability": 0.8,
                "sample_size": 80,
                "standard_error": 0.24,
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Scale heterogeneity": {
        "scenario_index": 5,
        "scenario_name": "Scale heterogeneity",
        "candidate": "Fisher-UP",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.991386267880196,
          "s5.z.1": 0.9544997361036416,
          "s5.bf.1": 0.9900990099009901,
          "s5.abs.1": 0.5890404340586651
        },
        "score_summary": "agg=0.9914, scores=[0.9545, 0.9901, 0.589]",
        "pass_reason": "component scores in [0.3, 0.99] with tolerance; ranking checked post-pass",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "heterogeneous": [
            {
              "index": 0,
              "method_ref": "s5.z.1",
              "kind": "ZScore",
              "score": 0.9544997361036416,
              "weight": 0.38089564976892915,
              "contribution": 0.363564797187468,
              "diagnostics": {
                "p_value": 0.04550026389635842,
                "p_adj": 0.04550026389635842,
                "log_evidence": 6.180074306244173,
                "reliability": 1.0,
                "sample_size": 120,
                "standard_error": 0.2,
                "raw_score": 0.9544997361036416,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s5.bf.1",
              "kind": "BayesFactor",
              "score": 0.9900990099009901,
              "weight": 0.5688861463196504,
              "contribution": 0.5632536102174757,
              "diagnostics": {
                "p_value": 0.00990099009900991,
                "p_adj": 0.00990099009900991,
                "log_evidence": 9.230241033682518,
                "reliability": 1.0,
                "sample_size": 120,
                "standard_error": 0.2,
                "raw_score": 0.9900990099009901,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s5.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.10961532815389333,
              "contribution": 0.06456786047525234,
              "diagnostics": {
                "p_value": 0.41095956594133487,
                "p_adj": 0.41095956594133487,
                "log_evidence": 1.7785208980605687,
                "reliability": 1.0,
                "sample_size": 120,
                "standard_error": 0.2,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Calibration decomposability": {
        "scenario_index": 6,
        "scenario_name": "Calibration decomposability",
        "candidate": "Fisher-UP",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.978416731510555,
          "reconstructed": 0.9784167315105549,
          "dominant_share": 0.5678955979366419
        },
        "score_summary": "agg=0.9784, recon=0.9784, dom_share=0.568",
        "pass_reason": "sum(weight*score) reconstructs aggregate and dominant component is identifiable",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "calibration": [
            {
              "index": 0,
              "method_ref": "s6.z.strong",
              "kind": "ZScore",
              "score": 0.9973002039367398,
              "weight": 0.5571427265121129,
              "contribution": 0.5556385547724014,
              "diagnostics": {
                "p_value": 0.002699796063260207,
                "p_adj": 0.002699796063260207,
                "log_evidence": 11.829158081900795,
                "reliability": 1.0,
                "sample_size": 220,
                "standard_error": 0.1,
                "raw_score": 0.9973002039367398,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s6.d.mid",
              "kind": "EffectSize",
              "score": 0.6318797493064809,
              "weight": 0.09413656372557377,
              "contribution": 0.05948298828748912,
              "diagnostics": {
                "p_value": 0.36812025069351906,
                "p_adj": 0.36812025069351906,
                "log_evidence": 1.9986912520027853,
                "reliability": 1.0,
                "sample_size": 140,
                "standard_error": 0.18,
                "raw_score": 0.6318797493064809,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s6.kl.weak",
              "kind": "KLDivergence",
              "score": 0.2591817793182821,
              "weight": 0.028259461374410197,
              "contribution": 0.007324337481595901,
              "diagnostics": {
                "p_value": 0.7408182206817179,
                "p_adj": 0.7408182206817179,
                "log_evidence": 0.6,
                "reliability": 1.0,
                "sample_size": 130,
                "standard_error": 0.24,
                "raw_score": 0.2591817793182821,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s6.abs.mid",
              "kind": "AbsoluteDifference",
              "score": 0.5597136492671929,
              "weight": 0.07727361002050229,
              "contribution": 0.04325109425662526,
              "diagnostics": {
                "p_value": 0.4402863507328071,
                "p_adj": 0.4402863507328071,
                "log_evidence": 1.6406599332528515,
                "reliability": 1.0,
                "sample_size": 110,
                "standard_error": 0.26,
                "raw_score": 0.5597136492671929,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s6.bf.strong",
              "kind": "BayesFactor",
              "score": 0.9230769230769231,
              "weight": 0.24161362431500855,
              "contribution": 0.22302796090616175,
              "diagnostics": {
                "p_value": 0.07692307692307687,
                "p_adj": 0.07692307692307687,
                "log_evidence": 5.129898714923074,
                "reliability": 1.0,
                "sample_size": 160,
                "standard_error": 0.16,
                "raw_score": 0.9230769230769231,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s6.custom.1",
              "kind": "Custom",
              "score": 0.7290879223493065,
              "weight": 0.12301917650380449,
              "contribution": 0.08969179580628145,
              "diagnostics": {
                "p_value": 0.2709120776506935,
                "p_adj": 0.2709120776506935,
                "log_evidence": 2.6119218949134417,
                "reliability": 1.0,
                "sample_size": 125,
                "standard_error": 0.25,
                "raw_score": 0.7290879223493065,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Boundary-seeking": {
        "scenario_index": 7,
        "scenario_name": "Boundary-seeking",
        "candidate": "Fisher-UP",
        "passed": false,
        "raw_scores": {
          "boundary": 0.9916773618000806,
          "non_boundary": 0.9916773618000806
        },
        "score_summary": "boundary=0.9917, non_boundary=0.9917",
        "pass_reason": "boundary < non_boundary for same values with lower uncertainty comparator",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.8273700771858428,
              "contribution": 0.8262330424483812,
              "diagnostics": {
                "p_value": 0.0013742758758317208,
                "p_adj": 0.0013742758758317208,
                "log_evidence": 13.179656644687336,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 1.2,
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.11299733969434127,
              "contribution": 0.06705604974322005,
              "diagnostics": {
                "p_value": 0.4065696597405991,
                "p_adj": 0.4065696597405991,
                "log_evidence": 1.8,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.2,
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.14411862309537737,
              "contribution": 0.09838826960847927,
              "diagnostics": {
                "p_value": 0.31731050786291415,
                "p_adj": 0.31731050786291415,
                "log_evidence": 2.295748928898636,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.25,
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "non_boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.8273700771858428,
              "contribution": 0.8262330424483812,
              "diagnostics": {
                "p_value": 0.0013742758758317208,
                "p_adj": 0.0013742758758317208,
                "log_evidence": 13.179656644687336,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.2,
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.11299733969434127,
              "contribution": 0.06705604974322005,
              "diagnostics": {
                "p_value": 0.4065696597405991,
                "p_adj": 0.4065696597405991,
                "log_evidence": 1.8,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.2,
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.14411862309537737,
              "contribution": 0.09838826960847927,
              "diagnostics": {
                "p_value": 0.31731050786291415,
                "p_adj": 0.31731050786291415,
                "log_evidence": 2.295748928898636,
                "reliability": 1.0,
                "sample_size": 100,
                "standard_error": 0.25,
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      }
    },
    "Hybrid": {
      "Noisy TV": {
        "scenario_index": 1,
        "scenario_name": "Noisy TV",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "base": 0.60448537094557,
          "doubled": 0.573838756044646
        },
        "score_summary": "base=0.6045, doubled=0.5738",
        "pass_reason": "doubled <= base",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "base": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.base",
              "kind": "AbsoluteDifference",
              "score": 0.6456563062257954,
              "weight": 0.9362339763691128,
              "contribution": 0.60448537094557,
              "diagnostics": {
                "precision": 6.791096610976364,
                "confidence": 0.9362339763691128,
                "gated_score": 0.60448537094557,
                "p_value": 0.39551462905443,
                "log_evidence": 1.85513500739775,
                "raw_score": 0.6456563062257954,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "doubled": [
            {
              "index": 0,
              "method_ref": "s1.absdiff.doubled",
              "kind": "AbsoluteDifference",
              "score": 0.8849332679544502,
              "weight": 0.6484542697452108,
              "contribution": 0.573838756044646,
              "diagnostics": {
                "precision": 5.40816778747595,
                "confidence": 0.6484542697452108,
                "gated_score": 0.573838756044646,
                "p_value": 0.42616124395535404,
                "log_evidence": 1.7058749947819918,
                "raw_score": 0.8849332679544502,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Unanimous weak signal": {
        "scenario_index": 2,
        "scenario_name": "Unanimous weak signal",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.9234566367020085,
          "max_single": 0.5738586978538172,
          "threshold": 0.8607880467807257
        },
        "score_summary": "agg=0.9235, max1=0.5739, target=0.8608",
        "pass_reason": "aggregate >= 1.5 * max_single",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "aggregate": [
            {
              "index": 0,
              "method_ref": "s2.z.1",
              "kind": "ZScore",
              "score": 0.23582284437790513,
              "weight": 0.25794547775866583,
              "contribution": 0.06082943625946624,
              "diagnostics": {
                "precision": 7.560601162752564,
                "confidence": 0.97897721963386,
                "gated_score": 0.23086519251523002,
                "p_value": 0.7691348074847699,
                "log_evidence": 0.5249780450385045,
                "raw_score": 0.23582284437790513,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s2.d.1",
              "kind": "EffectSize",
              "score": 0.1974126513658474,
              "weight": 0.2129469515003028,
              "contribution": 0.04203842229594929,
              "diagnostics": {
                "precision": 7.881372148481116,
                "confidence": 0.9869013152143131,
                "gated_score": 0.19482680527289947,
                "p_value": 0.8051731947271006,
                "log_evidence": 0.43339575195084984,
                "raw_score": 0.1974126513658474,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s2.kl.1",
              "kind": "KLDivergence",
              "score": 0.09516258196404048,
              "weight": 0.09733643118714434,
              "contribution": 0.00926278611093381,
              "diagnostics": {
                "precision": 8.132224747393392,
                "confidence": 0.9909718362812733,
                "gated_score": 0.09430343859417237,
                "p_value": 0.9056965614058277,
                "log_evidence": 0.1981019004468096,
                "raw_score": 0.09516258196404048,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s2.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.4690333805336482,
              "contribution": 0.17928900651315058,
              "diagnostics": {
                "precision": 8.29429960883224,
                "confidence": 0.9929064421366438,
                "gated_score": 0.3795405976620358,
                "p_value": 0.6204594023379641,
                "log_evidence": 0.9545902076280276,
                "raw_score": 0.382252125230751,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s2.z.2",
              "kind": "ZScore",
              "score": 0.26614347207205613,
              "weight": 0.294586634003748,
              "contribution": 0.07840230959977754,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.2590155131404932,
                "p_value": 0.7409844868595068,
                "log_evidence": 0.599551178634941,
                "raw_score": 0.26614347207205613,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s2.d.2",
              "kind": "EffectSize",
              "score": 0.2205224951115945,
              "weight": 0.24022257553953852,
              "contribution": 0.052974481740112524,
              "diagnostics": {
                "precision": 7.7222101405120025,
                "confidence": 0.9834277605535225,
                "gated_score": 0.2168679435192705,
                "p_value": 0.7831320564807295,
                "log_evidence": 0.4889078854053475,
                "raw_score": 0.2205224951115945,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 6,
              "method_ref": "s2.kl.2",
              "kind": "KLDivergence",
              "score": 0.11307956328284252,
              "weight": 0.1151232585276823,
              "contribution": 0.013018087798008091,
              "diagnostics": {
                "precision": 7.5180641812170865,
                "confidence": 0.9776231269327944,
                "gated_score": 0.11054919624876731,
                "p_value": 0.8894508037512326,
                "log_evidence": 0.23430216232311743,
                "raw_score": 0.11307956328284252,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 7,
              "method_ref": "s2.custom.1",
              "kind": "Custom",
              "score": 0.5817593768418363,
              "weight": 0.8382195900852429,
              "contribution": 0.4876421063846104,
              "diagnostics": {
                "precision": 7.856956327598623,
                "confidence": 0.986419335377267,
                "gated_score": 0.5738586978538172,
                "p_value": 0.42614130214618284,
                "log_evidence": 1.705968585065236,
                "raw_score": 0.5817593768418363,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Mixed signal": {
        "scenario_index": 3,
        "scenario_name": "Mixed signal",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "mixed": 0.9927663053670376,
          "all_contradiction": 0.9989298639964962,
          "all_agreement": 0.9199588415989806
        },
        "score_summary": "mixed=0.9928, allC=0.9989, allA=0.9200",
        "pass_reason": "all_agreement <= mixed <= all_contradiction",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "mixed": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.5884991576640656,
              "contribution": 0.5098671943188322,
              "diagnostics": {
                "precision": 7.633909534136771,
                "confidence": 0.9811250504429898,
                "gated_score": 0.850032613013263,
                "p_value": 0.14996738698673695,
                "log_evidence": 3.7946748572269513,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.2383117392017522,
              "contribution": 0.1312313722892554,
              "diagnostics": {
                "precision": 7.408715734917801,
                "confidence": 0.9737393899775615,
                "gated_score": 0.5362100785588085,
                "p_value": 0.46378992144119147,
                "log_evidence": 1.5366471695905606,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.28514119535493115,
              "contribution": 0.17614536743967196,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.601203108718315,
                "p_value": 0.39879689128168505,
                "log_evidence": 1.8386060722123354,
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.05351300367500616,
              "contribution": 0.008643062804716023,
              "diagnostics": {
                "precision": 7.633909534136771,
                "confidence": 0.9811250504429898,
                "gated_score": 0.15846476272868196,
                "p_value": 0.8415352372713181,
                "log_evidence": 0.3450547837421971,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.2049506387201301,
              "contribution": 0.10177547519107923,
              "diagnostics": {
                "precision": 7.408715734917801,
                "confidence": 0.9737393899775615,
                "gated_score": 0.48354467078566915,
                "p_value": 0.5164553292143308,
                "log_evidence": 1.3215329632941133,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.1584190726266643,
              "contribution": 0.06510383332348277,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.39995308554278625,
                "p_value": 0.6000469144572138,
                "log_evidence": 1.0214948721214119,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement"
              }
            }
          ],
          "all_contradiction": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.8663855974622838,
              "weight": 0.38476183591652074,
              "contribution": 0.33335211309122004,
              "diagnostics": {
                "precision": 7.633909534136771,
                "confidence": 0.9811250504429898,
                "gated_score": 0.850032613013263,
                "p_value": 0.14996738698673695,
                "log_evidence": 3.7946748572269513,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.5506710358827784,
              "weight": 0.15580865512141776,
              "contribution": 0.0857993135152137,
              "diagnostics": {
                "precision": 7.408715734917801,
                "confidence": 0.9737393899775615,
                "gated_score": 0.5362100785588085,
                "p_value": 0.46378992144119147,
                "log_evidence": 1.5366471695905606,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.617747874769249,
              "weight": 0.18642584002273363,
              "contribution": 0.1151641664761157,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.601203108718315,
                "p_value": 0.39879689128168505,
                "log_evidence": 1.8386060722123354,
                "raw_score": 0.617747874769249,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.8384866815324579,
              "weight": 0.3507641612594555,
              "contribution": 0.29411107757495675,
              "diagnostics": {
                "precision": 7.633909534136771,
                "confidence": 0.9811250504429898,
                "gated_score": 0.8226602877143079,
                "p_value": 0.17733971228569212,
                "log_evidence": 3.4593762148394096,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.5034146962085905,
              "weight": 0.1366252589697958,
              "contribution": 0.06877916323869976,
              "diagnostics": {
                "precision": 7.408715734917801,
                "confidence": 0.9737393899775615,
                "gated_score": 0.49019471919189234,
                "p_value": 0.5098052808081077,
                "log_evidence": 1.347452857011758,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.17269447769379997,
              "contribution": 0.1017240301002904,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.5732645219526435,
                "p_value": 0.42673547804735645,
                "log_evidence": 1.7031818941335537,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "all_agreement": [
            {
              "index": 0,
              "method_ref": "s3.z.c1",
              "kind": "ZScore",
              "score": 0.13361440253771617,
              "weight": 0.1266057435703042,
              "contribution": 0.016916350784989497,
              "diagnostics": {
                "precision": 7.633909534136771,
                "confidence": 0.9811250504429898,
                "gated_score": 0.13109243742972673,
                "p_value": 0.8689075625702732,
                "log_evidence": 0.28103706312154797,
                "raw_score": 0.8663855974622838,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 1,
              "method_ref": "s3.kl.c1",
              "kind": "KLDivergence",
              "score": 0.44932896411722156,
              "weight": 0.5184440947007984,
              "contribution": 0.23295194802460048,
              "diagnostics": {
                "precision": 7.408715734917801,
                "confidence": 0.9737393899775615,
                "gated_score": 0.43752931141875295,
                "p_value": 0.562470688581247,
                "log_evidence": 1.1508325109003739,
                "raw_score": 0.5506710358827784,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 2,
              "method_ref": "s3.abs.c1",
              "kind": "AbsoluteDifference",
              "score": 0.382252125230751,
              "weight": 0.41917480630295373,
              "contribution": 0.1602304605524925,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.37201449877711495,
                "p_value": 0.627985501222885,
                "log_evidence": 0.9304764000103157,
                "raw_score": 0.617747874769249,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 3,
              "method_ref": "s3.z.a1",
              "kind": "ZScore",
              "score": 0.16151331846754213,
              "weight": 0.15544539564618676,
              "contribution": 0.02510650169131565,
              "diagnostics": {
                "precision": 7.633909534136771,
                "confidence": 0.9811250504429898,
                "gated_score": 0.15846476272868196,
                "p_value": 0.8415352372713181,
                "log_evidence": 0.3450547837421971,
                "raw_score": 0.8384866815324579,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 4,
              "method_ref": "s3.kl.a1",
              "kind": "KLDivergence",
              "score": 0.4965853037914095,
              "weight": 0.595343765737247,
              "contribution": 0.2956389647689526,
              "diagnostics": {
                "precision": 7.408715734917801,
                "confidence": 0.9737393899775615,
                "gated_score": 0.48354467078566915,
                "p_value": 0.5164553292143308,
                "log_evidence": 1.3215329632941133,
                "raw_score": 0.5034146962085905,
                "direction_mode": "Agreement"
              }
            },
            {
              "index": 5,
              "method_ref": "s3.abs.a1",
              "kind": "AbsoluteDifference",
              "score": 0.41095956594133487,
              "weight": 0.4601781572925506,
              "contribution": 0.18911461577662994,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.39995308554278625,
                "p_value": 0.6000469144572138,
                "log_evidence": 1.0214948721214119,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Agreement"
              }
            }
          ]
        }
      },
      "Missing data": {
        "scenario_index": 4,
        "scenario_name": "Missing data",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "missing": 0.8903003153548937,
          "baseline_full": 0.959367693308684,
          "relative_delta": 0.07199260349865387
        },
        "score_summary": "missing=0.8903, baseline=0.9594, delta=0.072",
        "pass_reason": "finite and <=20% delta from baseline",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "missing": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.49457335775733885,
              "contribution": 0.36037971916541034,
              "diagnostics": {
                "precision": null,
                "confidence": 0.7,
                "gated_score": 0.5100675146750643,
                "p_value": 0.4899324853249357,
                "log_evidence": 1.4269753648436971,
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.3580346623776211,
              "contribution": 0.20633151016825557,
              "diagnostics": {
                "precision": null,
                "confidence": 0.7,
                "gated_score": 0.4034024419832447,
                "p_value": 0.5965975580167553,
                "log_evidence": 1.0330250001531034,
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.1818709838667337,
              "contribution": 0.05995921758863775,
              "diagnostics": {
                "precision": null,
                "confidence": 0.7,
                "gated_score": 0.23077596777505247,
                "p_value": 0.7692240322249475,
                "log_evidence": 0.5247460452268234,
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.49744858647450985,
              "contribution": 0.2636298684325902,
              "diagnostics": {
                "precision": 7.236979086861173,
                "confidence": 0.9662834574070494,
                "gated_score": 0.5120954962405189,
                "p_value": 0.48790450375948113,
                "log_evidence": 1.4352711625920813,
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "baseline_full": [
            {
              "index": 0,
              "method_ref": "s4.z.1",
              "kind": "ZScore",
              "score": 0.7286678781072347,
              "weight": 0.634015493504232,
              "contribution": 0.46198672433883997,
              "diagnostics": {
                "precision": 7.7191298408817435,
                "confidence": 0.9833522898740504,
                "gated_score": 0.7165372264944146,
                "p_value": 0.28346277350558535,
                "log_evidence": 2.521348949644472,
                "raw_score": 0.7286678781072347,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s4.d.1",
              "kind": "EffectSize",
              "score": 0.5762892028332067,
              "weight": 0.41775839504584594,
              "contribution": 0.2407496524578504,
              "diagnostics": {
                "precision": 7.564465441984008,
                "confidence": 0.9790961840842594,
                "gated_score": 0.5642425594229524,
                "p_value": 0.4357574405770476,
                "log_evidence": 1.6613390387863973,
                "raw_score": 0.5762892028332067,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s4.kl.1",
              "kind": "KLDivergence",
              "score": 0.3296799539643607,
              "weight": 0.19825629966594707,
              "contribution": 0.06536112774701393,
              "diagnostics": {
                "precision": 7.951737545116407,
                "confidence": 0.9881979095411121,
                "gated_score": 0.32578904132519126,
                "p_value": 0.6742109586748087,
                "log_evidence": 0.7884244439521709,
                "raw_score": 0.3296799539643607,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s4.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5299640517645717,
              "weight": 0.36091162811538857,
              "contribution": 0.19127018876497964,
              "diagnostics": {
                "precision": 7.236979086861173,
                "confidence": 0.9662834574070494,
                "gated_score": 0.5120954962405189,
                "p_value": 0.48790450375948113,
                "log_evidence": 1.4352711625920813,
                "raw_score": 0.5299640517645717,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Scale heterogeneity": {
        "scenario_index": 5,
        "scenario_name": "Scale heterogeneity",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.9995178575532121,
          "s5.z.1": 0.9544997361036416,
          "s5.bf.1": 0.9900990099009901,
          "s5.abs.1": 0.5890404340586651
        },
        "score_summary": "agg=0.9995, scores=[0.9545, 0.9901, 0.589]",
        "pass_reason": "component scores in [0.3, 0.99] with tolerance; ranking checked post-pass",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "heterogeneous": [
            {
              "index": 0,
              "method_ref": "s5.z.1",
              "kind": "ZScore",
              "score": 0.9544997361036416,
              "weight": 0.4055824444426798,
              "contribution": 0.38712833618880776,
              "diagnostics": {
                "precision": 8.006700845415375,
                "confidence": 0.9891217416377573,
                "gated_score": 0.9441164413676137,
                "p_value": 0.055883558632386254,
                "log_evidence": 5.768970126299206,
                "raw_score": 0.9544997361036416,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s5.bf.1",
              "kind": "BayesFactor",
              "score": 0.9900990099009901,
              "weight": 0.5454190807349449,
              "contribution": 0.5400188918167772,
              "diagnostics": {
                "precision": 8.006700845415375,
                "confidence": 0.9891217416377573,
                "gated_score": 0.9793284570670865,
                "p_value": 0.02067154293291351,
                "log_evidence": 7.757994524139621,
                "raw_score": 0.9900990099009901,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s5.abs.1",
              "kind": "AbsoluteDifference",
              "score": 0.5890404340586651,
              "weight": 0.12286190448586329,
              "contribution": 0.07237062954762717,
              "diagnostics": {
                "precision": 8.006700845415375,
                "confidence": 0.9891217416377573,
                "gated_score": 0.5826327000311674,
                "p_value": 0.4173672999688326,
                "log_evidence": 1.7475772592009788,
                "raw_score": 0.5890404340586651,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Calibration decomposability": {
        "scenario_index": 6,
        "scenario_name": "Calibration decomposability",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "aggregate": 0.9999905950837242,
          "reconstructed": 0.9999905950837241,
          "dominant_share": 0.5658259239005418
        },
        "score_summary": "agg=1.0000, recon=1.0000, dom_share=0.566",
        "pass_reason": "sum(weight*score) reconstructs aggregate and dominant component is identifiable",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "calibration": [
            {
              "index": 0,
              "method_ref": "s6.z.strong",
              "kind": "ZScore",
              "score": 0.9973002039367398,
              "weight": 0.5673523379636164,
              "contribution": 0.5658206023551008,
              "diagnostics": {
                "precision": 9.998843185752886,
                "confidence": 0.9994462618686876,
                "gated_score": 0.9967479607854544,
                "p_value": 0.0032520392145456434,
                "log_evidence": 11.456946057551813,
                "raw_score": 0.9973002039367398,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s6.d.mid",
              "kind": "EffectSize",
              "score": 0.6318797493064809,
              "weight": 0.09790679720062707,
              "contribution": 0.0618653224705327,
              "diagnostics": {
                "precision": 8.371470680558268,
                "confidence": 0.9936769220634581,
                "gated_score": 0.6278843244050936,
                "p_value": 0.37211567559490644,
                "log_evidence": 1.9771010342909403,
                "raw_score": 0.6318797493064809,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s6.kl.weak",
              "kind": "KLDivergence",
              "score": 0.2591817793182821,
              "weight": 0.0291396543035131,
              "contribution": 0.007552467451104163,
              "diagnostics": {
                "precision": 7.7222101405120025,
                "confidence": 0.9834277605535225,
                "gated_score": 0.2548865568112555,
                "p_value": 0.7451134431887445,
                "log_evidence": 0.5884375989166482,
                "raw_score": 0.2591817793182821,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 3,
              "method_ref": "s6.abs.mid",
              "kind": "AbsoluteDifference",
              "score": 0.5597136492671929,
              "weight": 0.07793018912151155,
              "contribution": 0.043618590541283725,
              "diagnostics": {
                "precision": 7.395242018409667,
                "confidence": 0.9732176074954298,
                "gated_score": 0.5447231786223536,
                "p_value": 0.45527682137764636,
                "log_evidence": 1.5736992927968259,
                "raw_score": 0.5597136492671929,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 4,
              "method_ref": "s6.bf.strong",
              "kind": "BayesFactor",
              "score": 0.9230769230769231,
              "weight": 0.24979455547628693,
              "contribution": 0.23057958967041872,
              "diagnostics": {
                "precision": 8.740496729892756,
                "confidence": 0.9963549921573666,
                "gated_score": 0.9197123004529538,
                "p_value": 0.08028769954704618,
                "log_evidence": 5.044277701990332,
                "raw_score": 0.9230769230769231,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 5,
              "method_ref": "s6.custom.1",
              "kind": "Custom",
              "score": 0.7290879223493065,
              "weight": 0.12420178666997526,
              "contribution": 0.09055402259528406,
              "diagnostics": {
                "precision": 7.601402334567742,
                "confidence": 0.9802005590788575,
                "gated_score": 0.7146523891044329,
                "p_value": 0.28534761089556715,
                "log_evidence": 2.508094309149948,
                "raw_score": 0.7290879223493065,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      },
      "Boundary-seeking": {
        "scenario_index": 7,
        "scenario_name": "Boundary-seeking",
        "candidate": "Hybrid",
        "passed": true,
        "raw_scores": {
          "boundary": 0.8948522898310401,
          "non_boundary": 0.9978236676119789
        },
        "score_summary": "boundary=0.8949, non_boundary=0.9978",
        "pass_reason": "boundary < non_boundary for same values with lower uncertainty comparator",
        "bounded": true,
        "warnings": [],
        "skipped": [],
        "decompositions": {
          "boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.1632472097729186,
              "contribution": 0.16302286307073083,
              "diagnostics": {
                "precision": 4.254824377100322,
                "confidence": 0.24642637914115365,
                "gated_score": 0.2460877213131314,
                "p_value": 0.7539122786868686,
                "log_evidence": 0.5649585180129015,
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.5082212378069612,
              "contribution": 0.30159390207883885,
              "diagnostics": {
                "precision": 7.824445930852629,
                "confidence": 0.9857503235006649,
                "gated_score": 0.5849741498858141,
                "p_value": 0.4150258501141859,
                "log_evidence": 1.7588289425191432,
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.6302067479238103,
              "contribution": 0.4302355246814705,
              "diagnostics": {
                "precision": 7.378383712980725,
                "confidence": 0.9725505405821877,
                "gated_score": 0.6639500346277021,
                "p_value": 0.3360499653722979,
                "log_evidence": 2.1809908472189417,
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction"
              }
            }
          ],
          "non_boundary": [
            {
              "index": 0,
              "method_ref": "s7.z.boundary",
              "kind": "ZScore",
              "score": 0.9986257241241683,
              "weight": 0.7657779024022613,
              "contribution": 0.7647255123047448,
              "diagnostics": {
                "precision": 7.824445930852629,
                "confidence": 0.9857503235006649,
                "gated_score": 0.9843956306114846,
                "p_value": 0.015604369388515393,
                "log_evidence": 8.320408629873704,
                "raw_score": 0.9986257241241683,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 1,
              "method_ref": "s7.kl.boundary",
              "kind": "KLDivergence",
              "score": 0.5934303402594009,
              "weight": 0.16187574411320002,
              "contribution": 0.09606197790884,
              "diagnostics": {
                "precision": 7.824445930852629,
                "confidence": 0.9857503235006649,
                "gated_score": 0.5849741498858141,
                "p_value": 0.4150258501141859,
                "log_evidence": 1.7588289425191432,
                "raw_score": 0.5934303402594009,
                "direction_mode": "Contradiction"
              }
            },
            {
              "index": 2,
              "method_ref": "s7.d.boundary",
              "kind": "EffectSize",
              "score": 0.6826894921370859,
              "weight": 0.20072987643242748,
              "contribution": 0.1370361773983939,
              "diagnostics": {
                "precision": 7.378383712980725,
                "confidence": 0.9725505405821877,
                "gated_score": 0.6639500346277021,
                "p_value": 0.3360499653722979,
                "log_evidence": 2.1809908472189417,
                "raw_score": 0.6826894921370859,
                "direction_mode": "Contradiction"
              }
            }
          ]
        }
      }
    }
  },
  "matrix_markdown": "| Candidate | S1 Noisy TV | S2 Unanimous weak signal | S3 Mixed signal | S4 Missing data | S5 Scale heterogeneity | S6 Calibration decomposability | S7 Boundary-seeking |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| IVW-CDF | base=0.6457, doubled=0.8849 (FAIL) | agg=0.2698, max1=0.5818, target=0.8726 (FAIL) | mixed=0.5171, allC=0.6761, allA=0.3239 (PASS) | missing=0.5300, baseline=0.5261, delta=0.007 (PASS) | agg=0.8445, scores=[0.9545, 0.9901, 0.589] (PASS) | agg=0.8684, recon=0.8684, dom_share=0.657 (PASS) | boundary=0.6344, non_boundary=0.7686 (PASS) |\n| HTG-Max | base=0.1165, doubled=0.0238 (PASS) | agg=0.3033, max1=0.3033, target=0.4549 (FAIL) | mixed=0.3795, allC=0.3795, allA=0.1774 (PASS) | missing=0.1593, baseline=0.3423, delta=0.535 (FAIL) | agg=0.5712, scores=[0.9545, 0.9901, 0.589] (PASS) | agg=0.9618, recon=0.9618, dom_share=1.000 (PASS) | boundary=0.3022, non_boundary=0.5085 (PASS) |\n| Fisher-UP | base=0.5639, doubled=0.8227 (FAIL) | agg=0.0056, max1=0.5818, target=0.8726 (FAIL) | mixed=0.4002, allC=0.7134, allA=0.0454 (PASS) | missing=0.0662, baseline=0.3250, delta=0.796 (FAIL) | agg=0.9914, scores=[0.9545, 0.9901, 0.589] (PASS) | agg=0.9784, recon=0.9784, dom_share=0.568 (PASS) | boundary=0.9917, non_boundary=0.9917 (FAIL) |\n| Hybrid | base=0.6045, doubled=0.5738 (PASS) | agg=0.9235, max1=0.5739, target=0.8608 (PASS) | mixed=0.9928, allC=0.9989, allA=0.9200 (PASS) | missing=0.8903, baseline=0.9594, delta=0.072 (PASS) | agg=0.9995, scores=[0.9545, 0.9901, 0.589] (PASS) | agg=1.0000, recon=1.0000, dom_share=0.566 (PASS) | boundary=0.8949, non_boundary=0.9978 (PASS) |\n",
  "ranking_reference": [
    "s5.bf.1",
    "s5.z.1",
    "s5.abs.1"
  ],
  "htg_lse_exploratory": {
    "Noisy TV": {
      "scenario_index": 1,
      "scenario_name": "Noisy TV",
      "candidate": "HTG-Max-LSE",
      "passed": true,
      "raw_scores": {
        "base": 0.10997309033907932,
        "doubled": 0.02353746040631799
      },
      "score_summary": "base=0.1100, doubled=0.0235",
      "pass_reason": "doubled <= base",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "base": [
          {
            "index": 0,
            "method_ref": "s1.absdiff.base",
            "kind": "AbsoluteDifference",
            "score": 0.6456563062257954,
            "weight": 0.18044210211465983,
            "contribution": 0.11650358113896905,
            "diagnostics": {
              "confidence": 0.18044210211465983,
              "gated_score": 0.11650358113896905,
              "precision": 6.791096610976364,
              "winner_gate": 1.0,
              "raw_score": 0.6456563062257954,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ],
        "doubled": [
          {
            "index": 0,
            "method_ref": "s1.absdiff.doubled",
            "kind": "AbsoluteDifference",
            "score": 0.8849332679544502,
            "weight": 0.026916031044386425,
            "contribution": 0.02381889131247231,
            "diagnostics": {
              "confidence": 0.026916031044386425,
              "gated_score": 0.02381889131247231,
              "precision": 5.40816778747595,
              "winner_gate": 1.0,
              "raw_score": 0.8849332679544502,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ]
      }
    },
    "Unanimous weak signal": {
      "scenario_index": 2,
      "scenario_name": "Unanimous weak signal",
      "candidate": "HTG-Max-LSE",
      "passed": false,
      "raw_scores": {
        "aggregate": 0.33239318223833303,
        "max_single": 0.26162076303342374,
        "threshold": 0.3924311445501356
      },
      "score_summary": "agg=0.3324, max1=0.2616, target=0.3924",
      "pass_reason": "aggregate >= 1.5 * max_single",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "aggregate": [
          {
            "index": 0,
            "method_ref": "s2.z.1",
            "kind": "ZScore",
            "score": 0.23582284437790513,
            "weight": 0.03524416470906109,
            "contribution": 0.00831137916941417,
            "diagnostics": {
              "confidence": 0.4111778702799699,
              "gated_score": 0.0969651349146718,
              "precision": 7.560601162752564,
              "winner_gate": 0.08571513025509722,
              "raw_score": 0.23582284437790513,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s2.d.1",
            "kind": "EffectSize",
            "score": 0.1974126513658474,
            "weight": 0.04838121543928118,
            "contribution": 0.00955106401617077,
            "diagnostics": {
              "confidence": 0.5304767276821589,
              "gated_score": 0.10472281729961361,
              "precision": 7.881372148481116,
              "winner_gate": 0.09120327606203552,
              "raw_score": 0.1974126513658474,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s2.kl.1",
            "kind": "KLDivergence",
            "score": 0.09516258196404048,
            "weight": 0.03941649169051485,
            "contribution": 0.0037509751212335403,
            "diagnostics": {
              "confidence": 0.6220684689819068,
              "gated_score": 0.059197641666735884,
              "precision": 8.132224747393392,
              "winner_gate": 0.0633635904340641,
              "raw_score": 0.09516258196404048,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s2.abs.1",
            "kind": "AbsoluteDifference",
            "score": 0.382252125230751,
            "weight": 0.21207074210000196,
            "contribution": 0.08106449186698825,
            "diagnostics": {
              "confidence": 0.6773127216648894,
              "gated_score": 0.2589042273022281,
              "precision": 8.29429960883224,
              "winner_gate": 0.3131060960714202,
              "raw_score": 0.382252125230751,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 4,
            "method_ref": "s2.z.2",
            "kind": "ZScore",
            "score": 0.26614347207205613,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.09387214789512971,
              "precision": 7.395242018409667,
              "winner_gate": 0.0,
              "raw_score": 0.26614347207205613,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 5,
            "method_ref": "s2.d.2",
            "kind": "EffectSize",
            "score": 0.2205224951115945,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.47086185567086464,
              "gated_score": 0.10383563126541456,
              "precision": 7.7222101405120025,
              "winner_gate": 0.0,
              "raw_score": 0.2205224951115945,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 6,
            "method_ref": "s2.kl.2",
            "kind": "KLDivergence",
            "score": 0.11307956328284252,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3958221227013417,
              "gated_score": 0.044759392772755424,
              "precision": 7.5180641812170865,
              "winner_gate": 0.0,
              "raw_score": 0.11307956328284252,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 7,
            "method_ref": "s2.custom.1",
            "kind": "Custom",
            "score": 0.5817593768418363,
            "weight": 0.23283917095843626,
            "contribution": 0.13545637100114968,
            "diagnostics": {
              "confidence": 0.5213456408495586,
              "gated_score": 0.303297715139847,
              "precision": 7.856956327598623,
              "winner_gate": 0.4466119071773829,
              "raw_score": 0.5817593768418363,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ]
      }
    },
    "Mixed signal": {
      "scenario_index": 3,
      "scenario_name": "Mixed signal",
      "candidate": "HTG-Max-LSE",
      "passed": true,
      "raw_scores": {
        "mixed": 0.3499540340793966,
        "all_contradiction": 0.3499540340793966,
        "all_agreement": 0.2410600346802333
      },
      "score_summary": "mixed=0.3500, allC=0.3500, allA=0.2411",
      "pass_reason": "all_agreement <= mixed <= all_contradiction",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "mixed": [
          {
            "index": 0,
            "method_ref": "s3.z.c1",
            "kind": "ZScore",
            "score": 0.8663855974622838,
            "weight": 0.2908100747472537,
            "contribution": 0.2519536603579508,
            "diagnostics": {
              "confidence": 0.43803624451807166,
              "gated_score": 0.37950829341692455,
              "precision": 7.633909534136771,
              "winner_gate": 0.6638950050062717,
              "raw_score": 0.8663855974622838,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s3.kl.c1",
            "kind": "KLDivergence",
            "score": 0.5506710358827784,
            "weight": 0.054993072402236644,
            "contribution": 0.030283092146116287,
            "diagnostics": {
              "confidence": 0.3573403917513822,
              "gated_score": 0.1967770036884915,
              "precision": 7.408715734917801,
              "winner_gate": 0.1538954836107579,
              "raw_score": 0.5506710358827784,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s3.abs.c1",
            "kind": "AbsoluteDifference",
            "score": 0.617747874769249,
            "weight": 0.06426758495061127,
            "contribution": 0.03970116401979228,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.21788744022449996,
              "precision": 7.395242018409667,
              "winner_gate": 0.18220951138297029,
              "raw_score": 0.617747874769249,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s3.z.a1",
            "kind": "ZScore",
            "score": 0.16151331846754213,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.43803624451807166,
              "gated_score": 0.07074868746117347,
              "precision": 7.633909534136771,
              "winner_gate": 0.0,
              "raw_score": 0.8384866815324579,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 4,
            "method_ref": "s3.kl.a1",
            "kind": "KLDivergence",
            "score": 0.4965853037914095,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3573403917513822,
              "gated_score": 0.17744998699480144,
              "precision": 7.408715734917801,
              "winner_gate": 0.0,
              "raw_score": 0.5034146962085905,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 5,
            "method_ref": "s3.abs.a1",
            "kind": "AbsoluteDifference",
            "score": 0.41095956594133487,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.144950604471406,
              "precision": 7.395242018409667,
              "winner_gate": 0.0,
              "raw_score": 0.5890404340586651,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          }
        ],
        "all_contradiction": [
          {
            "index": 0,
            "method_ref": "s3.z.c1",
            "kind": "ZScore",
            "score": 0.8663855974622838,
            "weight": 0.2908100747472537,
            "contribution": 0.2519536603579508,
            "diagnostics": {
              "confidence": 0.43803624451807166,
              "gated_score": 0.37950829341692455,
              "precision": 7.633909534136771,
              "winner_gate": 0.6638950050062717,
              "raw_score": 0.8663855974622838,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s3.kl.c1",
            "kind": "KLDivergence",
            "score": 0.5506710358827784,
            "weight": 0.054993072402236644,
            "contribution": 0.030283092146116287,
            "diagnostics": {
              "confidence": 0.3573403917513822,
              "gated_score": 0.1967770036884915,
              "precision": 7.408715734917801,
              "winner_gate": 0.1538954836107579,
              "raw_score": 0.5506710358827784,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s3.abs.c1",
            "kind": "AbsoluteDifference",
            "score": 0.617747874769249,
            "weight": 0.06426758495061127,
            "contribution": 0.03970116401979228,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.21788744022449996,
              "precision": 7.395242018409667,
              "winner_gate": 0.18220951138297029,
              "raw_score": 0.617747874769249,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s3.z.a1",
            "kind": "ZScore",
            "score": 0.8384866815324579,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.43803624451807166,
              "gated_score": 0.3672875570568982,
              "precision": 7.633909534136771,
              "winner_gate": 0.0,
              "raw_score": 0.8384866815324579,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 4,
            "method_ref": "s3.kl.a1",
            "kind": "KLDivergence",
            "score": 0.5034146962085905,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3573403917513822,
              "gated_score": 0.17989040475658077,
              "precision": 7.408715734917801,
              "winner_gate": 0.0,
              "raw_score": 0.5034146962085905,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 5,
            "method_ref": "s3.abs.a1",
            "kind": "AbsoluteDifference",
            "score": 0.5890404340586651,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.20776196504716782,
              "precision": 7.395242018409667,
              "winner_gate": 0.0,
              "raw_score": 0.5890404340586651,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ],
        "all_agreement": [
          {
            "index": 0,
            "method_ref": "s3.z.c1",
            "kind": "ZScore",
            "score": 0.13361440253771617,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.43803624451807166,
              "gated_score": 0.05852795110114709,
              "precision": 7.633909534136771,
              "winner_gate": 0.0,
              "raw_score": 0.8663855974622838,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s3.kl.c1",
            "kind": "KLDivergence",
            "score": 0.44932896411722156,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3573403917513822,
              "gated_score": 0.16056338806289072,
              "precision": 7.408715734917801,
              "winner_gate": 0.0,
              "raw_score": 0.5506710358827784,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s3.abs.c1",
            "kind": "AbsoluteDifference",
            "score": 0.382252125230751,
            "weight": 0.0,
            "contribution": 0.0,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.13482512929407386,
              "precision": 7.395242018409667,
              "winner_gate": 0.0,
              "raw_score": 0.617747874769249,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s3.z.a1",
            "kind": "ZScore",
            "score": 0.16151331846754213,
            "weight": 0.08491326658026918,
            "contribution": 0.013714623467298319,
            "diagnostics": {
              "confidence": 0.43803624451807166,
              "gated_score": 0.07074868746117347,
              "precision": 7.633909534136771,
              "winner_gate": 0.1938498643501314,
              "raw_score": 0.8384866815324579,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 4,
            "method_ref": "s3.kl.a1",
            "kind": "KLDivergence",
            "score": 0.4965853037914095,
            "weight": 0.16265442975502636,
            "contribution": 0.08077179941291825,
            "diagnostics": {
              "confidence": 0.3573403917513822,
              "gated_score": 0.17744998699480144,
              "precision": 7.408715734917801,
              "winner_gate": 0.45518064430900484,
              "raw_score": 0.5034146962085905,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 5,
            "method_ref": "s3.abs.a1",
            "kind": "AbsoluteDifference",
            "score": 0.41095956594133487,
            "weight": 0.1237913511134629,
            "contribution": 0.050873239920880095,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.144950604471406,
              "precision": 7.395242018409667,
              "winner_gate": 0.35096949134086375,
              "raw_score": 0.5890404340586651,
              "direction_mode": "Agreement",
              "mode": "lse_rebound"
            }
          }
        ]
      }
    },
    "Missing data": {
      "scenario_index": 4,
      "scenario_name": "Missing data",
      "candidate": "HTG-Max-LSE",
      "passed": false,
      "raw_scores": {
        "missing": 0.24484436967294143,
        "baseline_full": 0.3465334323536806,
        "relative_delta": 0.2934466149198349
      },
      "score_summary": "missing=0.2448, baseline=0.3465, delta=0.293",
      "pass_reason": "finite and <=20% delta from baseline",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "missing": [
          {
            "index": 0,
            "method_ref": "s4.z.1",
            "kind": "ZScore",
            "score": 0.7286678781072347,
            "weight": 0.0380303833880035,
            "contribution": 0.027711518766941137,
            "diagnostics": {
              "confidence": 0.15,
              "gated_score": 0.1093001817160852,
              "precision": null,
              "winner_gate": 0.2535358892533567,
              "raw_score": 0.7286678781072347,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s4.d.1",
            "kind": "EffectSize",
            "score": 0.5762892028332067,
            "weight": 0.03167510346821824,
            "contribution": 0.01825402012735883,
            "diagnostics": {
              "confidence": 0.15,
              "gated_score": 0.08644338042498101,
              "precision": null,
              "winner_gate": 0.21116735645478826,
              "raw_score": 0.5762892028332067,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s4.kl.1",
            "kind": "KLDivergence",
            "score": 0.3296799539643607,
            "weight": 0.023561167081366765,
            "contribution": 0.007767644478731605,
            "diagnostics": {
              "confidence": 0.15,
              "gated_score": 0.0494519930946541,
              "precision": null,
              "winner_gate": 0.15707444720911176,
              "raw_score": 0.3296799539643607,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s4.abs.1",
            "kind": "AbsoluteDifference",
            "score": 0.5299640517645717,
            "weight": 0.11368654681208598,
            "contribution": 0.06024978297965574,
            "diagnostics": {
              "confidence": 0.30058128429536257,
              "gated_score": 0.15929727530976898,
              "precision": 7.236979086861173,
              "winner_gate": 0.3782223070827433,
              "raw_score": 0.5299640517645717,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ],
        "baseline_full": [
          {
            "index": 0,
            "method_ref": "s4.z.1",
            "kind": "ZScore",
            "score": 0.7286678781072347,
            "weight": 0.24141356437797257,
            "contribution": 0.17591030970160157,
            "diagnostics": {
              "confidence": 0.4697108232291919,
              "gated_score": 0.34226318888641766,
              "precision": 7.7191298408817435,
              "winner_gate": 0.513962106979546,
              "raw_score": 0.7286678781072347,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s4.d.1",
            "kind": "EffectSize",
            "score": 0.5762892028332067,
            "weight": 0.09191423238992157,
            "contribution": 0.05296917971301401,
            "diagnostics": {
              "confidence": 0.412581963861255,
              "gated_score": 0.23776653105696155,
              "precision": 7.564465441984008,
              "winner_gate": 0.22277811547969392,
              "raw_score": 0.5762892028332067,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s4.kl.1",
            "kind": "KLDivergence",
            "score": 0.3296799539643607,
            "weight": 0.08034968036082897,
            "contribution": 0.02648967892240919,
            "diagnostics": {
              "confidence": 0.5566571982151446,
              "gated_score": 0.18351871948149887,
              "precision": 7.951737545116407,
              "winner_gate": 0.14434319832467937,
              "raw_score": 0.3296799539643607,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s4.abs.1",
            "kind": "AbsoluteDifference",
            "score": 0.5299640517645717,
            "weight": 0.035744098104780776,
            "contribution": 0.01894308705827997,
            "diagnostics": {
              "confidence": 0.30058128429536257,
              "gated_score": 0.15929727530976898,
              "precision": 7.236979086861173,
              "winner_gate": 0.11891657921608077,
              "raw_score": 0.5299640517645717,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ]
      }
    },
    "Scale heterogeneity": {
      "scenario_index": 5,
      "scenario_name": "Scale heterogeneity",
      "candidate": "HTG-Max-LSE",
      "passed": true,
      "raw_scores": {
        "aggregate": 0.4822057695531873,
        "s5.z.1": 0.9544997361036416,
        "s5.bf.1": 0.9900990099009901,
        "s5.abs.1": 0.5890404340586651
      },
      "score_summary": "agg=0.4822, scores=[0.9545, 0.9901, 0.589]",
      "pass_reason": "component scores in [0.3, 0.99] with tolerance; ranking checked post-pass",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "heterogeneous": [
          {
            "index": 0,
            "method_ref": "s5.z.1",
            "kind": "ZScore",
            "score": 0.9544997361036416,
            "weight": 0.24406545403802168,
            "contribution": 0.23296041147130717,
            "diagnostics": {
              "confidence": 0.5768977750500027,
              "gated_score": 0.5506487740440056,
              "precision": 8.006700845415375,
              "winner_gate": 0.4230653411982171,
              "raw_score": 0.9544997361036416,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s5.bf.1",
            "kind": "BayesFactor",
            "score": 0.9900990099009901,
            "weight": 0.287646868958595,
            "contribution": 0.28479888015702476,
            "diagnostics": {
              "confidence": 0.5768977750500027,
              "gated_score": 0.5711859158910918,
              "precision": 8.006700845415375,
              "winner_gate": 0.49860977351431657,
              "raw_score": 0.9900990099009901,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s5.abs.1",
            "kind": "AbsoluteDifference",
            "score": 0.5890404340586651,
            "weight": 0.04518545205338603,
            "contribution": 0.02661605829066351,
            "diagnostics": {
              "confidence": 0.5768977750500027,
              "gated_score": 0.33981611582293175,
              "precision": 8.006700845415375,
              "winner_gate": 0.07832488528746635,
              "raw_score": 0.5890404340586651,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ],
        "_ranking": [
          {
            "ranked_method_ref": "s5.bf.1"
          },
          {
            "ranked_method_ref": "s5.z.1"
          },
          {
            "ranked_method_ref": "s5.abs.1"
          }
        ]
      }
    },
    "Calibration decomposability": {
      "scenario_index": 6,
      "scenario_name": "Calibration decomposability",
      "candidate": "HTG-Max-LSE",
      "passed": false,
      "raw_scores": {
        "aggregate": 0.6262841841350903,
        "reconstructed": 0.9180294351838759,
        "dominant_share": 1.2827608462282927
      },
      "score_summary": "agg=0.6263, recon=0.9180, dom_share=1.283",
      "pass_reason": "sum(weight*score) reconstructs aggregate and dominant component is identifiable",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "calibration": [
          {
            "index": 0,
            "method_ref": "s6.z.strong",
            "kind": "ZScore",
            "score": 0.9973002039367398,
            "weight": 0.8055476443795889,
            "contribution": 0.8033728300205244,
            "diagnostics": {
              "confidence": 0.9643692344348546,
              "gated_score": 0.9617656341721981,
              "precision": 9.998843185752886,
              "winner_gate": 0.8353103931728606,
              "raw_score": 0.9973002039367398,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s6.d.mid",
            "kind": "EffectSize",
            "score": 0.6318797493064809,
            "weight": 0.009290471601832178,
            "contribution": 0.005870460866704697,
            "diagnostics": {
              "confidence": 0.7020765816852114,
              "gated_score": 0.44362797442920243,
              "precision": 8.371470680558268,
              "winner_gate": 0.013232846450357359,
              "raw_score": 0.6318797493064809,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s6.kl.weak",
            "kind": "KLDivergence",
            "score": 0.2591817793182821,
            "weight": 0.0004755887785378916,
            "contribution": 0.00012326394584525916,
            "diagnostics": {
              "confidence": 0.47086185567086464,
              "gated_score": 0.12203881356588285,
              "precision": 7.7222101405120025,
              "winner_gate": 0.0010100388740563668,
              "raw_score": 0.2591817793182821,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 3,
            "method_ref": "s6.abs.mid",
            "kind": "AbsoluteDifference",
            "score": 0.5597136492671929,
            "weight": 0.0006511083735410777,
            "contribution": 0.0003644342438231032,
            "diagnostics": {
              "confidence": 0.3527125695185738,
              "gated_score": 0.19741803942764943,
              "precision": 7.395242018409667,
              "winner_gate": 0.0018460027507094282,
              "raw_score": 0.5597136492671929,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 4,
            "method_ref": "s6.bf.strong",
            "kind": "BayesFactor",
            "score": 0.9230769230769231,
            "weight": 0.11578642345721,
            "contribution": 0.10687977549896309,
            "diagnostics": {
              "confidence": 0.803883438051732,
              "gated_score": 0.7420462505092912,
              "precision": 8.740496729892756,
              "winner_gate": 0.1440338461728065,
              "raw_score": 0.9230769230769231,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 5,
            "method_ref": "s6.custom.1",
            "kind": "Custom",
            "score": 0.7290879223493065,
            "weight": 0.0019458155381919338,
            "contribution": 0.0014186706080153548,
            "diagnostics": {
              "confidence": 0.42607178204403234,
              "gated_score": 0.3106437903421501,
              "precision": 7.601402334567742,
              "winner_gate": 0.004566872579209772,
              "raw_score": 0.7290879223493065,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ]
      }
    },
    "Boundary-seeking": {
      "scenario_index": 7,
      "scenario_name": "Boundary-seeking",
      "candidate": "HTG-Max-LSE",
      "passed": true,
      "raw_scores": {
        "boundary": 0.3074947702162716,
        "non_boundary": 0.41831123434023143
      },
      "score_summary": "boundary=0.3075, non_boundary=0.4183",
      "pass_reason": "boundary < non_boundary for same values with lower uncertainty comparator",
      "bounded": true,
      "warnings": [],
      "skipped": [],
      "decompositions": {
        "boundary": [
          {
            "index": 0,
            "method_ref": "s7.z.boundary",
            "kind": "ZScore",
            "score": 0.9986257241241683,
            "weight": 0.00026835858164139375,
            "contribution": 0.0002679897829165716,
            "diagnostics": {
              "confidence": 0.0048797799162317015,
              "gated_score": 0.0048730737524134565,
              "precision": 4.254824377100322,
              "winner_gate": 0.05499399281282086,
              "raw_score": 0.9986257241241683,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s7.kl.boundary",
            "kind": "KLDivergence",
            "score": 0.5934303402594009,
            "weight": 0.30202081082164617,
            "contribution": 0.17922831253130964,
            "diagnostics": {
              "confidence": 0.5091661970143007,
              "gated_score": 0.3021546695427816,
              "precision": 7.824445930852629,
              "winner_gate": 0.5931674423649209,
              "raw_score": 0.5934303402594009,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s7.d.boundary",
            "kind": "EffectSize",
            "score": 0.6826894921370859,
            "weight": 0.12207426344726786,
            "contribution": 0.08333881691582412,
            "diagnostics": {
              "confidence": 0.3469610089756286,
              "gated_score": 0.23686663500894278,
              "precision": 7.378383712980725,
              "winner_gate": 0.35183856482225834,
              "raw_score": 0.6826894921370859,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ],
        "non_boundary": [
          {
            "index": 0,
            "method_ref": "s7.z.boundary",
            "kind": "ZScore",
            "score": 0.9986257241241683,
            "weight": 0.3899225961243571,
            "contribution": 0.3893867349070617,
            "diagnostics": {
              "confidence": 0.5091661970143007,
              "gated_score": 0.5084664621929549,
              "precision": 7.824445930852629,
              "winner_gate": 0.7658061324785973,
              "raw_score": 0.9986257241241683,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 1,
            "method_ref": "s7.kl.boundary",
            "kind": "KLDivergence",
            "score": 0.5934303402594009,
            "weight": 0.07484758956062519,
            "contribution": 0.04441683054055779,
            "diagnostics": {
              "confidence": 0.5091661970143007,
              "gated_score": 0.3021546695427816,
              "precision": 7.824445930852629,
              "winner_gate": 0.14700031148871218,
              "raw_score": 0.5934303402594009,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          },
          {
            "index": 2,
            "method_ref": "s7.d.boundary",
            "kind": "EffectSize",
            "score": 0.6826894921370859,
            "weight": 0.030252764177275297,
            "contribution": 0.020653244211927095,
            "diagnostics": {
              "confidence": 0.3469610089756286,
              "gated_score": 0.23686663500894278,
              "precision": 7.378383712980725,
              "winner_gate": 0.08719355603269048,
              "raw_score": 0.6826894921370859,
              "direction_mode": "Contradiction",
              "mode": "lse_rebound"
            }
          }
        ]
      }
    }
  }
}
