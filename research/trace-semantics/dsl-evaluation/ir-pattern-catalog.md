# IR Pattern Catalog: Comparative Synthesis of RCA/FV and Provenance/Workflow Surveys

**Date:** 2026-02-20
**Step:** 2c (Comparative IR Synthesis)
**Input documents:**
- `rca-formal-verification-ir-survey.md` (RCA/FV survey) — 13 transferable patterns (P1-P13), 6 anti-patterns (AP1-AP6)
- `provenance-workflow-ir-survey.md` (Provenance survey) — 7 transferable patterns, 5 anti-patterns
**Architecture references:** ARCHITECTURE.md §4.5, §5.3, §3.1; VISION.md §4.1, Open Question #1
**Requirements baseline:** R1-R29 from FINDINGS.md accumulated findings item 19

---

## Section 1: Transferable Pattern Catalog

Seven pattern categories distilled from 20 patterns across the two surveys. Each entry identifies what transfers to ATHENA's three-stage fault classification IR, what does not, and how it maps to the LFI audit stages and R1-R29 requirements.

---

#### Pattern 1: Counter-Example Traces

**Source systems:** DRAT proofs (SAT Competition format), AIGER witness traces (hardware model checking), Clang Static Analyzer path-sensitive bug reports [RCA/FV §2.3, §3.1]

**Core mechanism:** When a system determines that a property is violated, it produces a structured trace demonstrating the violation. DRAT emits a sequence of clause operations building to the empty clause (unsatisfiability proof). AIGER produces a sequence of input assignments driving a circuit to a bad state, with full state variable values at each step. The Clang Static Analyzer produces annotated execution paths with symbolic constraints showing how a program reaches a bug-triggering state.

**Transferability to ATHENA:** MEDIUM

**What transfers:**
- Refutation-as-evidence structure: when the LFI classifies a failure as theoretical falsification (Stage 3), the evidence can be structured as a chain of independently verifiable steps: (1) hypothesis predicted X, (2) clean implementation produced Y, (3) X and Y contradict under conditions Z. Each step references specific trace data [RCA/FV §2.3, P8].
- Streaming/incremental verification: DRAT proofs are checked one clause at a time without full-proof buffering. This aligns with Rust streaming parsing for Stage 1 implementation checks [RCA/FV §2.3].
- State-transition traces with completeness: AIGER counter-examples include all state variable values at each step, establishing a standard for trace completeness. ATHENA's IR should capture sufficient state at each transition for the LFI to verify validity, and flag incomplete state capture explicitly [RCA/FV §2.3, AIGER section].
- Path constraints as failure conditions: Clang's approach of reporting "under what conditions" a bug manifests ("overflow when force > 1e15 AND timestep > 2fs") makes fault classification more informative than bare "failure at step N" [RCA/FV §3.1, P12].

**What doesn't transfer:**
- Propositional logic substrate: DRAT operates on SAT clauses; AIGER on discrete finite-state circuits. Scientific simulations have continuous, high-dimensional state. The chain structure transfers but the logical machinery is irrelevant [RCA/FV §2.3].
- Minimal witness property: AIGER counter-examples are shortest-path witnesses. ATHENA's traces are complete execution records, not minimal demonstrations of violation.
- Solver-generated nature: DRAT proofs are generated by SAT solvers through automated reasoning. ATHENA's refutation evidence must be constructed from empirical data, a fundamentally different process [RCA/FV §2.3].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): State-transition invariant checking (AIGER pattern) enables per-timestep verification of numerical bounds, conservation laws, and resource constraints. Path constraints report failure conditions precisely.
- Stage 2 (Methodology audit): Not directly applicable. Counter-example traces address "what went wrong," not "was the experiment design adequate."
- Stage 3 (Theory evaluation): Refutation chain structure (DRAT pattern) provides machine-checkable evidence for theoretical falsification. Each inference step from observation to contradiction is independently auditable.

**R1-R29 coverage:** R1 (execution completion via state-transition termination), R6 (numerical status via invariant checking), R17 (comparison result via refutation chain structure), R20 (provenance chain via step-by-step evidence references), R21 (temporal ordering inherent in state sequences), R25 (classification confidence via path constraint specificity)

**Rust/streaming compatibility:** High. DRAT's incremental format is a direct model for streaming verification. State-transition records are naturally append-only. Path constraint expressions are compact structured records. All representable as Rust enums and structs with zero-copy semantics [ADR 001].

---

#### Pattern 2: Entity-Activity-Agent Model

**Source systems:** W3C PROV-DM (W3C Recommendation, 2013), PROV-O (OWL ontology), PROV-CONSTRAINTS (validation framework) [Prov §1-3]

**Core mechanism:** Provenance is modeled as a graph of three core types: Entities (things with fixed aspects -- data, parameters, results), Activities (time-bounded operations -- simulation steps, computations), and Agents (bearers of responsibility -- frameworks, users, systems). Six core relations connect them: `wasGeneratedBy`, `used`, `wasAttributedTo`, `wasDerivedFrom`, `wasAssociatedWith`, `actedOnBehalfOf`. Qualified relations (Usage, Generation, Derivation, Association with Plans) add structured detail about how entities participated in activities and what agents intended. PROV-CONSTRAINTS provides temporal ordering, derivation chain integrity, and uniqueness constraints.

**Transferability to ATHENA:** HIGH (for the data model concepts); LOW (for the technology stack)

**What transfers:**
- Derivation chains (`wasDerivedFrom`) as causal lineage: traversing backward from a failed output Entity to its causal ancestors identifies all entities that contributed to the failure. This is the structural backbone for "which prior events influenced this outcome?" queries [Prov §1, Pattern 1].
- Qualified relations for role encoding: Qualified Usage records *how* an entity participated (e.g., "as-force-field-parameters" vs. "as-gpu-memory-allocation"), providing the closest PROV-DM approximation to layer typing [Prov §1, Qualified Relations].
- Plans as expected behavior: Qualified Association with Plans encodes what an agent intended the activity to do. If the Plan encodes hypothesis predictions, divergence between Plan and actual output is a fault signal for Stage 3 [Prov §1, §5].
- Temporal ordering constraints (PROV-CONSTRAINTS): generation must precede usage, usage occurs within activity bounds, entity generation is unique. These are built-in consistency checks useful for Stage 1 implementation audit [Prov §1, PROV-CONSTRAINTS].
- Agent delegation (`actedOnBehalfOf`): models "framework acted on behalf of user specification," capturing the implementation-theory relationship [Prov §6, Pattern 7].

**What doesn't transfer:**
- Untyped agent responsibility: PROV-DM Agents capture "who is responsible" but not "what kind of responsibility" (theory vs. implementation vs. methodology). Two agents associated with the same activity are structurally indistinguishable. This is the central gap [Prov §1, Mapping assessment].
- RDF/SPARQL technology stack: triple stores, RDF serialization, SPARQL query engines are antithetical to Rust-for-throughput. No mature Rust RDF stores exist at production quality [Prov §3, §4, Anti-Pattern 4].
- Graph-based overhead: faithful PROV-DM implementation involves 3-5 triples per data point, yielding 10^4 to 5x10^7 triples for megabyte-scale traces. Custom Rust graph structures are 10-100x faster for targeted queries [Prov §4].
- No fault semantics: PROV-DM records provenance (what happened), not diagnostics (what went wrong). The LFI's three-way classification output has no PROV-DM representation [Prov §1, Limitations; §6].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): PROV-CONSTRAINTS temporal consistency checks detect implementation-layer anomalies (out-of-order execution, race conditions). Derivation chains trace data pipeline correctness.
- Stage 2 (Methodology audit): Plans provide expected-vs-actual comparison foundation. Agent delegation captures theory-to-implementation responsibility chain. But: no native mechanism for sampling sufficiency or confounder detection.
- Stage 3 (Theory evaluation): Derivation chains identify which theory parameters are in the causal ancestry of failed predictions. Plan divergence detects prediction-observation mismatch. But: Plans are unstructured entities requiring ATHENA-specific formalization.

**R1-R29 coverage:** R8 (observable measurement via Entity), R9 (observable-to-DAG linkage via qualified derivation), R15 (prediction record via Plan), R16 (observation record via Entity), R19 (layer tag -- requires extension, not native), R20 (provenance chain -- native and strong), R21 (temporal ordering via PROV-CONSTRAINTS), R22 (experiment specification linkage via Plan/Association), R24 (queryability via graph traversal), R29 (cross-experiment via Bundles)

**Rust/streaming compatibility:** The data model is streaming-compatible (generation events create nodes incrementally; forward-flowing traces satisfy the "used Entity already exists" requirement). The technology stack (RDF) is not. Recommendation: adopt the conceptual model, implement as Rust-native graph (petgraph or custom adjacency list) [Prov §4, Streaming section; ADR 001].

---

#### Pattern 3: Typed Event Chains

**Source systems:** Chain-of-Event (FSE 2024), distributed tracing (Jaeger/Zipkin, OpenTelemetry) [RCA/FV §1.2]

**Core mechanism:** Typed event nodes connected by directed causal links. Each node carries: event type (from a domain-specific taxonomy), source component, timestamp, severity, observable attributes. Each link carries: link type (direct_call, cascading_failure, resource_contention), confidence score, and evidence references (which log entries or metric correlations support the causal claim). Multiple chains can coexist for the same incident, representing alternative root cause hypotheses. Ranking is transparent and decomposable (chain length, aggregate confidence, symptom coverage). Jaeger/Zipkin add hierarchical span trees (parent-child invocation structure) with tags and structured logs, naturally streaming-compatible.

**Transferability to ATHENA:** HIGH

**What transfers:**
- Typed event nodes map to ATHENA's need for typed trace events classified by layer. The DSL event taxonomy replaces the microservice taxonomy: instead of {error, latency_spike, timeout}, ATHENA uses {framework_exception, numerical_overflow, parameter_violation, theory_prediction_mismatch} [RCA/FV §1.2, CoE section].
- Causal links with evidence references ensure every fault classification is traceable to specific trace data. This addresses the "Biased" LLM RCA failure mode (classifications unsupported by evidence) [RCA/FV §1.2; §1.1, arxiv:2601.22208].
- Multiple coexisting chains support evaluation of alternative fault hypotheses before committing to a classification. The LFI can maintain chains for "implementation fault," "methodology fault," and "theory contradiction" simultaneously, then select based on evidence [RCA/FV §1.2, P3].
- Hierarchical span model (Jaeger/Zipkin) captures natural DSL trace hierarchy: experiment -> phase -> timestep -> kernel call. Spans are emitted individually (streaming-compatible). Tags carry domain metadata without modifying the core structure [RCA/FV §1.2, Jaeger section].
- Decomposable ranking provides transparency the LFI needs for auditable fault classification [RCA/FV §1.2, CoE section].

**What doesn't transfer:**
- Microservice-specific event taxonomy: CoE's types (error, latency_spike, timeout) are domain-specific and must be replaced with DSL-specific types [RCA/FV §1.2].
- Request-response optimization: Jaeger/Zipkin spans are designed for web request lifecycles. DSL simulations have millions of timesteps without clear request-response boundaries. Adaptation needed for batch simulation patterns [RCA/FV §1.2].
- Post-mortem construction: CoE operates on completed incidents. ATHENA needs streaming event chain construction during experiment execution [RCA/FV §1.2].
- Confidence scoring model: CoE's link confidence requires a scoring methodology. ATHENA's confidence model must account for DSL-specific evidence quality, not microservice metric correlation [RCA/FV §1.2].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): Event chains trace failure propagation from root cause to symptom through implementation-layer components. Typed events enable filtering to implementation-layer events only.
- Stage 2 (Methodology audit): Span hierarchy captures experiment design structure (which phases were executed, in what order). Alternative chains support "was the methodology adequate?" reasoning.
- Stage 3 (Theory evaluation): Event chains build refutation evidence (chain from hypothesis prediction through clean execution to contradictory observation). Evidence references ensure the refutation is grounded in specific trace data.

**R1-R29 coverage:** R1-R2 (execution status and exceptions as typed events), R6 (numerical status as typed events), R8 (observable measurement as chain endpoints), R17 (comparison result via chain structure), R19 (layer tag via event typing), R20 (provenance chain via evidence references on links), R21 (temporal ordering via timestamps), R24 (queryability via typed traversal), R25 (classification confidence via link confidence scores)

**Rust/streaming compatibility:** High. Event nodes are Rust structs with enum-typed fields. Causal links are edges in an incrementally-constructed graph. Spans are emitted and parsed individually. Tag bags use `HashMap<String, Value>`. All compatible with zero-copy construction [ADR 001; RCA/FV §1.2].

---

#### Pattern 4: SSA Data Flow

**Source systems:** LLVM IR (Single Static Assignment form) [RCA/FV §2.1, LLVM section]

**Core mechanism:** Every variable is assigned exactly once, making data flow explicit. Each value has a unique, unambiguous definition, and all uses are direct references to that definition. Use-def chains are trivially computable. Code is organized into basic blocks (contiguous instruction sequences with single entry/exit) forming a control flow graph. Metadata attachments on instructions carry analysis results without modifying core semantics. Intrinsic functions provide extensibility for domain-specific operations.

**Transferability to ATHENA:** MEDIUM-HIGH

**What transfers:**
- SSA-like event identification: each trace event receives a unique, immutable identifier with explicit data-flow links to prior events. "Which prior events influenced this outcome?" becomes a structural traversal, not a search problem. Directly supports Stage 1 data pipeline correctness checking [RCA/FV §2.1, P5].
- Typed instructions with metadata: DSL trace events as typed operations (force_calculation, parameter_update, data_load) with metadata bags for domain-specific annotations. The type system enables stage-appropriate filtering [RCA/FV §2.1].
- Basic block structure for trace segmentation: traces segmented into contiguous same-layer operation sequences with single entry/exit. Enables hierarchical analysis: check block-level properties first, drill into individual events only on block failure [RCA/FV §2.1].

**What doesn't transfer:**
- Strict single-assignment: trace parameters are legitimately updated multiple times during simulation (temperature ramp, adaptive timestep). Requires versioned identifiers (parameter_v1, parameter_v2) rather than strict SSA [RCA/FV §2.1, P5 limitations].
- Compilation semantics: LLVM IR represents transformations ("how to compute"); ATHENA's IR represents observations ("what happened"). The structural patterns transfer but the semantic interpretation differs [RCA/FV §2.1].
- No "why": LLVM IR does not carry the purpose of a computation, only its mechanics. ATHENA's IR must carry both what was specified and what happened [RCA/FV §2.1].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): Use-def chains trace data pipeline correctness. Given a failed output, traverse backward through definitions to find which input or computation step is responsible. Block-level analysis enables efficient first-pass screening.
- Stage 2 (Methodology audit): Not directly applicable. SSA data flow tracks computation mechanics, not experimental design adequacy.
- Stage 3 (Theory evaluation): Not directly applicable, though unique event IDs support cross-referencing observations to predictions.

**R1-R29 coverage:** R4 (actual input observation via defined values), R5 (input validation via use-def chain inspection), R19 (layer tag via instruction type), R20 (provenance chain via SSA definitions), R21 (temporal ordering via block sequencing and instruction ordering), R24 (queryability via use-def chain traversal)

**Rust/streaming compatibility:** High. Monotonic event ID counter is trivial in Rust. Use-def chains are adjacency list edges. Basic blocks are `Vec<Event>` with entry/exit metadata. Versioned identifiers use `(name, version)` tuples [ADR 001; RCA/FV §6.4].

---

#### Pattern 5: Multi-Level Abstraction / Dialect System

**Source systems:** MLIR (Multi-Level Intermediate Representation, LLVM project) [RCA/FV §2.1, MLIR section]

**Core mechanism:** Operations are organized into namespaced collections called dialects. Each dialect represents a specific abstraction level or domain. A single IR module can contain operations from multiple dialects simultaneously (multi-level coexistence). Progressive lowering transforms high-level operations into lower-level equivalents through a sequence of passes. Regions and blocks support recursive nesting. Each dialect defines its own types and attributes. Custom dialects can be defined for any domain.

**Transferability to ATHENA:** HIGH -- the most structurally relevant pattern across both surveys.

**What transfers:**
- Dialect system maps directly to ATHENA's layer separation. Three dialects: `theory` (user specifications, hypothesis predictions, causal claims -- e.g., set_force_field, define_ensemble, specify_observable), `methodology` (experiment design, variable control, sampling configuration), `implementation` (framework execution, resource management, numerical computation -- e.g., allocate_memory, compute_forces, write_checkpoint). The DSL's API separation maps directly to dialect boundaries [RCA/FV §2.1, P1].
- Multi-level coexistence: a single trace IR carries both theory-level operations and implementation-level events, linked by explicit lowering relationships. When the user specifies `set_temperature(300K)`, the IR records both the theory-level operation and the implementation events it generated (thermostat initialization, velocity rescaling). The LFI checks: did the implementation faithfully realize the theory-level operation? [RCA/FV §2.1, P1]
- Progressive lowering as audit hierarchy: the three audit stages map to three abstraction levels. Level 0 (implementation) audited first, Level 1 (methodology) second, Level 2 (theory) last. Each level's consistency is verified before proceeding to the next [RCA/FV §2.1, P6].
- Region nesting for trace structure: simulation hierarchy (experiment -> phase -> timestep -> force calculation) maps to MLIR's recursive region structure [RCA/FV §2.1].
- Extensibility per DSL: each target DSL defines its own dialect within a shared framework. OpenMM, GROMACS, and VASP get DSL-specific dialects while sharing the common three-layer audit interface [RCA/FV §2.1].
- Dialect tags as Rust enum variants: zero-cost abstraction enabling structural routing to the correct audit stage [RCA/FV §6.4].

**What doesn't transfer:**
- Compile-time framework: MLIR does not natively support streaming or runtime trace construction. The dialect pattern transfers but the implementation machinery (passes, pattern matching, rewriting) does not [RCA/FV §2.1].
- Computational semantics: MLIR operations represent transformations; ATHENA trace events are observations. Lowering in MLIR means "compile higher-level op into lower-level ops." In ATHENA, the "lowering" relationship means "this theory-level specification was realized by these implementation-level events" -- a causal relationship, not a compilation step [RCA/FV §2.1].
- Dialect boundary clarity: not all trace events cleanly map to exactly one dialect. Boundary parameters (e.g., GROMACS `dt`, VASP `PREC`) serve dual roles [RCA/FV §2.1, P1 limitations; FINDINGS.md DSL surveys].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): Filter to implementation dialect; check all implementation-layer operations for exceptions, numerical failures, resource issues. Dialect tag enables structural routing -- only implementation events are inspected.
- Stage 2 (Methodology audit): Filter to methodology dialect; check experiment design operations for sampling sufficiency, confounder control, variable coverage. Cross-reference against theory dialect to verify the methodology tests the right causal claims.
- Stage 3 (Theory evaluation): Filter to theory dialect; compare theory-level predictions against observations. The lowering relationship traces which theory operations are affected by which implementation/methodology outcomes.

**R1-R29 coverage:** R1-R7 (all Stage 1 requirements -- implementation dialect carries these), R8-R14 (all Stage 2 requirements -- methodology dialect carries these, cross-referenced against theory dialect), R15-R18 (all Stage 3 requirements -- theory dialect carries predictions, compared against observations), R19 (layer tag -- this IS the dialect tag, the primary mechanism), R21 (temporal ordering within and across dialects), R24 (queryability via dialect-based filtering and routing)

**Rust/streaming compatibility:** High. Dialect tags are enum variants (`enum Layer { Theory, Methodology, Implementation }`). Operations are structs parameterized by dialect. Region nesting uses tree structures (arena-allocated). Lowering relationships are cross-references between events. All compatible with streaming construction [ADR 001; RCA/FV §6.4].

---

#### Pattern 6: Specification-Implementation Contracts

**Source systems:** Boogie (from Dafny, VCC, Corral), Why3 (from Frama-C, SPARK), Facebook Infer (compositional bi-abduction) [RCA/FV §2.2, §3.3]

**Core mechanism:** Each procedure has a contract consisting of: `requires` (preconditions -- what must hold before execution), `ensures` (postconditions -- what must hold after execution), `modifies` (what the implementation may change), and optionally `invariant` (what must hold throughout). The verifier checks the implementation body against the contract. When verification fails, a counter-example (concrete violating execution) is produced. Why3 adds ghost state (specification-only variables with no runtime counterpart) and theories (reusable mathematical vocabularies). Infer adds compositional analysis (per-procedure summaries composable for interprocedural reasoning) and bi-abductive inference (deriving preconditions from implementation behavior).

**Transferability to ATHENA:** HIGH

**What transfers:**
- Experiment specifications as contracts: an ATHENA experiment has a specification (hypothesis predictions, controlled variables, expected observables) and an execution (what happened in the DSL framework). `requires` maps to preconditions on data loading, parameter validity, hardware availability. `ensures` maps to predicted observable values and expected relationships. `modifies` maps to which state variables the simulation updates [RCA/FV §2.2, P2].
- Three-level contract checking maps to three-stage audit: Stage 1 checks execution against implementation-level contract terms. Stage 2 checks whether the contract is adequate to test the hypothesis. Stage 3 checks whether contract-satisfying execution contradicts predictions [RCA/FV §2.2, P2; §6.1].
- Ghost state for methodological metadata: properties like "sampling sufficient," "confounders controlled," "variable coverage adequate" are not in the execution trace but are needed for Stage 2. They are ghost variables populated from the experiment specification and DAG, not from trace data [RCA/FV §2.2, Why3 section, P4].
- Havoc for unobserved state: when the trace does not capture a variable's value, represent it as `havoc` (explicitly unknown) rather than silently absent. This prevents the LFI from making assumptions about unobserved state. Addresses the "Stalled" failure mode from arxiv:2601.22208 and the incomplete observability risk (ARCHITECTURE.md §8.4) [RCA/FV §2.2, P7].
- Compositional trace summaries: analyze each simulation phase independently, produce a phase summary, compose summaries for experiment-level classification. Enables streaming analysis of megabyte-scale traces without full-trace buffering [RCA/FV §3.3, P10].
- Theory libraries for domain axioms: conservation laws, thermodynamic constraints, and other domain axioms as reusable specification vocabularies referenced during Stage 3 evaluation [RCA/FV §2.2, Why3 theories].

**What doesn't transfer:**
- Static/deductive verification: Boogie/Why3 verify against all possible executions. ATHENA checks against a single actual execution trace. The verification conditions become empirical checks, not logical proofs [RCA/FV §2.2].
- First-order logic limitation: Boogie's logic is first-order. ATHENA's causal reasoning involves graph structures and probabilistic relationships exceeding first-order expressiveness [RCA/FV §2.2].
- Full functional correctness: Boogie/Why3 verify correctness. ATHENA classifies faults (a much weaker property) [RCA/FV §2.2, Why3 non-transferable].
- Separation logic specificity: Infer's heap reasoning is Java/C-specific and irrelevant to numerical simulation data [RCA/FV §3.3].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): Check trace events against implementation-level preconditions (`requires`: valid inputs loaded, correct precision mode) and postconditions (`ensures`: no exceptions, numerical health maintained, resources not exhausted). `havoc` marks unobservable elements.
- Stage 2 (Methodology audit): Ghost state carries methodological metadata (sampling sufficiency, confounder control). Check whether the contract (experiment specification) is adequate to test the hypothesis. This is the level where the experiment design is audited against the causal claims.
- Stage 3 (Theory evaluation): Check whether contract-satisfying execution (Stages 1-2 passed) produced results contradicting theoretical predictions (`ensures` at theory level). Domain axiom theories provide the reference framework.

**R1-R29 coverage:** R1-R5 (execution and input validation via contract checking), R7 (resource status via implementation preconditions), R8-R9 (observable measurement via postconditions), R10-R11 (intervention specification via contract structure), R12-R14 (sampling and confounder metadata via ghost state), R15 (prediction record via theory-level `ensures`), R22 (experiment specification linkage -- contract IS the specification), R23 (hypothesis linkage via theory-level contract terms), R25-R26 (confidence and observability gaps via `havoc` representation)

**Rust/streaming compatibility:** High. Contracts are structured records parsed once from the experiment specification before trace processing begins. Ghost state is populated pre-trace-parsing. `havoc` is a variant in a value representation enum (`enum Value { Known(f64), Havoc, ... }`). Compositional summaries are produced per-phase, enabling streaming. Contract checking is a set of conditional comparisons on structured records [ADR 001; RCA/FV §6.4].

---

#### Pattern 7: Causal Dependency / Conformance Checking

**Source systems:** Process mining (XES, IEEE 1849-2016), conformance checking (alignment-based, token replay, declarative), ProvONE prospective/retrospective separation, scientific workflow systems (Kepler, Taverna, VisTrails, Galaxy, CWL) [Prov §3, §4, §5]

**Core mechanism:** Two complementary ideas. First, conformance checking compares an expected process model (normative) against an actual execution log (descriptive). Alignment-based conformance finds the optimal alignment between log and model, classifying each deviation as "move on log" (unexpected event) or "move on model" (missing event). This directly addresses expected-vs-actual comparison. Second, ProvONE separates prospective provenance (the workflow definition: Programs, Ports, Channels, Controllers) from retrospective provenance (the execution trace: Executions, actual data), providing structural backbone for specification-vs-execution comparison. Typed Ports classify inputs/outputs by domain role.

**Transferability to ATHENA:** MEDIUM

**What transfers:**
- Alignment-based conformance checking for Stages 1 and 2: encode expected implementation behavior as a normative model. "Moves on log" (unexpected events) identify implementation anomalies. "Moves on model" (missing events) identify incomplete execution. Encode expected methodology as a second normative model for Stage 2 [Prov §3, Conformance Checking section].
- Prospective/retrospective separation: DSL frameworks inherently separate user specification (prospective) from execution (retrospective). This two-way split provides structural backbone, though ATHENA needs a three-way split [Prov §2, ProvONE section].
- Typed Ports for parameter classification: each computational step has typed I/O Ports classified as theory-Ports (force field parameters), methodology-Ports (sampling frequency), or implementation-Ports (GPU device). Port typing at DSL API boundaries structurally encodes the three-layer distinction [Prov §2, ProvONE Ports; Prov §6, Pattern 3].
- Provenance graph versioning for iterative refinement: each ATHENA cycle produces a provenance graph. VisTrails-style version trees link graphs across cycles, supporting "what changed between working and failing runs?" queries. PROV-DM Bundles group provenance by cycle [Prov §5, VisTrails section; Prov §6, Pattern 6].
- Workflow evolution tracking: VisTrails records diffs between workflow versions, relevant to ATHENA's iterative hypothesis refinement across the 50-cycle litmus test [Prov §5, VisTrails section].

**What doesn't transfer:**
- Two-way split only: ProvONE provides specification-vs-execution, not theory-vs-methodology-vs-implementation. The prospective layer collapses theory and methodology together [Prov §2, ProvONE assessment].
- Control-flow-only conformance: process mining discovers and checks control flow structure, not data values. It detects "step X was skipped" but not "step X produced wrong energy values." ATHENA needs both structural and value-level deviation detection [Prov §3, Discovery Algorithms assessment].
- Coarse granularity: scientific workflow systems (Kepler, Taverna, Galaxy) operate at the tool/service level, not at the DSL-internal semantic level ATHENA requires [Prov §5, Summary Table].
- Pre-defined process models: conformance checking requires a normative model to be constructed per experiment. This adds complexity and depends on correct specification [Prov §3, Conformance Checking limitations].

**LFI Stage mapping:**
- Stage 1 (Implementation audit): Conformance checking with an expected implementation process model detects missing steps (incomplete execution) and unexpected events (crashes, anomalies). Typed Ports filter to implementation-layer parameters.
- Stage 2 (Methodology audit): Conformance checking with an expected experimental protocol detects methodology deviations (steps skipped, wrong order, insufficient sampling). Prospective/retrospective comparison reveals specification-execution gaps.
- Stage 3 (Theory evaluation): Cross-cycle provenance versioning enables "what changed between runs?" analysis. Prospective Plans encode predictions; comparison against retrospective results detects contradictions. But conformance checking is primarily structural, not value-level.

**R1-R29 coverage:** R3-R5 (input specification via prospective layer, actual input via retrospective layer, validation via conformance alignment), R8 (observable measurement via Port outputs), R10 (intervention specification via prospective model), R19 (layer tag via Port typing), R21 (temporal ordering via XES timestamps and lifecycle transitions), R22 (experiment specification via prospective provenance/workflow definition), R28 (interventional vs. observational via conformance model structure), R29 (cross-experiment via version trees and Bundles)

**Rust/streaming compatibility:** Moderate. XES event logs are naturally streaming (append-only events). Conformance checking can operate incrementally for prefix-based detection (detect deviations as events arrive). Port typing is static metadata (parsed once). Provenance graph construction is incremental. But: alignment-based conformance for optimal global alignment requires the full trace and model, conflicting with streaming. For ATHENA, prefix-based conformance (detecting deviations early) is more useful than optimal global alignment [ADR 001; Prov §4, Streaming section].

---

## Section 2: Stage-by-Stage Requirements Mapping

### Stage 1 (Implementation Audit) -- Detecting Execution Errors

**Requirements:** R1 (execution completion), R2 (exception/error events), R3-R5 (input specification, actual input, validation), R6 (numerical status), R7 (resource/environment status)

**Contributing patterns:**

| Pattern | Contribution | Mechanism |
|:---|:---|:---|
| P5: Dialect system | PRIMARY. Routes events to Stage 1 via implementation dialect tag. | Implementation-layer events are structurally identified by dialect, enabling efficient stage-specific filtering. |
| P6: Contracts | Checks execution against implementation-level preconditions and postconditions. | `requires`: valid inputs, correct precision. `ensures`: no exceptions, bounded numerics. |
| P3: Typed event chains | Traces failure propagation through implementation components. | Typed events with causal links show how an error in component A cascaded to component B. |
| P1: Counter-example traces | Per-timestep invariant checking; path constraints for failure conditions. | State-transition model enables checking conservation laws and numerical bounds at each step. |
| P4: SSA data flow | Traces data pipeline correctness via use-def chains. | "Which inputs influenced this failed output?" is a structural traversal. |
| P7: Conformance checking | Detects missing or unexpected execution steps. | Alignment against expected implementation process identifies structural deviations. |
| P2: Entity-Activity-Agent | PROV-CONSTRAINTS temporal consistency checks. | Out-of-order execution, temporal inconsistencies flagged automatically. |

**Coverage assessment:** Stage 1 is the best-covered stage. All seven patterns contribute. R1-R2 (execution status, exceptions) are directly served by P3 (typed events) and P6 (contract postconditions). R3-R5 (input validation) are served by P6 (contract preconditions), P4 (SSA data flow), and P7 (conformance checking). R6 (numerical status) is served by P1 (invariant checking) and P3 (typed numerical events). R7 (resource status) is served by P3 (typed resource events) and P6 (implementation preconditions).

**Gaps:** No significant gaps for Stage 1. The primary challenge is per-DSL implementation: which specific events constitute "implementation dialect" operations varies across OpenMM, GROMACS, and VASP. This is an adaptation problem, not a pattern gap.

---

### Stage 2 (Methodology Audit) -- Spec-vs-Actual Comparison

**Requirements:** R8-R9 (observable measurement and DAG linkage), R10-R11 (intervention specification and DAG linkage), R12 (sampling metadata), R13-R14 (controlled variables and confounder query support)

**Contributing patterns:**

| Pattern | Contribution | Mechanism |
|:---|:---|:---|
| P5: Dialect system | Routes events to Stage 2 via methodology dialect tag. | Methodology-layer operations identified by dialect. |
| P6: Contracts | Ghost state carries methodological metadata not in the trace. | Sampling sufficiency, confounder control, variable coverage as ghost variables populated from experiment specification and DAG. |
| P7: Conformance checking | Compares expected experimental protocol against actual execution. | Alignment against methodology model detects deviations (steps skipped, wrong order, insufficient repetition). |
| P2: Entity-Activity-Agent | Plans encode expected behavior; qualified roles encode parameter purpose. | Plan divergence signals methodology failure. Port typing classifies parameters. |
| P3: Typed event chains | Alternative chains support "was the methodology adequate?" reasoning. | Multiple hypothesis chains evaluated against methodology criteria. |

**Coverage assessment:** Stage 2 is the weakest stage. R8-R9 (observable measurement and DAG linkage) are served by P2 (Entities as observations) and P3 (typed event endpoints), but R9 requires DAG cross-referencing that no pattern fully automates. R10-R11 (intervention specification and DAG linkage) are served by P6 (contract structure) and P7 (prospective provenance), but R11 requires causal graph reasoning beyond what the IR patterns provide. R12 (sampling metadata) is served by P6 (ghost state), but the metadata itself must come from domain rules external to the trace. R13-R14 (controlled variables and confounder detection) are served by P6 (ghost state) and P7 (conformance checking), but R14 depends on DAG accuracy (ARCHITECTURE.md §8.5).

**Gaps:**
- **Methodology detection rules are not provided by any pattern.** All patterns provide *structure* for encoding methodological metadata but none provide *rules* for determining when methodology is adequate. "Is sampling sufficient?" requires domain-specific statistical criteria (convergence tests, autocorrelation analysis) that are external to IR design patterns. This is the expected weakest point: methodology adequacy assessment requires domain knowledge that IR structure alone cannot provide.
- **DAG cross-referencing (R9, R11, R14)** is a coordination requirement between the IR and the Causal Graph Manager. No pattern addresses the shared ontology/naming convention needed for this join.
- **R28 (interventional vs. observational distinction)** is partially addressed by P7 (conformance model structure can encode whether a parameter was actively varied), but requires explicit representation in the IR that none of the existing patterns natively provide.

---

### Stage 3 (Theory Evaluation) -- Prediction-vs-Observation

**Requirements:** R15 (prediction record), R16 (observation record), R17 (comparison result), R18 (causal implication mapping)

**Contributing patterns:**

| Pattern | Contribution | Mechanism |
|:---|:---|:---|
| P5: Dialect system | Routes events to Stage 3 via theory dialect tag. Lowering relationships trace which theory operations are affected by implementation/methodology outcomes. | Theory-layer predictions and observations identified by dialect. |
| P6: Contracts | Theory-level `ensures` encode predicted outcomes. Domain axiom theories provide reference framework. | Contract satisfaction checked at theory level: did the clean execution produce results matching predictions? |
| P1: Counter-example traces | Refutation chain structure for machine-checkable falsification evidence. | Step-by-step chain from prediction through observation to contradiction, each step independently verifiable. |
| P2: Entity-Activity-Agent | Plans encode hypothesis predictions. Derivation chains identify which theory parameters are in the causal ancestry of failed predictions. | Plan divergence signals theory-observation mismatch. |
| P3: Typed event chains | Evidence-backed chains from prediction to contradictory observation. | Each causal link in the refutation carries evidence references. |

**Coverage assessment:** Stage 3 is moderately covered. R15 (prediction record) is served by P6 (theory-level `ensures`) and P2 (Plans), though neither provides a native formalization for quantitative scientific predictions -- the prediction vocabulary must be ATHENA-specific. R16 (observation record) is served by P3 (typed events) and P2 (Entities). R17 (comparison result) is served by P6 (contract checking) and P1 (refutation chains), but the quantitative comparison logic (effect sizes, statistical divergence measures, confidence intervals) is novel and not provided by any pattern. R18 (causal implication mapping) is partially served by P2 (derivation chains back to theory parameters) and P5 (lowering relationships from theory to implementation), but the specific mapping from "this prediction was falsified" to "these DAG edges should be pruned" requires novel logic.

**Gaps:**
- **Quantitative comparison formalization (R17).** DRAT-style refutation is propositional; scientific falsification is probabilistic. What constitutes a "step" in a quantitative refutation chain (effect size calculation? Bayes factor? z-score?) is an open research question [FINDINGS.md "What We Don't Know" item 9].
- **Causal implication logic (R18).** The mapping from a contradicted prediction to specific DAG edges requiring update is novel. Derivation chains trace causal ancestry, but the decision of which edges to prune/reweight requires Bayesian reasoning beyond what any IR pattern provides.
- **Prediction vocabulary.** Encoding quantitative scientific predictions (distributions, directional effects, tolerance intervals) requires an ATHENA-specific vocabulary. Neither PROV-DM Plans nor Boogie `ensures` provide this natively.

---

### Cross-Cutting Requirements (R19-R29)

**R19 (Layer tag):** PRIMARY mechanism is P5 (dialect system). Dialect tags directly implement layer classification. P3 (typed events) provides a secondary mechanism via event type hierarchies. P2 (qualified relations with role encoding) provides an attributional fallback. Assessment: P5 is necessary and sufficient for R19; it is the load-bearing structural distinction [FINDINGS.md item 21].

**R20 (Provenance chain):** PRIMARY mechanism is P2 (Entity-Activity-Agent derivation chains). PROV-DM's `wasDerivedFrom` natively provides causal lineage tracing. P3 (evidence references on causal links) ensures each step is grounded in trace data. P4 (SSA use-def chains) provides data-flow-level provenance. Assessment: Well-covered by multiple patterns.

**R21 (Temporal ordering):** P2 (PROV-CONSTRAINTS temporal ordering), P3 (event timestamps), P1 (state-transition sequences), P7 (XES temporal extensions). Assessment: Well-covered; all patterns incorporate temporal information. The key design decision is whether to use simulation time, wall-clock time, or logical sequence numbers (likely all three, with simulation time primary).

**R22 (Experiment specification linkage):** P6 (contracts -- the contract IS the specification) and P7 (prospective provenance -- the workflow definition IS the specification). Assessment: Well-covered.

**R23 (Hypothesis linkage):** P6 (theory-level contract terms reference hypothesis elements) and P2 (Plans encode hypothesis predictions). Assessment: Covered but requires explicit cross-referencing mechanism in the IR schema.

**R24 (Queryability):** P5 (dialect-based filtering for stage relevance), P4 (use-def chains for data flow queries), P2 (graph traversal for causal queries), P3 (typed event filtering for event-type queries). Assessment: Well-covered structurally; performance depends on indexing decisions in Rust implementation.

**R25 (Classification confidence):** P3 (link confidence scores), P6 (`havoc` for incomplete information), P1 (path constraint specificity). Assessment: Partially covered; confidence computation logic is novel.

**R26 (Observability gap record):** P6 (`havoc` for unobserved state) is the primary mechanism. Assessment: Well-covered by a single specific pattern. The key is systematic application: every expected IR element that cannot be populated must be marked `havoc`, not silently omitted.

**R27 (Confounder-as-methodological classification):** P6 (ghost state for confounder metadata) + P5 (methodology dialect for classification) + P7 (conformance checking for confounder detection). Assessment: Structurally supported; domain rules needed.

**R28 (Interventional vs. observational distinction):** No pattern natively provides this. Must be added as an explicit tag on observation records. Assessment: Gap. Requires novel IR element.

**R29 (Cross-experiment queryability):** P2 (PROV-DM Bundles for per-experiment grouping), P7 (VisTrails version trees for cross-cycle comparison). Assessment: Structurally supported; implementation requires experiment_cycle_id on every IR element.

**Cross-cutting gaps:**
- R28 (interventional vs. observational) is unaddressed by existing patterns.
- R25 (classification confidence) is partially addressed but needs novel computation logic.
- The shared variable naming ontology between IR and DAG (needed for R9, R11, R14, R18) is not addressed by any pattern.

---

## Section 3: Anti-Pattern Registry

#### Anti-Pattern: Specification-Implementation Conflation

**Severity:** CRITICAL
**Source:** RCA/FV survey [RCA/FV §5, AP2]; Provenance survey [Prov §6, Anti-Pattern 3]
**Description:** Representing specification and implementation in the same namespace without structural distinction. Treating "what was intended" and "what happened" as the same kind of entity. Observed in LLM-based RCA systems where specifications and observations are both embedded in unstructured prompts (arxiv:2403.04123), and in systems that log only execution events without recording the experiment specification.
**Why harmful for ATHENA:** The entire three-stage audit depends on separating specification from execution. If the IR conflates them, the LFI cannot determine whether a discrepancy indicates a specification violation (implementation bug, Stage 1) or a specification satisfaction that contradicts predictions (theoretical falsification, Stage 3). A system with temperature_setting=300K and temperature_actual=305K must distinguish these as separate entities. If both are recorded as "temperature=X" in the same namespace, the LFI cannot detect the discrepancy. This is the single most harmful anti-pattern for ATHENA's design -- it directly disables the three-way fault classification that is ATHENA's architectural differentiator [FINDINGS.md item 11].
**How to avoid:** Enforce structural separation through the dialect system (Pattern 5). Specification elements live in the theory/methodology dialects; execution elements live in the implementation dialect. Cross-references between dialects use explicit linkage relations (lowering relationships), never namespace sharing. At the schema level: every IR element type exists in exactly one dialect. At the implementation level: Rust type system enforces dialect separation via distinct struct types for specification vs. execution records, making conflation a compile-time error.

---

#### Anti-Pattern: Post-Mortem-Only Design

**Severity:** HIGH
**Source:** RCA/FV survey [RCA/FV §5, AP1]
**Description:** IRs designed exclusively for post-mortem analysis that cannot be constructed or queried incrementally during execution. The entire trace must be buffered before any analysis begins.
**Why harmful for ATHENA:** Megabyte-scale traces under Rust zero-copy/streaming parsing (ADR 001) must not require full buffering. Early fault detection (identifying an implementation crash at timestep 100 of a million-timestep simulation) is only possible with incremental IR construction. Post-mortem-only design creates memory pressure (the entire trace in RAM), latency (no results until simulation completes), and inability to provide real-time monitoring feedback.
**How to avoid:** Adopt streaming-compatible patterns: compositional trace summaries (Pattern 6, P10) produce per-phase summaries enabling streaming analysis. Hierarchical spans (Pattern 3) are emitted individually. DRAT-style incremental verification (Pattern 1) processes events one at a time. Design the IR with append-only construction: new events extend the graph; previously emitted events are immutable. Stage 1 (implementation audit) operates in full streaming mode. Stage 2 (methodology audit) operates in phase-level streaming. Stage 3 (theory evaluation) may require full-trace access but should use phase summaries where possible [RCA/FV §5, AP1 mitigation].

---

#### Anti-Pattern: Full-Granularity Entity Recording

**Severity:** HIGH
**Source:** Provenance survey [Prov §6, Anti-Pattern 1]
**Description:** Recording every floating-point value, every memory allocation, and every intermediate computational step as a separate provenance Entity with full derivation chains. At DSL simulation scale (millions of timesteps, thousands of atoms), this produces provenance graphs with 10^8+ nodes.
**Why harmful for ATHENA:** Storage becomes gigabytes; queries become seconds to minutes; incremental construction requires sustained high write throughput that overwhelms the Rust parsing pipeline. The LFI needs to trace causal chains for a specific failure, not analyze the entire simulation provenance. Over-recording creates a needle-in-a-haystack problem where relevant causal chains are buried in irrelevant provenance.
**How to avoid:** Selective recording at DSL API-call granularity, not internal computation granularity. Record simulation steps, parameter changes, observable outputs, and error events. Record fine-grained provenance only in regions flagged as potentially faulty (adaptive granularity). Use compositional summaries (Pattern 6, P10) to compress phases. When compression is applied, it must be layer-aware: implementation-layer events can be summarized for methodology audit but must be preserved in full for implementation audit. The target is 10^3-10^5 nodes per experiment, not 10^8 [Prov §6, AP1 alternative].

---

#### Anti-Pattern: Untyped Agent Proliferation

**Severity:** MEDIUM
**Source:** Provenance survey [Prov §6, Anti-Pattern 2]
**Description:** Creating a separate Agent for every software component involved in a simulation (Python interpreter, CUDA driver, OpenMM kernel, file system, etc.) without a classification hierarchy that groups them by layer.
**Why harmful for ATHENA:** With dozens of untyped agents associated with every activity, determining which agent's failure caused the outcome requires exhaustive inspection. The flat Agent model provides no structural guidance for the outside-in audit order. The LFI must check implementation agents before methodology agents before theory agents; without agent typing, the audit order cannot be derived from the graph structure, forcing reliance on external metadata.
**How to avoid:** Strict three-layer agent classification. Every agent is typed as exactly one of {TheoryAgent, MethodologyAgent, ImplementationAgent}. The typing is structural (a Rust enum variant), not attributional (a string metadata tag). This enables the LFI to query "all implementation-layer agents" directly. For components that span layers (boundary parameters), assign the agent to the layer where its failure would have the most diagnostic impact, and add an explicit cross-layer annotation [Prov §6, AP2 alternative].

---

#### Anti-Pattern: Flat Event Namespace

**Severity:** MEDIUM
**Source:** RCA/FV survey [RCA/FV §5, AP5]
**Description:** Treating all trace events as the same kind of entity, differing only in attributes. No structural distinction between "out of memory" (implementation), "insufficient sampling" (methodology), and "temperature reached equilibrium" (theory-relevant observation). Generic log aggregation systems (ELK, Splunk) exhibit this pattern.
**Why harmful for ATHENA:** The three-stage audit requires routing events to the correct audit stage. If all events are flat, routing requires per-event attribute inspection -- fragile (attributes may be ambiguous about layer) and slow (every event examined by every stage). A typed hierarchy enables structural routing: implementation events go only to Stage 1, methodology events only to Stage 2.
**How to avoid:** Combine Pattern 5 (dialect-based layer separation) with Pattern 3 (typed event nodes). Define a type hierarchy rooted in the three audit stages. Every event type belongs to exactly one dialect. Use Rust's enum + match pattern for exhaustive stage routing. The compiler enforces that every event type has a defined audit stage handler [RCA/FV §5, AP5 mitigation].

---

#### Anti-Pattern: Binary Pass/Fail

**Severity:** HIGH
**Source:** RCA/FV survey [RCA/FV §5, AP6]
**Description:** Representing outcomes as binary (pass/fail) or scalar (accuracy score) without characterizing the nature of the failure. This is the pattern used by generation-first AI co-scientist systems (Sakana V2 uses retry-prune with scalar rewards; Google Co-Scientist uses Elo ranking; AI2 CodeScientist uses genetic fitness). Also present in CI/CD test result reporting.
**Why harmful for ATHENA:** This is the fundamental anti-pattern ATHENA exists to avoid (ARCHITECTURE.md §2.1). A binary/scalar outcome collapses the three-way failure ambiguity. If the IR represents outcomes as pass/fail, it has already lost the information needed for fault classification. An experiment that "failed" because the GPU ran out of memory (implementation) is fundamentally different from one that "failed" because the predicted binding energy did not match observation (theory), but binary pass/fail treats them identically.
**How to avoid:** Represent outcomes as structured failure records containing: the observation, the prediction, the discrepancy, the execution context, AND the fault classification (implementation/methodology/theory) with supporting evidence chains. Pattern 6 (contracts: what was expected vs. what happened) and Pattern 3 (typed event chains: the causal path to the failure) together provide this structure. The IR must never reduce a structured failure to a scalar -- the three-way classification is the IR's primary output [RCA/FV §5, AP6 mitigation].

---

#### Anti-Pattern: Implicit Causal Ordering

**Severity:** MEDIUM
**Source:** RCA/FV survey [RCA/FV §5, AP3]; Provenance survey [Prov §6, Anti-Pattern 5]
**Description:** Representing events in temporal order only, with causal relationships inferred ad-hoc rather than structurally encoded. "Event A happened before event B" is treated as evidence that A caused B.
**Why harmful for ATHENA:** Temporal ordering is necessary but not sufficient for fault classification. Two events may be temporally proximate but causally independent (a GPU memory warning at timestep 4999 and an energy divergence at timestep 5000 may be independent, or the memory issue may have caused the divergence). Without structural causal encoding, the LFI must infer causality from temporal proximity -- the same heuristic that produces the "Confused" failure mode in LLM-based RCA (arxiv:2601.22208).
**How to avoid:** Encode causal relationships explicitly via Pattern 3 (typed causal links with evidence references) and Pattern 2 (PROV-DM `wasDerivedFrom` derivation chains). Use the DSL's API structure to derive structural causal links: when a theory-layer operation triggers implementation-layer events through a known API call, that call constitutes a structural causal link. Always record both temporal and causal ordering; never assume one implies the other [RCA/FV §5, AP3 mitigation; Prov §6, AP5].

---

#### Anti-Pattern: Lossy Compression Without Principled Selection

**Severity:** HIGH
**Source:** RCA/FV survey [RCA/FV §5, AP4]
**Description:** Reducing trace size by discarding events without a principled strategy for what to keep. LLM context window limitations force this in LLM-based systems. Some log aggregation systems apply aggressive sampling or tail-dropping.
**Why harmful for ATHENA:** If the IR discards events critical for distinguishing implementation faults from methodology faults from theory contradictions, the LFI's classification becomes unreliable. Worse, the LFI may not know that critical information is missing, leading to confident but wrong classifications. The "Confused" failure mode (arxiv:2601.22208) is partly caused by important events dropped during compression.
**How to avoid:** Use Pattern 6's `havoc` to explicitly mark every piece of information that was not captured. Use compositional summaries (Pattern 6, P10) for principled compression that preserves layer-level information even when individual events are summarized. If compression is necessary, it must be layer-aware: implementation-layer events can be summarized for methodology audit (Stage 2 does not inspect individual implementation events) but must be preserved in full for implementation audit (Stage 1 needs them). Apply adaptive granularity: record at API-call level by default, increase to per-timestep in regions flagged as potentially faulty [RCA/FV §5, AP4 mitigation].

---

#### Anti-Pattern: RDF Triple Store as Primary

**Severity:** MEDIUM
**Source:** Provenance survey [Prov §6, Anti-Pattern 4]
**Description:** Storing the ATHENA IR in a standard RDF triple store (Jena, Virtuoso, Blazegraph) and querying with SPARQL. The schema-flexible store is used for data with a known, fixed schema.
**Why harmful for ATHENA:** Triple stores are optimized for schema-flexible, interoperable data. ATHENA's IR has a fixed schema (three-layer typing, derivation chains, agent hierarchies). Using a schema-flexible store forfeits performance. Standard triple stores are Java or C++ implementations that do not integrate with Rust (ADR 001). No mature Rust RDF triple store exists at production quality. Oxigraph (Rust) is closest but lacks key features for production use. SPARQL query expressiveness exceeds ATHENA's needs (arbitrary queries over flexible schemas) while underperforming for ATHENA's specific needs (fast path queries over fixed-schema graphs).
**How to avoid:** Adopt PROV-DM's *data model* (Entity-Activity-Agent, derivation chains, qualified relations) without its *serialization format* (RDF triples) or *query language* (SPARQL). Implement in Rust-native graph structures (petgraph or custom adjacency lists) with purpose-built query functions for LFI operations. This provides 10-100x performance improvement over triple stores for the specific queries the LFI needs while maintaining conceptual compatibility with PROV-DM for documentation and interoperability purposes [Prov §6, AP4 alternative; Prov §4, Performance table].

---

## Section 4: Tension Resolution -- MLIR Dialects vs. PROV-DM Hybrid

### The Two Recommendations

The RCA/FV survey and the Provenance survey each recommend a different foundational approach for ATHENA's IR. At first glance, these appear to be competing architectures. This section demonstrates they are complementary and proposes a unified structural foundation.

**The MLIR dialect approach (RCA/FV survey recommendation) [RCA/FV §6]:**
- Three namespaced dialects: `theory`, `methodology`, `implementation`
- Operations typed by dialect, enabling structural routing to the correct LFI audit stage
- Progressive lowering maps to the three-stage audit hierarchy (Level 0: implementation -> Level 1: methodology -> Level 2: theory)
- Multi-level coexistence: all three layers present in a single IR simultaneously
- Extensibility: per-DSL dialects within the shared three-layer framework
- Rust-compatible: dialect tags as enum variants (zero-cost abstraction)

**The PROV-DM hybrid approach (Provenance survey recommendation) [Prov §6]:**
- Entity-Activity-Agent model for causal ancestry tracking
- `wasDerivedFrom` chains for tracing causal paths through data transformations
- Qualified relations for detailed provenance (roles, usage patterns, generation context)
- Plans for encoding hypothesis predictions and expected behavior
- PROV-CONSTRAINTS for temporal consistency and derivation integrity
- Bundles for cross-experiment provenance grouping
- BUT: requires extensions for three-way typing, has no native fault semantics, designed for RDF ecosystem

### Why These Are Not Contradictory

The two approaches operate at different structural levels and solve different problems:

**The MLIR dialect approach answers: WHERE does an IR element belong?** It determines routing -- which audit stage processes which events. It provides the organizational skeleton of the IR: every event is tagged by layer, and the LFI uses this tag to determine its inspection order. The dialect system is primarily about *classification and routing*.

**The PROV-DM approach answers: HOW are IR elements causally related?** It determines causal reasoning within and across layers. Derivation chains trace how one entity led to another. Qualified relations record the roles entities play. Plans establish expected behavior. The provenance model is primarily about *causal structure and ancestry*.

These are orthogonal concerns. An event can simultaneously be: (a) classified as implementation-layer (dialect tag), and (b) linked by a `wasDerivedFrom` chain to a theory-layer entity (provenance structure). The dialect tag determines that Stage 1 inspects this event. The derivation chain enables the LFI to trace backward from this event to the theory-level specification that caused it, which is needed when Stage 1 passes and Stage 3 must determine whether the theory was contradicted.

### The Unified Structural Foundation

The proposed architecture uses MLIR dialect structure as the primary organization and PROV-DM-like causal graphs within each layer:

```
+-------------------------------------------------------------------+
|                         ATHENA Trace IR                           |
|                                                                   |
|  +-----------------------+  Lowering   +------------------------+ |
|  |   THEORY DIALECT      |  relations  |  METHODOLOGY DIALECT   | |
|  |                       | <-------->  |                        | |
|  |  Entities: hypotheses,|             |  Entities: experiment  | |
|  |  predictions, causal  |             |  designs, sampling     | |
|  |  claims               |             |  configs, controlled   | |
|  |                       |             |  variable sets         | |
|  |  Activities: predict, |             |                        | |
|  |  specify_observable,  |             |  Activities: configure,| |
|  |  define_ensemble      |             |  sample, intervene,    | |
|  |                       |             |  measure               | |
|  |  wasDerivedFrom chains|             |                        | |
|  |  (within theory)      |             |  wasDerivedFrom chains | |
|  |                       |             |  (within methodology)  | |
|  +-----------------------+             +------------------------+ |
|            |                                      |               |
|            | Lowering relations                   | Lowering      |
|            | (theory -> methodology)               | (meth -> impl)|
|            v                                      v               |
|  +--------------------------------------------------------------+ |
|  |              IMPLEMENTATION DIALECT                           | |
|  |                                                              | |
|  |  Entities: parameter snapshots, computed values, errors,     | |
|  |  resource states                                             | |
|  |                                                              | |
|  |  Activities: load_data, compute_forces, integrate,           | |
|  |  allocate_memory, checkpoint                                 | |
|  |                                                              | |
|  |  wasDerivedFrom chains (within implementation)               | |
|  |                                                              | |
|  +--------------------------------------------------------------+ |
|                                                                   |
|  Cross-layer links: explicit lowering relations connecting        |
|  theory/methodology entities to the implementation entities       |
|  that realize them                                                |
+-------------------------------------------------------------------+
```

**Structural rules:**

1. **Dialect determines routing.** Every IR node belongs to exactly one dialect. The LFI inspects implementation dialect first (Stage 1), methodology dialect second (Stage 2), theory dialect third (Stage 3). Dialect assignment is determined by the DSL's API structure and the per-DSL parameter classification table.

2. **Derivation chains provide intra-layer causality.** Within each dialect, PROV-DM-style `wasDerivedFrom` chains track how entities within that layer causally depend on each other. Within the implementation dialect: how one computed value derived from prior computations. Within the theory dialect: how one prediction derives from prior causal claims.

3. **Lowering relations provide cross-layer traceability.** Explicit lowering relations connect entities across dialects. A theory-level `set_temperature(300K)` entity has lowering relations to the methodology-level `configure_thermostat` entity and the implementation-level `velocity_rescaling` entities it generated. These lowering relations are NOT `wasDerivedFrom` (they are not data derivations but specification-realization relationships).

4. **Contracts attach to dialect boundaries.** At the theory-methodology boundary: "the experiment must measure the variables the hypothesis links causally." At the methodology-implementation boundary: "the implementation must execute the specified configuration without error." These contracts are first-class entities checked by the LFI at each stage transition.

5. **Agent typing follows dialect structure.** Each Agent is typed by dialect: TheoryAgent (user's scientific specification), MethodologyAgent (experiment design), ImplementationAgent (DSL framework, hardware). Agent delegation (`actedOnBehalfOf`) flows from theory through methodology to implementation.

### What Is Novel (Not From Either Survey)

The unified architecture requires four elements that exist in neither survey:

1. **Three-way layer typing vocabulary.** Neither MLIR nor PROV-DM provides a native three-way classification of operations/entities as theory/methodology/implementation. MLIR provides the dialect *mechanism* but not the *specific* dialect definitions. PROV-DM provides agent/entity types but not the *specific* type taxonomy. ATHENA must define the classification rules per DSL: which OpenMM API calls are theory-layer, which are methodology-layer, which are implementation-layer. This is a domain engineering task, not an IR design task, but it must be done for each target DSL.

2. **Fault classification ontology.** Neither survey's patterns produce a formal vocabulary for fault classification outputs. The LFI's output is a structured record: {fault_type: Theory|Methodology|Implementation, evidence_chain: [...], confidence: float, implicated_elements: [...], proposed_DAG_update: [...]}. This output type must be designed as part of the IR, not inherited from any existing pattern.

3. **Formalized prediction-observation comparison.** PROV-DM Plans are unstructured entities. Boogie `ensures` are propositional. Neither provides a formalization for quantitative scientific comparison: "the predicted binding energy was -8.5 kcal/mol +/- 0.5; the observed was -12.1 kcal/mol; the Bayes factor for divergence is 47." This comparison vocabulary -- with distributions, effect sizes, statistical tests, and tolerance specifications -- must be designed by ATHENA. It will draw on the contract pattern (comparison as postcondition checking) and the refutation chain pattern (the comparison as a verifiable inference step), but the specific formalization is novel.

4. **Methodology-layer detection rules.** The methodology dialect identifies *what kind* of element is being checked (a sampling configuration, a control variable set). But the *rules* for determining adequacy (is 10ns enough equilibration for this system? is the k-mesh dense enough for this metal?) are domain-specific and must be provided as pluggable rule sets, not encoded in the IR structure. The IR provides the *slots* (R8-R14); the rules provide the *assessments*. This separation between structure and rules is itself a design decision that neither survey addresses.

### Technology Recommendation

**Adopt PROV-DM concepts. Reject the RDF/SPARQL stack. Implement Rust-native per ADR 001.**

Justification:

1. **PROV-DM concepts transfer cleanly.** Entity (simulation state, parameter value, result) -> Rust struct. Activity (simulation step, computation) -> Rust struct with start/end timestamps. Agent (framework, user spec) -> Rust enum. `wasDerivedFrom` (causal lineage) -> directed edge in petgraph or custom adjacency list. Qualified relations (roles, usage details) -> Rust structs on edges. Plans (expected behavior) -> structured contract records. None of these concepts require RDF to implement.

2. **The RDF stack is a performance liability.** At megabyte-scale traces (10^4-10^6 graph nodes), custom Rust graph implementations handle path queries in <1ms. RDF triple stores take 10-1000ms for the same queries [Prov §4, Performance table]. For the LFI's sequential audit, which may require hundreds of queries per fault classification, this difference is significant. The 10-100x performance advantage of native Rust justifies forgoing SPARQL interoperability.

3. **No mature Rust RDF store exists.** Oxigraph is the closest but lacks features needed for production use. FFI to Java (Jena) or C++ (Virtuoso) stores introduces complexity, latency, and deployment burden [Prov §6, AP4].

4. **Fixed schema eliminates the need for schema flexibility.** RDF's strength is schema-flexible data. ATHENA's IR has a known, fixed schema (three dialects, typed entities, typed activities, typed agents, derivation chains, contracts, lowering relations). A fixed-schema Rust type system is both safer (compile-time enforcement) and faster (no runtime type resolution) than a schema-flexible triple store.

5. **Interoperability can be achieved at the export layer.** If ATHENA ever needs to produce PROV-compatible output (for provenance publication, data sharing, or compliance), a Rust-native IR can be serialized to PROV-JSON or PROV-N format on demand. This is a one-way export, not a bidirectional storage requirement.

**Risks of this recommendation:**

- **Conceptual drift.** Without the discipline of a W3C specification, ATHENA's "PROV-DM-compatible" IR may drift from PROV-DM semantics over time, reducing interoperability potential. Mitigation: maintain a mapping document from ATHENA IR types to PROV-DM concepts; verify compatibility periodically.
- **Reinventing validation.** PROV-CONSTRAINTS provides a validated set of consistency rules. Reimplementing these in Rust requires careful testing. Mitigation: use PROV-CONSTRAINTS as a specification for Rust validation functions; write property-based tests against the W3C specification.
- **Loss of query generality.** Purpose-built query functions handle known LFI queries efficiently but cannot handle arbitrary queries. If the LFI's query patterns change during research, new query functions must be written. Mitigation: design the query API around the R1-R29 requirements; add new queries as requirements evolve.

---

## Section 5: Decision Gate 2 Assessment

**Question:** "Are existing IR designs adaptable for ATHENA, or is a fully novel design needed?"

**Verdict:** Hybrid adaptation. Risk level: MEDIUM.

**What transfers from existing systems: ~65-70%**

The following elements can be adopted from existing IR patterns with adaptation:

- Dialect-based layer separation (MLIR) -- the primary organizational structure
- Entity-Activity-Agent causal model (PROV-DM) -- the causal reasoning substrate
- Specification-implementation contracts (Boogie/Why3) -- the experiment specification representation
- Typed event chains with evidence links (CoE) -- the fault evidence structure
- Ghost state for methodological metadata (Why3) -- the methodology audit support
- Havoc for unobserved state (Boogie) -- the observability gap representation
- Compositional trace summaries (Infer) -- the scalability mechanism
- SSA-style event identification (LLVM) -- the unique event ID scheme
- Temporal ordering constraints (PROV-CONSTRAINTS) -- the consistency validation
- Streaming construction compatibility (DRAT, Jaeger) -- the Rust implementation model
- Prospective/retrospective separation (ProvONE, CWL) -- the two-way spec-vs-exec backbone
- Conformance checking (process mining) -- the expected-vs-actual comparison mechanism

These provide the structural bones. Each requires DSL-specific adaptation (taxonomy definitions, parameter classifications, dialect boundaries) but the core patterns are established and validated in their source domains.

**What requires novel design: ~30-35%**

The following elements have no direct precedent in the surveyed systems:

1. **Three-way layer typing vocabulary.** The specific classification of DSL operations into theory/methodology/implementation does not exist in any surveyed system. All provide two-way separation at most (specification vs. execution in ProvONE; contract vs. body in Boogie). The methodology layer -- distinguishing "the experiment design was inadequate" from both "the code crashed" and "the theory is wrong" -- is ATHENA-specific. Justification: this is the core architectural distinction that separates ATHENA from generation-first AI co-scientists.

2. **Fault classification ontology.** The structured output of the LFI (fault type + evidence chain + confidence + DAG update directive) has no precedent. Existing systems produce root cause rankings (CoE), verification verdicts (Boogie/Why3: proved/counterexample), or provenance records (PROV-DM: what happened). None produce a three-way classification with actionable graph update directives. Justification: this ontology embodies ATHENA's core value proposition.

3. **Quantitative prediction-observation comparison formalization.** The specific mechanism for comparing quantitative scientific predictions against observations -- with distributions, effect sizes, statistical tests, and tolerance specifications -- is not provided by any surveyed IR. DRAT refutation is propositional. Boogie `ensures` are predicate-based. PROV-DM Plans are unstructured. Scientific falsification requires a richer comparison vocabulary. Justification: this is the Stage 3 evaluation mechanism; without it, the LFI cannot classify theoretical contradictions.

4. **Methodology-layer detection rules.** The rules for determining whether an experiment's methodology is adequate (sampling sufficiency, confounder control, variable coverage) are domain-specific and have no IR precedent. The IR provides the structural slots for methodology metadata; the rules provide the assessments. Justification: this is the Stage 2 evaluation mechanism.

5. **Cross-dialect lowering relation semantics.** The specific meaning of "this theory-level specification was realized by these implementation-level events" (and the diagnostic implications when the realization fails) is novel. MLIR lowering means "compile into"; ATHENA lowering means "was realized by." The realization relationship carries diagnostic payload: if implementation events diverge from the theory specification, the lowering relation enables tracing the divergence back to its origin. Justification: this is the mechanism that connects fault classification across the three stages.

**Risk assessment:**

- **Medium risk: Impedance mismatch during adaptation.** Patterns designed for different domains (compiler IR, provenance tracking, microservice RCA) may interact poorly when combined. The MLIR dialect system assumes operations are *compiled*; adapting it for observational trace events requires reinterpreting "lowering" as "realization." The PROV-DM model assumes *provenance recording*; adapting it for fault classification requires adding diagnostic semantics the model was not designed for. Risk: subtle semantic mismatches between the patterns' original meanings and ATHENA's usage may surface during implementation as edge cases where the IR does not cleanly represent a specific failure scenario.
- **Medium risk: Per-DSL adaptation cost.** The dialect boundary definition (which operations belong to which layer) must be done separately for each target DSL. Three frameworks (OpenMM, GROMACS, VASP) means three classification tables. The DSL trace surveys (FINDINGS.md items 1-6) show this is tractable but non-trivial: VASP has ~200-300 INCAR tags requiring classification; GROMACS has 10+ boundary parameters; OpenMM has a clean API boundary but hidden atom-type-assignment ambiguity.
- **Low risk: Rust implementation feasibility.** All patterns are Rust-compatible. The primary patterns (dialect tags, contracts, typed events, derivation chains) map to standard Rust constructs (enums, structs, adjacency lists). The performance requirements (megabyte-scale traces, sub-millisecond queries) are well within Rust's capabilities.

**Fallback:** If the hybrid adaptation produces an IR that is too complex to implement or too slow to meet performance requirements, the fallback is a minimal novel design that implements only the essential patterns:

1. Three-dialect event classification (Pattern 5, minimal)
2. Typed event chains with evidence links (Pattern 3, core)
3. Specification-implementation contracts (Pattern 6, essential)

This fallback sacrifices provenance richness (no derivation chains, no qualified relations, no Plans) and scalability mechanisms (no compositional summaries) in exchange for implementation simplicity. It would cover R1-R7 (Stage 1), R15-R17 (Stage 3 core), and R19-R21 (cross-cutting essentials) but would leave R8-R14 (Stage 2) weakly supported. This is acceptable for an initial prototype but not for the production system.

---

## Section 6: Three Candidate IR Foundation Preview

### Candidate 1: Layered Event Log (LEL)

**Catalog patterns drawn from:**
- Pattern 3 (Typed event chains) -- primary structure
- Pattern 5 (Dialect system) -- layer tags on events
- Pattern 4 (SSA data flow) -- unique event IDs with version tracking
- Pattern 1 (Counter-example traces) -- state-transition sequences with invariant checking

**Natural strengths:**
- Simplicity: a flat (or lightly hierarchical) log of typed events with layer tags. Closest to what DSL frameworks actually emit. Low impedance mismatch with raw trace data.
- Streaming: append-only event log is trivially streaming-compatible. Events are parsed and classified as they arrive.
- Rust performance: `Vec<TypedEvent>` with layer-tag enum is the most cache-friendly representation. O(1) append, O(n) sequential scan, indexed lookup by event type or layer.
- Stage 1 strength: implementation-layer events are directly extractable and checkable.

**Natural weaknesses:**
- Causal reasoning: without graph structure, causal queries ("which prior events influenced this outcome?") require sequential search with inference, not structural traversal. This is the flat-log vs. graph tradeoff identified in the Provenance survey [Prov §4, Comparison table].
- Stage 2 weakness: methodology audit requires cross-referencing events against experiment specification and DAG -- a log does not naturally support these joins.
- Stage 3 weakness: prediction-observation comparison requires structured comparison records, not just event sequences.

**Best-fit stage coverage:** Stage 1 (strong), Stage 2 (weak), Stage 3 (moderate with extensions).

---

### Candidate 2: Dual-Graph IR (DGR)

**Catalog patterns drawn from:**
- Pattern 5 (Dialect system) -- primary organization into three dialect-graphs
- Pattern 2 (Entity-Activity-Agent) -- PROV-DM-like causal graph structure within each dialect
- Pattern 6 (Contracts) -- specification entities with requires/ensures at dialect boundaries
- Pattern 7 (Conformance checking) -- expected-vs-actual comparison via graph alignment

**Natural strengths:**
- Causal reasoning: `wasDerivedFrom` chains enable structural causal queries. "Which theory parameters are in the causal ancestry of this failed prediction?" is a graph traversal, not a search.
- Stage 2 and 3 strength: graph structure naturally represents relationships between entities (predictions linked to observations, interventions linked to observables, confounders linked to causal paths).
- Specification-execution separation: the "dual" in DGR is the prospective graph (specification) and retrospective graph (execution), bridged by lowering/realization relations.
- Cross-experiment analysis: graph structure supports cross-cycle comparisons and pattern detection.

**Natural weaknesses:**
- Complexity: graph construction from raw trace data requires substantial parsing and entity resolution. Higher impedance mismatch with raw traces than LEL.
- Performance: graph queries (transitive closure, path finding) are more expensive than sequential log scans for simple lookups. Cache-unfriendly for large graphs.
- Streaming: graph construction from streaming data requires forward-reference management. Iterative DSL workflows (convergence loops) create back-references that require buffering.
- Over-engineering risk: the full Entity-Activity-Agent model with qualified relations may be more structure than the LFI needs for initial fault classification.

**Best-fit stage coverage:** Stage 1 (moderate), Stage 2 (strong), Stage 3 (strong).

---

### Candidate 3: Typed Assertion Log (TAL)

**Catalog patterns drawn from:**
- Pattern 6 (Contracts) -- primary structure: the IR is a sequence of assertions (claims to be checked)
- Pattern 5 (Dialect system) -- assertions typed by layer
- Pattern 1 (Counter-example traces) -- failed assertions produce structured refutation evidence
- Pattern 3 (Typed event chains) -- evidence chains back assertions

**Natural strengths:**
- Fault classification native: the IR is designed around the LFI's central task (checking assertions). Each assertion is a checkable claim: "the input data matches specification" (Stage 1), "the sampling was sufficient" (Stage 2), "the prediction matches observation" (Stage 3). The LFI processes assertions in order; the first failed assertion determines the fault classification.
- Audit trail: every assertion carries the evidence that supports or refutes it. The LFI's reasoning is fully auditable.
- Sequential audit mapping: assertion ordering maps directly to the three-stage audit. Implementation assertions first, methodology assertions second, theory assertions third.
- Ghost state natural: methodology assertions are naturally populated from experiment specification and DAG, not from trace data.

**Natural weaknesses:**
- Assertion derivation complexity: converting raw trace data into structured assertions requires substantial domain-specific logic. Where do the assertions come from? Some (R1: execution completed) are straightforward; others (R12: sampling sufficient) require domain rules.
- Causal reasoning: an assertion log does not natively support causal graph traversal. If the LFI needs to trace "which theory parameter caused this implementation failure," the assertion log does not provide graph structure for this query.
- Rigid ordering: the sequential assertion model assumes a fixed audit order. If the LFI needs to reason across stages (a theory-level issue that manifests as implementation symptoms), the rigid ordering may require backtracking.
- Novelty risk: this design has no close precedent in the surveyed systems. It is the most ATHENA-specific candidate and therefore the highest-risk if the design assumptions prove incorrect.

**Best-fit stage coverage:** Stage 1 (strong), Stage 2 (moderate), Stage 3 (strong for comparison, weak for causal reasoning).

---

## Section 7: Implications for Downstream Steps

### For Step 5a (Candidate IR Schemas)

Each candidate should build on the following structural foundation established by this synthesis:

1. **All candidates must implement the dialect/layer tag (Pattern 5) as the primary organizational mechanism.** This is non-negotiable: it satisfies R19 (the load-bearing requirement) and enables the LFI's three-stage routing. The question for Step 5a is not whether to use dialect tags but how they interact with each candidate's primary data structure (event log, graph, or assertion sequence).

2. **All candidates should incorporate specification-implementation contracts (Pattern 6).** The experiment specification must be a first-class entity, not an external annotation. Step 5a should define the contract structure for each candidate: what does `requires`/`ensures`/`modifies` look like in LEL vs. DGR vs. TAL?

3. **Candidates differ primarily in their causal reasoning substrate.** LEL uses sequential search with optional index structures. DGR uses graph traversal. TAL uses assertion-chain backtracking. Step 5a should evaluate each substrate against the LFI's actual query patterns (derived from R1-R29) to determine which performs best for the specific queries needed.

4. **Step 5a should evaluate each candidate against the anti-pattern registry.** Specifically: does the candidate avoid specification-implementation conflation (CRITICAL)? Does it support streaming construction (avoid post-mortem-only)? Does it maintain reasonable granularity (avoid full-granularity recording)?

5. **The unified architecture (Section 4) suggests DGR is the natural synthesis of the two surveys' recommendations,** with LEL as the simpler fallback and TAL as the most ATHENA-specific alternative. Step 5a should test this ordering: is DGR's additional complexity (over LEL) justified by its causal reasoning capability? Is TAL's novelty risk acceptable given its native fault classification alignment?

### For the Adapter Architecture

The pattern catalog informs adapter design in several ways:

1. **Per-DSL dialect boundary definitions** must be encoded in each adapter. The OpenMM adapter classifies ForceField/System operations as theory-layer and Platform/Context operations as implementation-layer. The GROMACS adapter classifies .mdp theory parameters and mdrun flags as implementation-layer. The VASP adapter uses the external INCAR classification table. These classifications are adapter-specific but feed into the common three-dialect structure.

2. **Multi-source correlation** is an adapter responsibility. GROMACS requires .log + .edr + .tpr fusion; VASP requires vasprun.xml + OUTCAR + stdout fusion; OpenMM requires reporter + exception + API state fusion. The adapter produces a unified stream of typed, layer-tagged events regardless of how many source files contribute.

3. **Contract extraction** is partially adapter-specific. OpenMM's System object provides queryable preconditions. GROMACS's .tpr contains the compiled run specification. VASP's input files (INCAR + POSCAR + POTCAR + KPOINTS) collectively define the specification. Each adapter extracts specification elements in its own format and maps them to the common contract structure.

4. **Adaptive granularity** may be adapter-controlled. If the trace shows anomalies (energy spikes, convergence issues), the adapter could request higher-resolution logging for subsequent phases. This requires the adapter to have bidirectional communication with the DSL framework, which is possible for OpenMM (custom reporters) but not for VASP (closed-source, batch execution).

### Open Questions for Step 5a

1. **Which causal reasoning substrate (log search, graph traversal, or assertion chains) best supports the LFI's actual query patterns?** Step 5a should enumerate the specific queries derived from R1-R29 and benchmark each candidate's query performance.

2. **How should boundary parameters (GROMACS `dt`, VASP `PREC`) be represented?** They span dialects. Options: assign to primary dialect with cross-reference annotation; create a "boundary" sub-dialect; duplicate with explicit derivation link. Each candidate handles this differently.

3. **What is the streaming/buffering trade-off for Stage 3?** LEL is fully streaming. DGR may require partial graph buffering. TAL may require assertion reordering. The trade-off depends on how often Stage 3 needs full-trace access vs. phase-level summaries.

4. **How should the cross-experiment dimension (R29) interact with the per-experiment IR?** Is each experiment's IR a standalone artifact aggregated post-hoc, or does a shared structure span experiments? This affects storage, lifecycle, and query interface design.

5. **Can the unified architecture (Section 4) be incrementally implemented?** Start with LEL for a Stage 1 prototype (implementation audit on OpenMM traces), add graph structure for Stage 2/3, evolve toward DGR as requirements solidify. Or must the full architecture be designed up-front?
