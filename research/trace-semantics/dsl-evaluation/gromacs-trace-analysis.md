# GROMACS Trace Output System: Structured Analysis

## Purpose

This document catalogs and classifies the complete trace output of GROMACS molecular dynamics simulations, mapping each output element to the theory layer or implementation layer as defined in ATHENA's Trace Semantics Engine requirements (ARCHITECTURE.md 4.5, VISION.md 4.1). The analysis supports the design of an intermediate representation (IR) that enables the Lakatosian Fault Isolator's three-stage audit: implementation, methodology, theory.

## Terminology

Throughout this document, "theory layer" refers to parameters and outputs that encode the user's physical model: force field choice, integrator equations, coupling algorithms, thermodynamic ensembles, and observable quantities. "Implementation layer" refers to parameters and outputs that encode execution mechanics: output frequency, parallelization, memory management, file I/O, and numerical precision settings. Parameters that serve both roles are classified as "boundary" and discussed explicitly.

---

## 1. Output File Inventory

GROMACS produces a well-defined set of output files during simulation execution. The primary outputs are generated by `mdrun`, the simulation engine. Additional files are produced by preprocessing (`grompp`) and analysis tools.

### 1.1 Run Input File (.tpr)

| Property | Detail |
|:---|:---|
| **Producer** | `grompp` (preprocessor) |
| **Format** | Binary (XDR-encoded) |
| **Content** | Complete, self-contained simulation specification: topology (atom types, bonds, angles, dihedrals, force field parameters), initial coordinates, initial velocities, all .mdp run parameters (both theory and implementation), and box vectors. The .tpr bundles the entire state needed to reproduce a simulation from scratch. |
| **Programmatic access** | `gmx dump -s topol.tpr` produces a human-readable text dump of the full contents. The GROMACS C++ API (`TprReader`) provides programmatic access. Python libraries: `MDAnalysis` can read topology and coordinates from .tpr; `gmxapi` provides Python bindings for .tpr inspection. |
| **Layer classification** | **Both layers, opaquely merged.** The .tpr is the compiled product of grompp and bundles theory-layer parameters (force field, integrator, ensemble) and implementation-layer parameters (output frequencies, parallelization hints) into a single binary object. This is the central challenge for ATHENA's trace semantics: the .tpr is the boundary object where theory and implementation become entangled. Recovery of the original layer separation requires either: (a) retaining the source .mdp and .top files, or (b) parsing the .tpr dump to re-classify each parameter. |
| **Relevance to IR** | Critical. The .tpr defines the "experiment specification" that the LFI must audit. The IR must be able to reconstruct which parameters came from the theory layer and which from the implementation layer, even though the .tpr merges them. |

### 1.2 Log File (.log)

| Property | Detail |
|:---|:---|
| **Producer** | `mdrun` |
| **Format** | Plain text, semi-structured |
| **Content** | Header section: GROMACS version, build info, hardware detection, MPI/thread configuration. Run parameter echo: complete reproduction of all .mdp parameters used (identical to what is in the .tpr). Neighbor search statistics. Domain decomposition layout. Performance counters (wall time per step, ns/day, load imbalance). Energy averages and drift over the run. Warning and error messages. LINCS/SETTLE constraint diagnostics when failures occur. Final performance summary. |
| **Programmatic access** | Text parsing. No official structured parser exists. The log format has remained broadly stable across GROMACS versions but is not formally specified as a machine-readable schema. Fields are separated by whitespace with section headers. Some sections use fixed-width columnar formatting. |
| **Layer classification** | **Mixed, but predominantly implementation.** The parameter echo section contains theory-layer information (force field, integrator, coupling), but this is a reproduction of the .tpr contents, not new information. The performance data, hardware configuration, domain decomposition, and timing are purely implementation-layer. Error and warning messages span both layers (see Section 5). |
| **Relevance to IR** | High. The log file is the primary human-readable diagnostic artifact. For ATHENA, it is the richest source of error and warning messages, but its semi-structured format is a parsing challenge. The IR must extract structured events from free-text log entries. |

### 1.3 Energy File (.edr)

| Property | Detail |
|:---|:---|
| **Producer** | `mdrun`, written every `nstenergy` steps |
| **Format** | Binary (XDR-encoded), structured time series |
| **Content** | Time series of all computed energy terms at the configured output frequency: kinetic energy, potential energy, total energy, temperature, pressure, volume, density, individual Lennard-Jones and Coulomb components, bond/angle/dihedral energies, constraint energy (LINCS/SETTLE/SHAKE contributions), virial components, pressure tensor, enthalpy, dispersion correction energies, PME mesh/real-space decomposition, and any user-defined energy groups. Also stores lambda values for free energy calculations. |
| **Programmatic access** | `gmx energy` (interactive selection of terms, outputs to .xvg). `panedr` Python library (reads .edr directly into pandas DataFrames, no GROMACS installation required). `gmxapi` Python bindings. The .edr format uses XDR encoding; the `xdrlib` Python module can read the raw binary, but the GROMACS-specific framing requires knowledge of the enxnm (energy name) block structure. `panedr` is the most practical programmatic access path. |
| **Layer classification** | **Theory layer (observables), with implementation-layer metadata.** The energy values themselves are theory-layer observables: they are direct outputs of the physical model (force field + integrator + ensemble). The output frequency (`nstenergy`) and precision are implementation-layer. The presence or absence of specific energy terms depends on theory-layer choices (e.g., dispersion correction energy only appears if `DispCorr` is enabled). |
| **Relevance to IR** | Critical. The .edr is the primary quantitative record of simulation behavior. Anomalies in energy time series (drift, NaN, sudden jumps, constraint violations) are the most common signals of both theory-layer failures (unstable force field) and implementation-layer failures (numerical overflow). The IR must represent energy trajectories as structured time series with anomaly detection. |

### 1.4 Full-Precision Trajectory (.trr)

| Property | Detail |
|:---|:---|
| **Producer** | `mdrun`, written every `nstxout` (coordinates), `nstvout` (velocities), `nstfout` (forces) steps |
| **Format** | Binary (XDR-encoded) |
| **Content** | Full-precision (double or single, matching compilation) coordinates, velocities, and/or forces for all atoms at configured intervals. Box vectors at each frame. Step number and simulation time. |
| **Programmatic access** | `gmx trjconv` for format conversion and frame selection. `MDAnalysis`, `MDTraj`, `pytraj` Python libraries for direct reading. `gmxapi` for programmatic access. |
| **Layer classification** | **Theory layer (state trajectory).** The coordinates, velocities, and forces are the physical state of the system as computed by the theory (force field + integrator). The output frequency and precision are implementation-layer. |
| **Relevance to IR** | Medium for fault isolation. Trajectory data is primarily used for scientific analysis (RMSD, radial distribution, etc.), not for diagnosing failures. However, examination of atomic positions near failure points (e.g., atoms flying apart before a crash) can provide causal evidence for theory-layer issues (bad force field parameters, missing interactions). |

### 1.5 Compressed Trajectory (.xtc)

| Property | Detail |
|:---|:---|
| **Producer** | `mdrun`, written every `nstxout-compressed` steps |
| **Format** | Binary, lossy-compressed (XDR with inter-frame compression) |
| **Content** | Reduced-precision coordinates only (no velocities or forces). Default precision: 1000 (0.001 nm). Box vectors. Step number and time. |
| **Programmatic access** | Same as .trr: `gmx trjconv`, `MDAnalysis`, `MDTraj`, `pytraj`. |
| **Layer classification** | **Theory layer (state trajectory), implementation-layer compression.** The coordinates are theory-layer; the lossy compression and reduced precision are implementation-layer choices that may affect downstream analysis accuracy. |
| **Relevance to IR** | Low for fault isolation. The .xtc is a storage-optimized version of the .trr. Its lossy compression introduces a subtle boundary issue: if downstream analysis produces unexpected results due to compression artifacts, this is an implementation-layer failure masquerading as a theory-layer result. The IR should flag when analysis is performed on .xtc rather than .trr data. |

### 1.6 Checkpoint File (.cpt)

| Property | Detail |
|:---|:---|
| **Producer** | `mdrun`, written every `nstcheckpoint` steps (default: every 15 minutes of wall time) |
| **Format** | Binary (portable, architecture-independent) |
| **Content** | Complete simulation state for exact restart: all coordinates, velocities, forces, box vectors, RNG state, integrator state (for algorithms like Nose-Hoover or Parrinello-Rahman that carry state), constraint solver state, free energy lambda state, replica exchange state. Also stores the step number and simulation time for continuation. |
| **Programmatic access** | `gmx check -f state.cpt` for inspection. `gmx dump -cp state.cpt` for detailed contents. Limited programmatic access outside GROMACS tools. |
| **Layer classification** | **Both layers.** The physical state (coordinates, velocities, thermostat/barostat extended variables) is theory-layer. The RNG state, integrator internal state, and checkpointing metadata are implementation-layer. The checkpoint is the only file that preserves the complete internal state of the integrator, which is crucial for exact bitwise continuation. |
| **Relevance to IR** | High for failure analysis. When a simulation crashes, the last checkpoint before the crash captures the system state that led to the failure. Comparing the checkpoint state to the crash state can reveal whether the failure developed gradually (theory/methodology issue) or suddenly (implementation issue). The presence of a valid .cpt also determines whether recovery and re-execution are possible. |

### 1.7 Structure File (.gro)

| Property | Detail |
|:---|:---|
| **Producer** | `mdrun` writes a final structure; also produced by various GROMACS tools |
| **Format** | Plain text, fixed-width columns |
| **Content** | Atom coordinates (and optionally velocities) for a single frame. Residue numbers, residue names, atom names, atom indices. Box vectors on the last line. Limited precision (3 decimal places by default in .gro format, though the actual simulation precision is higher). |
| **Programmatic access** | Direct text parsing (fixed-width format). `MDAnalysis`, `MDTraj`, `BioPython` for reading. |
| **Layer classification** | **Theory layer (structural state).** The .gro is a snapshot of the physical system. The limited precision of the text format is an implementation artifact. |
| **Relevance to IR** | Low for fault isolation. The .gro is primarily an interchange format. The final .gro output by mdrun records the end state, but trajectory files (.trr/.xtc) provide the same information with more context. |

### 1.8 Analysis Output (.xvg)

| Property | Detail |
|:---|:---|
| **Producer** | GROMACS analysis tools (`gmx energy`, `gmx rms`, `gmx rdf`, `gmx msd`, etc.) |
| **Format** | Plain text, XMGrace-compatible. Header lines begin with `@` (metadata, axis labels, legend). Data lines are whitespace-separated columns. Comment lines begin with `#`. |
| **Content** | Varies by analysis tool. Typically: time series of a computed quantity (RMSD, RDF, MSD, energy terms, hydrogen bonds, etc.). |
| **Programmatic access** | Direct text parsing (skip `@` and `#` lines, read whitespace-separated columns). `numpy.loadtxt` with comment character handling. |
| **Layer classification** | **Theory layer (derived observables).** The .xvg contains quantities derived from the physical trajectory. The choice of analysis and its parameters are methodology-layer decisions. |
| **Relevance to IR** | Medium. Post-simulation analysis results are methodology-layer artifacts. The IR should track which analyses were performed and their parameters, as methodological errors often manifest in analysis choices rather than simulation execution. |

### 1.9 Summary Table

| File | Format | Layer | Primary Content | Parsing Difficulty | Fault Isolation Value |
|:---|:---|:---|:---|:---|:---|
| .tpr | Binary (XDR) | Both (merged) | Complete run specification | Medium (gmx dump) | Critical: experiment spec |
| .log | Text (semi-structured) | Mixed (mostly impl.) | Diagnostics, performance, errors | High (free-text parsing) | Critical: error messages |
| .edr | Binary (XDR) | Theory (observables) | Energy time series | Low (panedr) | Critical: anomaly detection |
| .trr | Binary (XDR) | Theory (state) | Full-precision trajectory | Low (MDAnalysis) | Medium: crash analysis |
| .xtc | Binary (compressed) | Theory (state) | Compressed trajectory | Low (MDAnalysis) | Low |
| .cpt | Binary | Both | Checkpoint state | Medium (gmx dump) | High: pre-crash state |
| .gro | Text (fixed-width) | Theory (state) | Structure snapshot | Low (text parsing) | Low |
| .xvg | Text (columnar) | Theory (derived) | Analysis results | Low (numpy) | Medium: methodology audit |

---

## 2. Theory-Implementation API Boundary

### 2.1 The .mdp File as Specification Interface

The `.mdp` (molecular dynamics parameter) file is the primary user-facing configuration interface in GROMACS. It is a plain-text key-value file where the user specifies all simulation parameters. This is the closest analog to a "theory specification API" in the GROMACS ecosystem.

The .mdp parameters can be classified into three categories:

#### 2.1.1 Theory-Layer Parameters

These specify the physical model and are the user's scientific claims about the system:

| Parameter | Domain | What It Specifies |
|:---|:---|:---|
| `integrator` | Equations of motion | Algorithm for time evolution (md, md-vv, sd, steep, cg, nm, etc.). Determines the statistical ensemble. |
| `tcoupl` | Thermostat | Temperature coupling algorithm (no, berendsen, v-rescale, nose-hoover, andersen). Determines the thermodynamic ensemble (NVE vs NVT). |
| `pcoupl` | Barostat | Pressure coupling algorithm (no, berendsen, parrinello-rahman, mttk, c-rescale). Determines NPT vs NVT ensemble. |
| `coulombtype` | Electrostatics | Method for computing electrostatic interactions (Cut-off, Reaction-Field, PME, P3M-AD, Ewald). Determines physical accuracy of long-range forces. |
| `vdwtype` | Van der Waals | Method for computing van der Waals interactions (Cut-off, PME). |
| `rcoulomb` | Electrostatics | Coulomb cutoff distance. Affects physical accuracy. |
| `rvdw` | Van der Waals | Van der Waals cutoff distance. |
| `DispCorr` | Long-range corrections | Dispersion correction for energy and/or pressure (no, EnerPres, Ener). |
| `constraints` | Bond constraints | Which bonds to constrain (none, h-bonds, all-bonds, h-angles). Determines whether hydrogen mass repartitioning is used. |
| `constraint-algorithm` | Constraints | Algorithm for enforcing constraints (LINCS, SHAKE). |
| `ref-t` | Temperature | Reference temperature(s) for thermostat coupling groups. |
| `ref-p` | Pressure | Reference pressure for barostat. |
| `tau-t` | Temperature | Thermostat coupling time constant. |
| `tau-p` | Pressure | Barostat coupling time constant. |
| `compressibility` | Pressure | Isothermal compressibility for pressure coupling. |
| `free-energy` | Alchemical | Free energy calculation settings (lambda schedules, soft-core parameters). |
| `pull` | Enhanced sampling | Pull code settings for umbrella sampling, steered MD. |
| `awh` | Enhanced sampling | Accelerated weight histogram settings. |
| `tc-grps` | Temperature | Groups for independent temperature coupling. |
| `energygrps` | Analysis | Groups for energy decomposition. |

#### 2.1.2 Implementation-Layer Parameters

These specify execution mechanics and do not affect the physical model:

| Parameter | Domain | What It Specifies |
|:---|:---|:---|
| `nstlog` | I/O | Frequency of log file output (steps between writes). |
| `nstenergy` | I/O | Frequency of energy file (.edr) output. |
| `nstxout` | I/O | Frequency of full-precision coordinate output (.trr). |
| `nstvout` | I/O | Frequency of velocity output (.trr). |
| `nstfout` | I/O | Frequency of force output (.trr). |
| `nstxout-compressed` | I/O | Frequency of compressed coordinate output (.xtc). |
| `compressed-x-precision` | I/O | Precision of compressed trajectory (default 1000 = 0.001 nm). |
| `nstlist` | Performance | Frequency of neighbor list update. (Boundary: see 2.1.3.) |
| `ns-type` | Performance | Neighbor search type (grid, simple). |
| `pbc` | Boundary conditions | Periodic boundary conditions (xyz, no, xy). (Boundary: see 2.1.3.) |
| `comm-mode` | Numerical | Center of mass motion removal (Linear, Angular, None). |
| `comm-grps` | Numerical | Groups for COM motion removal. |
| `nstcomm` | Numerical | Frequency of COM motion removal. |

Command-line mdrun parameters (not in .mdp) that are purely implementation-layer:

| Parameter | What It Specifies |
|:---|:---|
| `-ntomp` | Number of OpenMP threads |
| `-ntmpi` | Number of MPI ranks |
| `-gpu_id` | GPU device selection |
| `-pin` | Thread pinning |
| `-pme` | PME rank assignment |
| `-dd` | Domain decomposition grid |
| `-npme` | Number of dedicated PME ranks |
| `-nb` | Non-bonded calculation location (auto, gpu, cpu) |
| `-bonded` | Bonded calculation location (auto, gpu, cpu) |
| `-update` | Update calculation location (auto, gpu, cpu) |
| `-resetstep` | Step to reset performance counters |
| `-maxh` | Maximum wall time |

#### 2.1.3 Boundary Parameters (Theory-Implementation Blur)

These parameters serve dual roles, affecting both physics and execution. They are the primary challenge for clean layer separation:

| Parameter | Theory Role | Implementation Role | Classification |
|:---|:---|:---|:---|
| `dt` | Timestep determines integration accuracy. Too large a dt for the force field causes integration instability (energy drift, constraint failures). Physically meaningful: must be small enough to resolve the fastest motions. | Directly determines computational cost (total steps = total time / dt). Larger dt = faster simulation. | **Boundary, theory-dominant.** The choice of dt is a physical/methodological decision (what timescale resolution is needed), but errors in dt manifest as implementation-like symptoms (numerical instability). |
| `nsteps` | Total simulation time (nsteps * dt) determines whether enough sampling occurs to observe the phenomenon of interest. Insufficient sampling is a methodological failure. | Determines total compute cost. | **Boundary, methodology-dominant.** The value is a scientific judgment about required sampling, but it has no effect on the physics per step. |
| `rlist` | Buffer radius for neighbor list. If too small relative to `rcoulomb`/`rvdw`, particles can move past the buffer between updates, producing incorrect forces. GROMACS auto-sets this with Verlet cutoff scheme. | Larger rlist = more pairs to compute = slower. | **Boundary, implementation-dominant** (when Verlet scheme auto-manages it). Theory-relevant only when manually set with group scheme (deprecated). |
| `nstlist` | Determines how often the neighbor list is rebuilt. Too infrequent updates with fast-moving particles can miss interactions. GROMACS auto-tunes this with Verlet buffer. | More frequent updates = higher overhead. | **Boundary, implementation-dominant** (when auto-tuned). |
| `pbc` | Periodic boundary conditions are a physical model choice (infinite periodic crystal vs. isolated system). | PBC also affects domain decomposition and parallelization strategy. | **Boundary, theory-dominant.** The choice of PBC is a physical model decision; the implementation consequences are secondary. |
| `fourierspacing` | Controls PME grid spacing, affecting electrostatic accuracy. | Finer grid = more accurate but slower. | **Boundary.** Affects both physical accuracy and computational cost. |
| `lincs-order` | Higher order = more accurate constraint enforcement but slower. Default (4) is adequate for most simulations. Higher values needed for very stiff systems. | Directly affects constraint solver iterations per step. | **Boundary, methodology-dominant.** Usually an accuracy vs. speed tradeoff. |
| `lincs-iter` | Number of LINCS iterations. Affects constraint satisfaction accuracy. | More iterations = slower but more accurate constraints. | **Boundary, methodology-dominant.** |
| `verlet-buffer-tolerance` | Controls the energy drift tolerance, which GROMACS uses to auto-set rlist. Tighter tolerance = more accurate pair interactions. | Tighter tolerance = larger rlist = more computation. | **Boundary.** Encodes an accuracy-performance tradeoff. |
| `cutoff-scheme` | Verlet (modern, GPU-compatible) vs. group (deprecated). Verlet provides automatic buffer management. | Verlet enables GPU offloading and auto-tuning. Group is CPU-only. | **Boundary, implementation-dominant.** The cutoff scheme affects performance path but both should produce equivalent physics when properly configured. |

### 2.2 grompp: The Compilation Boundary

`grompp` (GROMACS preprocessor) is the architectural boundary between the user's specification and the simulation engine. It compiles three inputs into the binary .tpr:

1. **.mdp** (run parameters) -- theory + implementation parameters
2. **.top** (topology) -- force field parameters, molecular connectivity, atom types
3. **.gro** (or .pdb) (structure) -- initial coordinates and box dimensions

**What grompp does:**
- Reads and validates all .mdp parameters
- Parses the topology file, expanding `#include` directives to pull in force field definition files (.itp)
- Assigns force field parameters to the molecular system based on atom types
- Validates coordinate-topology consistency (atom counts must match)
- Generates missing velocities from Maxwell-Boltzmann distribution at reference temperature
- Computes charge groups and neighbor search structures
- Validates box size against cutoff radii
- Writes the combined binary .tpr

**grompp's validation is documented in detail in Section 7.**

### 2.3 Assessment: Is the Boundary Clean Enough?

**Strengths for ATHENA's purposes:**
1. The .mdp file provides a text-based, declarative specification that cleanly separates user intent from execution. Most theory-layer parameters (integrator, tcoupl, pcoupl, coulombtype, force field) have no implementation side effects.
2. The force field is entirely theory-layer: it is a set of mathematical functions and parameters (.itp files) that define the potential energy surface. GROMACS does not allow the user to modify force field evaluation at the implementation level through the standard API.
3. mdrun command-line parameters are purely implementation-layer (threading, GPU assignment, domain decomposition). They have no theory-layer content whatsoever.
4. grompp's compilation step creates a natural audit point: the .tpr records exactly what was compiled, and `gmx dump` can reconstruct the full parameter set.

**Weaknesses and ambiguities:**
1. The .tpr merges both layers into a single opaque binary. Recovering the layer separation requires either the original .mdp (which may not be preserved) or parsing the `gmx dump` output and re-classifying each parameter. This is tractable but requires a GROMACS-specific parameter classification table (like the one above).
2. Boundary parameters (`dt`, `fourierspacing`, `nstlist`, `lincs-order`) blur the line. An incorrect `dt` causes symptoms that look like implementation failures (LINCS warnings, energy explosions) but the root cause is a theory/methodology decision. The IR must represent these parameters with their dual classification.
3. The Verlet buffer auto-tuning means that some parameters the user specified are silently adjusted by GROMACS at runtime. The .log file records these adjustments, but the user's original intent and the actual runtime value may differ. The IR must track both.
4. GROMACS allows runtime auto-tuning of PME parameters and domain decomposition. These adjustments are logged but change execution characteristics without user input. They are purely implementation-layer but could mask or cause performance-related failures.

**Overall assessment:** The GROMACS theory-implementation boundary is **sufficiently clean for deterministic auditing**, provided the IR maintains a classification table for boundary parameters and tracks the difference between user-specified and runtime-adjusted values. The boundary is cleaner than most scientific software but not perfectly binary. The main challenge is the .tpr opacity, which is resolvable through `gmx dump` parsing.

---

## 3. Log File Structure

### 3.1 Format Classification

The GROMACS .log file is **semi-structured free text**. It is not machine-readable in any formal sense: there is no schema, no XML/JSON structure, no formal grammar. However, it follows a consistent, version-stable layout with recognizable section headers, fixed-width tables, and predictable formatting patterns.

### 3.2 Section Layout

A typical GROMACS .log file contains the following sections in order:

1. **Header block** (lines 1-~20): GROMACS version string, build date, build configuration (single/double precision, GPU support, MPI/thread-MPI, SIMD instruction set), executable path, working directory, command line used.

2. **Hardware detection block**: Detected CPUs (model, cache sizes, SIMD capabilities), detected GPUs (CUDA/OpenCL device names, compute capability, memory), number of nodes and ranks, hardware topology.

3. **Input parameter echo** (labeled "Input Parameters:"): A complete reproduction of all .mdp parameters in the .tpr, grouped by category. This section uses a consistent `key = value` format, one parameter per line. This is the most machine-parseable section of the log file. Example:
   ```
   integrator                     = md
   tinit                          = 0
   dt                             = 0.002
   nsteps                         = 500000
   ...
   coulombtype                    = PME
   rcoulomb                       = 1.0
   vdwtype                        = Cut-off
   rvdw                           = 1.0
   ```

4. **Neighbor search information**: Cutoff scheme, Verlet buffer calculation (showing the computed rlist from the tolerance), buffer verification statistics. If the Verlet buffer is auto-computed, this section shows the calculation details and the resulting pair-list cutoff.

5. **Domain decomposition layout** (if parallel): Grid dimensions, cell sizes, communication patterns, PME rank assignment.

6. **Constraint information**: Number of constraints, LINCS order and iterations, settled water molecules.

7. **Run progress** (during long runs): Step number, time, performance metrics written at intervals controlled by `nstlog`.

8. **Energy statistics**: At the end of the run, averages, RMSD, and drift for all energy terms computed during the simulation, presented in a fixed-width table.

9. **Performance summary**: Wall time, core hours, ns/day, hours/ns. Per-step timing breakdown (force calculation, PME, constraint solving, communication, update, neighbor search). Load imbalance metrics for parallel runs.

10. **Warnings and notes**: Interspersed throughout the log. WARNING lines begin with "WARNING" or "NOTE". Fatal errors begin with "Fatal error:" or "Program gmx mdrun" followed by the error message.

### 3.3 Verbosity

GROMACS log verbosity is controlled by:
- `nstlog`: Controls how frequently step-level performance data is written. Default: 1000. Setting to 0 suppresses step-level output. Setting to 1 produces extremely verbose output (every step).
- `-verbose` flag on mdrun: Adds per-step energy output to the log.
- Build-time debug flags: Debug builds produce additional internal diagnostic output.

The parameter echo (section 3) and final statistics (sections 8-9) are always present regardless of verbosity. Error and warning messages are always emitted regardless of nstlog.

### 3.4 Machine-Parseability Assessment

| Section | Parseable? | Method | Difficulty |
|:---|:---|:---|:---|
| Header | Partially | Regex for version, precision, SIMD | Low |
| Hardware | Partially | Regex for GPU/CPU model | Medium |
| Input parameters | Yes | `key = value` parsing | Low |
| Neighbor search | Partially | Regex for rlist, buffer tolerance | Medium |
| Domain decomposition | Partially | Regex for grid dimensions | Medium |
| Run progress | Yes | Fixed-width columns (step, time, lambda) | Low |
| Energy statistics | Yes | Fixed-width table | Medium (column alignment) |
| Performance summary | Yes | Labeled key-value pairs | Low |
| Errors/warnings | Partially | Regex for "WARNING", "Fatal error:", "NOTE" | Medium (free-text messages) |

**Key finding:** The input parameter echo section is reliably machine-parseable and provides the complete theory-layer specification. The error/warning messages are the hardest to parse because they are free-text descriptions with no error codes, no structured severity levels, and no machine-readable classification.

---

## 4. Energy File (.edr) Access

### 4.1 Data Contents

The .edr file stores a time series of energy-like quantities computed during the simulation. The exact set of terms depends on the simulation setup, but a typical production run includes:

**Core thermodynamic quantities:**
- Kinetic Energy, Potential Energy, Total Energy
- Temperature, Pressure, Volume, Density
- Enthalpy (for NPT)

**Force field energy decomposition:**
- Bond, Angle, Proper Dihedral, Improper Dihedral, Ryckaert-Bellemans
- Lennard-Jones (14), Coulomb (14) -- 1-4 interactions
- Lennard-Jones (SR) -- short-range van der Waals
- Coulomb (SR) -- short-range electrostatics
- Coul. recip. -- PME reciprocal-space contribution
- Dispersion correction

**Constraint contributions:**
- Constraint energy (LINCS/SETTLE/SHAKE total correction)

**Pressure tensor components:**
- Pres-XX, Pres-YY, Pres-ZZ, Pres-XY, Pres-XZ, Pres-YZ

**Box dimensions:**
- Box-X, Box-Y, Box-Z, Box-XX, Box-YY, Box-ZZ (for triclinic)

**Virial components:**
- Vir-XX, Vir-YY, Vir-ZZ, Vir-XY, Vir-XZ, Vir-YZ

**Thermostat/barostat extended variables:**
- Nose-Hoover chain variables (when `tcoupl = nose-hoover`)
- Parrinello-Rahman box deformation variables (when `pcoupl = parrinello-rahman`)
- Conserved energy quantity (for Nose-Hoover: total energy + thermostat energy)

**Per-group decomposition (if `energygrps` is specified):**
- Coul-SR:Group1-Group2, LJ-SR:Group1-Group2 for each pair of energy groups

**Free energy quantities (if `free-energy = yes`):**
- dH/dl, Delta-H for each lambda state, foreign lambda energies

### 4.2 Resolution and Completeness

- **Time resolution:** Controlled by `nstenergy` parameter. Default: 1000 steps. At 2 fs timestep, this gives one energy frame every 2 ps. Can be set as low as 1 (every step) for maximum resolution, but produces large files.
- **Completeness:** All energy terms that GROMACS computes are written to .edr. There is no mechanism to selectively exclude terms (though energy groups can be configured to control the decomposition level). The .edr is complete by design.
- **Precision:** Energies are stored in single precision in the standard build, double precision in the double-precision build. The .edr uses XDR encoding which provides platform-independent representation.
- **Post-run statistics:** The .edr contains block-average data in addition to instantaneous values, enabling computation of standard errors and autocorrelation times.

### 4.3 Programmatic Access

| Method | Requirements | Returns | Suitability |
|:---|:---|:---|:---|
| `gmx energy` | GROMACS installation | Interactive term selection, outputs .xvg text file | Good for manual analysis, poor for automation (interactive prompt) |
| `panedr` | Python package (`pip install panedr`) | `pandas.DataFrame` with all energy terms indexed by time | Excellent for automation. No GROMACS installation required. Direct programmatic access to all terms. |
| `gmxapi` | GROMACS + gmxapi Python bindings | Pythonic access to simulation data | Good but requires GROMACS build with gmxapi support |
| `alchemlyb` | Python package | Specialized for free energy data from .edr | Good for free energy analysis specifically |
| Manual XDR | `xdrlib` (Python stdlib, deprecated in 3.11) | Raw binary parsing | Possible but requires knowledge of GROMACS XDR frame structure. Not recommended. |

**Recommended access path for ATHENA:** `panedr` provides the cleanest programmatic access. Example:
```python
import panedr
df = panedr.edr_to_df("ener.edr")
# df columns: Time, Bond, Angle, ..., Temperature, Pressure, ...
# df index: integer frame number
# df["Temperature"] gives full temperature time series
```

### 4.4 Relevance to Fault Isolation

The .edr is the primary quantitative signal for detecting simulation anomalies:

| Signal | Detection | Classification |
|:---|:---|:---|
| Total energy drift > threshold | Linear regression on total energy vs. time | Methodology (dt too large) or Theory (force field instability) |
| Temperature deviation from target | Mean(T) vs. ref-t, standard deviation | Methodology (thermostat misconfiguration) or Implementation (coupling group error) |
| Pressure oscillation amplitude | Standard deviation of pressure time series | Theory (incorrect compressibility) or Methodology (barostat coupling too tight) |
| NaN in any energy term | `df.isna().any()` | Implementation (numerical overflow, bad coordinates) |
| Sudden energy jump | First-difference time series, threshold detection | Implementation (restart artifact) or Theory (force field discontinuity) |
| Constraint energy spike | Time series anomaly detection on constraint term | Theory (stiff DOF not constrained) or Implementation (LINCS failure) |
| Conserved quantity drift | Linear regression on conserved energy (Nose-Hoover) | Methodology (timestep too large for Nose-Hoover integration) |

---

## 5. Error and Warning Taxonomy

### 5.1 Error Reporting Mechanism

GROMACS uses three severity levels for diagnostic messages, but these are not formally structured:

- **NOTE:** Informational. Alerts the user to non-default behavior or potential concerns. Does not affect execution.
- **WARNING:** Potentially problematic configuration or runtime condition. grompp may convert warnings to errors with `-maxwarn 0` (default). mdrun emits warnings to the log.
- **Fatal error:** Simulation cannot continue. mdrun terminates with a non-zero exit code.

Error messages are emitted as free-text strings to stderr and/or the .log file. There are no structured error codes, no error taxonomies, and no machine-readable error classification in GROMACS's design. This is a significant challenge for automated trace parsing.

### 5.2 Constraint Algorithm Failures

#### LINCS (LINear Constraint Solver)

**Manifestation:** Warning messages in the .log file, typically:
```
Step N, time T: LINCS WARNING
relative constraint deviation after LINCS:
rms X, max Y (between atoms A and B)
bonds that rotated more than 30 degrees:
atom1 atom2  angle
```

If LINCS cannot satisfy constraints within the configured tolerance and iteration count, it emits this warning. If the violations are severe enough (typically when atoms have moved far from equilibrium positions), LINCS fails to converge and mdrun terminates with:
```
Fatal error:
Step N: The coordinates have become invalid; the average distance is X.
```
or:
```
Fatal error:
Too many LINCS warnings (N)
If you know what you are doing you can adjust the lincs warning threshold in your mdp file
```

**Classification: Ambiguous (methodology/theory/implementation).**
- **Methodology:** Most common cause. Timestep `dt` is too large for the force field and system. Heavy atoms bonded to hydrogens require constraints (LINCS) when using dt >= 2 fs. If dt is 2 fs but constraints are not applied to H-bonds, LINCS failures result. This is a methodology error: the user's simulation protocol is inconsistent.
- **Theory:** The force field assigns incorrect equilibrium bond lengths or force constants, causing unphysical strain that LINCS cannot resolve. This is a theory-layer error: the force field (the physical model) is wrong.
- **Implementation:** Incorrect topology (wrong atom types assigned, missing bonds), initial structure with severe clashes or overlapping atoms, restart from corrupted checkpoint. These are implementation-layer errors.

The LINCS warning itself does not distinguish between these causes. The LFI must examine context: the .mdp parameters (is dt appropriate?), the topology (are constraints properly specified?), the initial structure (are there clashes?), and the energy trajectory (is there energy drift before the LINCS failure?).

#### SETTLE

**Manifestation:** Similar to LINCS but specific to water molecules:
```
Fatal error:
SETTLE: Water molecule starting at atom N can not be settled.
Check for bad contacts and/or reduce the timestep if appropriate.
```

**Classification: Same ambiguity as LINCS.** SETTLE failures in water molecules are almost always caused by: (a) bad initial water geometry, (b) too large a timestep, or (c) nearby atoms with extreme forces disrupting water geometry. The root cause is usually elsewhere in the system, with the water constraint failure being a symptom.

#### SHAKE

**Manifestation:**
```
Fatal error:
SHAKE: Bond N between atoms A and B is not converged after N iterations.
```

**Classification: Same ambiguity as LINCS/SETTLE.** SHAKE is an older algorithm; the same causes apply.

### 5.3 Domain Decomposition / Spatial Errors

**"Atoms too far" / charge group errors:**
```
Fatal error:
Atom N in domain decomposition cell X Y Z is not at the expected position.
```
or:
```
Fatal error:
One or more atoms are not in the expected domain decomposition cell.
```

**Classification: Implementation, with methodology overlap.**
- **Implementation (primary):** Domain decomposition requires that atoms do not move more than a certain distance between neighbor list updates. This error usually indicates that atoms have "exploded" — moved unphysically far in a single step — due to extremely large forces. The domain decomposition grid cannot track them.
- **Methodology:** The root cause of the atomic explosion is usually an unstable simulation: bad initial structure (clashes), missing interactions, or too large a timestep. The domain decomposition error is an implementation-layer symptom of a methodology-layer cause.

### 5.4 NaN Generation

**Manifestation:**
```
Fatal error:
Particle N in domain X Y Z has no charge group, but moved to X Y Z.
```
or detected via the .edr file containing NaN values in energy terms. In severe cases:
```
Fatal error:
There are N non-finite values in the force array.
```

**Classification: Implementation or methodology.**
- **Implementation:** Division by zero (overlapping atoms), numerical overflow (single-precision underflow in PME), or corrupted state from restart.
- **Methodology:** Force field parameters that produce extremely large forces (e.g., missing Lennard-Jones parameters leading to zero sigma, causing 1/r^12 singularity at short range).

### 5.5 Software Inconsistency Errors

**Manifestation:**
```
Fatal error:
Number of atoms in the tpr file (N) does not match the number in the coordinate file (M).
```
or:
```
Fatal error:
Can not do PME with decomposition that splits charge groups.
```

**Classification: Implementation.** These are purely software/configuration errors — mismatched input files, incompatible parameter combinations, or incorrect file paths. No theory content.

### 5.6 Memory and Resource Errors

**Manifestation:**
```
Fatal error:
Not enough memory to allocate N bytes.
```
or GPU-related:
```
Fatal error:
GPU error N: out of memory.
```
or MPI-related:
```
Fatal error:
MPI_Allreduce failed with error code N.
```

**Classification: Implementation.** Purely hardware/resource issues. No theory or methodology content. The simulation's physical model is unrelated to available memory.

### 5.7 Summary Classification Table

| Error Category | Typical Message Pattern | Primary Classification | Can Be Caused By |
|:---|:---|:---|:---|
| LINCS warning/failure | "LINCS WARNING", "relative constraint deviation" | **Ambiguous** | dt too large (methodology), bad FF (theory), clashes (implementation) |
| SETTLE failure | "Water molecule...can not be settled" | **Ambiguous** | Bad geometry (impl), dt too large (methodology), nearby explosion (theory/methodology) |
| SHAKE failure | "Bond...not converged" | **Ambiguous** | Same as LINCS |
| Domain decomposition | "atom not at expected position" | **Implementation** (symptom) | Usually secondary to methodology/theory root cause |
| NaN in forces | "non-finite values in force array" | **Implementation/methodology** | Overlapping atoms (impl), missing LJ params (methodology/theory) |
| Atom count mismatch | "Number of atoms does not match" | **Implementation** | File mismatch |
| Parameter incompatibility | "Can not do PME with..." | **Implementation** | Bad configuration |
| Memory/resource | "Not enough memory" | **Implementation** | Hardware limitation |
| GPU error | "GPU error" | **Implementation** | Driver/hardware issue |
| MPI error | "MPI failed" | **Implementation** | Network/configuration issue |

### 5.8 Key Finding: Ambiguity Clustering

The most common simulation failures — constraint violations (LINCS, SETTLE, SHAKE) and domain decomposition errors — are exactly those that cluster at the theory-implementation-methodology boundary. These are the errors the LFI must resolve most frequently, and they are the hardest to classify from the error message alone. Correct classification requires cross-referencing:
1. The .mdp parameters (was the protocol reasonable?)
2. The .edr trajectory (was there energy drift before the crash?)
3. The topology (are force field parameters complete and correct?)
4. The initial structure (were there clashes?)
5. The failure timing (did it happen at step 0 vs. after 10 ns?)

This cross-referencing requirement is central to the IR design: the IR cannot represent these errors as isolated events. It must link each error to the relevant parameter, energy, and structural context.

---

## 6. Failure Walkthrough: LINCS Constraint Failure

This section traces a concrete LINCS constraint failure through the complete GROMACS output system to document what an external observer (the Trace Semantics Engine) can reconstruct.

### 6.1 Scenario

A user runs an MD simulation of a protein in explicit water. The timestep is dt = 0.002 ps (2 fs). Hydrogen bonds are constrained with LINCS (`constraints = h-bonds`). After 50,000 steps (100 ps), a LINCS warning appears, and at step 52,341 the simulation crashes.

### 6.2 What Appears in the .log File

**Pre-crash (around step 50,000):**
```
Step 50000, time 100.000: LINCS WARNING
relative constraint deviation after LINCS:
rms 0.0143, max 0.2871 (between atoms 4523 and 4525)
bonds that rotated more than 30 degrees:
 atom 4523  atom 4525  angle  89.3

Step 50200, time 100.400: LINCS WARNING
relative constraint deviation after LINCS:
rms 0.0891, max 0.9123 (between atoms 4523 and 4525)
bonds that rotated more than 30 degrees:
 atom 4523  atom 4525  angle  134.7
 atom 4527  atom 4529  angle  67.2
```

**Crash (step 52,341):**
```
Step 52341, time 104.682:

Fatal error:
Step 52341: The coordinates have become invalid; the average abs
distance from the mean is 2.38e+08 nm.
If everything else looks fine, this is probably a bug in GROMACS.
Provide us with a run input file or the relevant .tpr and .log files
for investigation.
```

**Post-crash:**
- The log file is closed with the partial performance summary up to the crash point.
- No final energy statistics are written (the run did not complete normally).

### 6.3 What the .edr Contains

The .edr file contains energy frames up to the last `nstenergy` interval before the crash. Assuming `nstenergy = 1000`:

- Frames exist at steps 0, 1000, 2000, ..., 52000.
- Step 52341 (crash step) is NOT in the .edr unless nstenergy divides it exactly.
- The last valid frame (step 52000) may show:
  - Elevated potential energy (rising from baseline)
  - Elevated temperature (kinetic energy increase)
  - Large constraint energy term
  - Possibly NaN if the explosion propagated before step 52000

**Observable in panedr:**
```python
import panedr
df = panedr.edr_to_df("ener.edr")
# Last few frames:
# Time=100.0: Potential=-350000 kJ/mol, Temperature=302 K (normal)
# Time=102.0: Potential=-280000 kJ/mol, Temperature=450 K (abnormal)
# Time=104.0: Potential=NaN or extremely large positive value
```

The energy trajectory provides a temporal signature: gradual escalation over ~2-4 ps before the crash indicates a physical instability (methodology or theory issue), while a sudden single-step jump would suggest an implementation issue (e.g., corrupted state from restart).

### 6.4 Exit Code

GROMACS mdrun exits with a non-zero exit code on fatal error (typically exit code 1, though the exact value is not formally specified in the documentation). The error message is written to both stderr and the .log file.

### 6.5 State Files Produced

| File | Status at crash | Content |
|:---|:---|:---|
| .log | Partially written | Contains all output up to the crash, including the fatal error message |
| .edr | Partially written | Contains energy frames up to the last nstenergy interval before crash |
| .trr | Partially written | Contains trajectory frames up to the last nstxout interval before crash |
| .xtc | Partially written | Contains compressed trajectory frames up to the last nstxout-compressed interval before crash |
| .cpt | Last checkpoint | Contains state from the last checkpoint (potentially 15+ minutes of wall time before the crash). NOT the crash state. |
| state_prev.cpt | Penultimate checkpoint | Previous checkpoint (GROMACS keeps two for safety) |

**Critical gap:** There is no state dump at the exact crash point. The crash state (the coordinates and velocities that caused the fatal error) is lost unless the user had set `nstxout = 1` or `nstenergy = 1` (which would produce enormous files). The .cpt only contains the last periodic checkpoint, which may be thousands of steps before the crash.

GROMACS 2020+ produces a `confout.gro` file on crash that contains the coordinates at the crash step, but velocities and forces at the crash point are not preserved.

### 6.6 What an External Observer Can Reconstruct

**Reconstructible with high confidence:**
1. The crash was caused by coordinate explosion (from the fatal error message).
2. LINCS warnings preceded the crash by ~2,000 steps (from the log).
3. The specific bond that failed (atoms 4523-4525) and when it first showed trouble (from LINCS warning messages).
4. The energy trajectory showed escalating instability before the crash (from .edr).
5. All simulation parameters (from .log parameter echo or .tpr dump).
6. The initial structure and topology (from .tpr or original input files).

**Reconstructible with moderate confidence:**
7. Whether the timestep was appropriate for the system (by comparing dt, constraint settings, and force field to established best practices).
8. Whether the force field parameters for the failing atoms are correct (by examining the topology for the atoms involved in the LINCS failure).
9. Whether the initial structure had clashes near the failing region (by examining the initial coordinates around atoms 4523-4525).

**Not directly reconstructible:**
10. Whether the force field is physically correct for this system (requires domain knowledge beyond what the trace contains).
11. Whether a methodology error (insufficient equilibration, wrong ensemble) caused the instability vs. a genuine force field deficiency.
12. The exact coordinates and velocities at the crash step (lost; only the last checkpoint is available).

### 6.7 Implications for IR Design

This walkthrough reveals several IR requirements:

1. **Temporal linking:** The IR must connect the LINCS warnings at step 50,000 to the crash at step 52,341 as a causal sequence, not independent events. The time-to-crash and escalation pattern are diagnostic signals.

2. **Cross-file correlation:** The IR must merge data from .log (LINCS warnings, error messages), .edr (energy escalation), and .tpr (parameters) into a unified failure narrative. No single file contains enough information.

3. **Atom-level resolution:** The LINCS warning identifies specific atoms (4523, 4525). The IR must be able to trace these atoms back to their residue, chain, and force field atom type to determine whether the cause is in the topology (theory) or the structure (implementation).

4. **Absence detection:** The IR must note what is NOT in the trace: the crash-state coordinates, the forces at the crash step, any structural context about why those specific atoms failed. Absence of data constrains the confidence of fault classification.

5. **Pattern matching:** The escalation pattern (warning -> more warnings -> crash) is a known signature of timestep/constraint misconfiguration. The IR should support pattern matching against known failure signatures to accelerate classification.

---

## 7. Preprocessing Validation (grompp)

### 7.1 What grompp Checks

grompp performs extensive validation before producing the .tpr. The checks fall into several categories:

#### Topology-Structure Consistency
- Atom count in .gro/.pdb matches atom count in .top
- Residue names in coordinate file are consistent with topology (warns on mismatch)
- Box vectors are present and valid (non-zero volume)
- Coordinate values are finite (no NaN or Inf)

#### Parameter Validation
- All required .mdp parameters are present (missing parameters use documented defaults)
- Enumerated parameters have valid values (e.g., `integrator` must be one of the allowed strings)
- Numerical parameters are within valid ranges (e.g., `dt > 0`, `nsteps >= 0`)
- Parameter combinations are consistent (e.g., `pcoupl != no` requires `ref-p` and `compressibility`)
- Deprecated parameters generate warnings with migration guidance

#### Force Field Validation
- All atom types referenced in the topology have parameters defined
- Bond, angle, and dihedral parameters exist for all interactions
- Charge neutrality check (warns if system is not neutral, errors if PME is used with net charge without background ion correction)
- Pair interactions (1-4) are correctly specified

#### Geometry Validation
- Box dimensions are large enough for the specified cutoff radii (`rcoulomb`, `rvdw`): minimum image convention requires box edge > 2 * max(rcoulomb, rvdw)
- Warns if molecules span more than half the box edge (could interact with own periodic image)
- Checks for extremely short distances between atoms (potential clashes), though this is a warning, not an error

#### Constraint Validation
- Verifies that constrained bonds have valid reference lengths
- Checks that LINCS order and iterations are consistent with the number of coupled constraints
- Warns if using constraints with algorithms that do not support them (e.g., normal mode analysis)

#### Ensemble Consistency
- Thermostat coupling groups cover all atoms in the system
- No atom is in multiple coupling groups
- Warns if using Berendsen thermostat (does not produce correct canonical ensemble) for production runs
- Warns if using Berendsen barostat for production runs (use Parrinello-Rahman or C-rescale instead)
- Verifies that velocity generation is consistent with thermostat choice

#### Free Energy Validation
- Lambda schedules are monotonic or at least well-defined
- Soft-core parameters are valid when specified
- Foreign lambda states are consistent with the lambda vector

### 7.2 What grompp Catches vs. What Slips Through

#### Caught by grompp (errors that prevent .tpr generation):

| Check | Severity | Example |
|:---|:---|:---|
| Atom count mismatch | Fatal | .gro has 45,000 atoms, .top defines 44,998 |
| Missing force field parameters | Fatal | Atom type "XX" has no LJ parameters |
| Box too small for cutoffs | Fatal | Box edge 2.0 nm, rcoulomb 1.2 nm (minimum image: need > 2.4 nm) |
| Invalid parameter values | Fatal | `integrator = invalid_string` |
| Missing required parameters | Fatal | `pcoupl = parrinello-rahman` without `ref-p` |
| PME with net charge (no correction) | Fatal (GROMACS 2021+) or Warning | Non-neutral system with PME |

#### Caught by grompp (warnings, convertible to errors with `-maxwarn 0`):

| Check | Severity | Example |
|:---|:---|:---|
| Berendsen thermostat | Warning | "Using Berendsen thermostat does not reproduce the correct ensemble" |
| Short atomic distances | Warning | Atoms within 0.1 nm in initial structure |
| Large charge groups | Warning | Charge groups too large for domain decomposition |
| Unused .mdp parameters | Warning | Typo in parameter name (silently ignored, warned) |
| Deprecated parameters | Warning | Old parameter name that has been replaced |
| Non-neutral system | Warning | Total system charge is not zero |

#### NOT caught by grompp (slips through to runtime):

| Issue | Why grompp misses it | When it manifests |
|:---|:---|:---|
| Timestep too large for force field | No physics-based dt validation. grompp does not analyze the highest-frequency motions in the system. | LINCS/SETTLE failure during mdrun, typically within first few ps. |
| Incorrect force field assignment | If atom types are valid but wrong (e.g., SP2 carbon assigned SP3 parameters), grompp accepts it — the parameters exist, they are just wrong for the chemistry. | Incorrect energies, structural drift, or instability during mdrun. |
| Insufficient equilibration | grompp does not evaluate whether the initial structure is properly equilibrated for the target ensemble. | Large initial forces, temperature/pressure spikes in early steps of mdrun. |
| Missing solvent interactions | If a solute has valid parameters but its interactions with water are wrong (e.g., incorrect combination rules), grompp cannot detect this. | Incorrect solvation behavior, gradual structural artifacts during mdrun. |
| Inadequate sampling | nsteps may be too short to observe the phenomenon of interest. grompp has no way to evaluate scientific adequacy of simulation length. | No error — the simulation completes normally but produces scientifically inadequate results. |
| Periodic image artifacts | Molecule interacting with its own periodic image. grompp checks box size vs. cutoffs but not whether the molecule's radius of gyration approaches half the box edge. | Artificial self-interaction, incorrect structural behavior. |
| Non-physical force field | A force field that is syntactically valid but physically nonsensical (e.g., all charges set to zero). grompp checks syntax, not physics. | Simulation runs but produces non-physical results. |
| Subtle topology errors | Incorrect dihedral multiplicities, wrong improper dihedral parameters, missing backbone dihedrals — all syntactically valid. | Incorrect conformational behavior, may not manifest as errors. |

### 7.3 Coverage Assessment

**grompp validation coverage is strong for syntactic/structural errors and weak for physical/scientific errors.** This aligns with its role as a compiler: it validates that the input is well-formed and internally consistent, not that it is scientifically correct.

For ATHENA's IR design, this means:
1. **Implementation-layer errors are mostly caught by grompp.** Mismatched files, missing parameters, invalid configurations — these are detected before the simulation runs. If a simulation starts, the implementation-layer specification is syntactically valid.
2. **Methodology-layer errors partially slip through.** Timestep adequacy, equilibration quality, and sampling sufficiency are not validated. These must be detected at runtime from the .edr and .log.
3. **Theory-layer errors are invisible to grompp.** Wrong force field assignment, incorrect physics, and non-physical models are accepted as long as the syntax is valid. Theory-layer validation requires comparing simulation results against expected physical behavior — which is the LFI's Stage 3 responsibility.

This creates a clear audit hierarchy:
- **Pre-grompp:** Is the specification syntactically valid? (grompp handles this.)
- **Runtime:** Is the execution numerically stable? (Detected from .log and .edr.)
- **Post-run analysis:** Are the results physically reasonable? (Requires domain knowledge and the causal graph.)

### 7.4 grompp as Audit Artifact

The grompp output itself is valuable for the IR:
- grompp writes a processed topology to `processed.top` (with `-pp` flag) showing the fully expanded topology after all `#include` processing. This is the ground truth for the force field as applied.
- grompp writes warnings and notes to stderr. These can be captured and parsed.
- The .tpr file produced by grompp is a cryptographic-grade reproducibility artifact: given the same .mdp, .top, and .gro, grompp produces an identical .tpr (modulo velocity generation, which depends on the random seed `gen-seed`).

---

## 8. Summary: GROMACS Trace Quality for ATHENA's IR

### 8.1 Strengths

1. **Rich quantitative data.** The .edr file provides a complete, structured time series of all computed observables at configurable resolution. This is the highest-quality data source for anomaly detection and is accessible via `panedr` without any GROMACS installation.

2. **Complete parameter echo.** The .log file reproduces all simulation parameters, enabling full reconstruction of the experiment specification from the trace alone.

3. **Self-contained .tpr.** The .tpr binary bundles the complete experiment specification (topology, parameters, initial state) into a single reproducible artifact. `gmx dump` can extract all contents.

4. **Clean mdrun CLI separation.** Implementation-layer parameters (threading, GPU assignment, domain decomposition) are entirely on the mdrun command line, never in the .mdp or .tpr. This is the cleanest layer separation in the GROMACS output system.

5. **Checkpoint system.** The .cpt file preserves complete state for restart, enabling re-execution from a known state.

### 8.2 Weaknesses

1. **No structured error taxonomy.** Error messages are free-text strings with no error codes, severity levels, or machine-readable classification. Parsing errors requires regex matching against known message patterns — a brittle, version-dependent approach.

2. **Ambiguous failure classification.** The most common failures (LINCS, SETTLE, domain decomposition) are inherently ambiguous between theory, methodology, and implementation causes. The error messages do not provide enough context for automated classification; cross-file correlation is required.

3. **No crash-state dump.** When a simulation crashes, the exact state at the crash point is not fully preserved. The last checkpoint may be thousands of steps before the crash. This limits forensic analysis of the failure.

4. **Semi-structured log format.** The .log file is human-readable but not machine-parseable without format-specific regex patterns. The format is not formally specified and can change between GROMACS versions.

5. **Opaque .tpr boundary.** The .tpr merges theory and implementation layers into a single binary. Recovering the layer separation requires external classification knowledge (the parameter tables in Section 2.1).

6. **No provenance tracking.** GROMACS does not track the provenance chain: which .mdp, .top, and .gro files produced the .tpr, whether the .mdp was modified from a template, or whether the force field files were standard or customized.

### 8.3 IR Design Requirements Derived from This Analysis

1. The IR must maintain a **GROMACS parameter classification table** mapping every .mdp parameter to theory/implementation/boundary, with the boundary parameters carrying dual annotations.

2. The IR must implement **cross-file correlation** to merge .log events, .edr time series, and .tpr parameters into unified failure narratives.

3. The IR must support **temporal linking** of related events (e.g., LINCS warnings escalating to crashes).

4. The IR must include an **error pattern library** for GROMACS-specific error messages, mapping known message patterns to preliminary fault classifications (with confidence levels reflecting the inherent ambiguity).

5. The IR must track **data absence** — what information is NOT available in the trace — as a first-class concept, since the confidence of fault classification depends on data completeness.

6. The IR must distinguish between **user-specified parameters** and **runtime-adjusted parameters** (auto-tuned nstlist, auto-computed rlist, runtime PME tuning).

---

## References

- GROMACS Reference Manual (manual.gromacs.org): file formats, mdp options, mdrun documentation, grompp documentation
- GROMACS GitHub repository (github.com/gromacs/gromacs): source code for error messages, constraint solvers, output routines
- panedr library (github.com/panedr/panedr): Python .edr reader documentation
- MDAnalysis documentation (mdanalysis.org): .tpr and trajectory reader capabilities
- GROMACS user forums and mailing list archives: real-world failure cases and diagnostic discussions
- Lemkul, J. "GROMACS Tutorials" (mdtutorials.com): canonical simulation workflows documenting expected output
- Abraham, M.J. et al. "GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers." SoftwareX 1-2, 19-25 (2015): Architecture and design documentation
